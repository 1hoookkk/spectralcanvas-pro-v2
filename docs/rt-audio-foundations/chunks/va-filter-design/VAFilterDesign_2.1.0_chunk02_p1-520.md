# VA Filter Design — Chunk 2/2

> Source: `VAFilterDesign_2.1.0.pdf` · Pages ~1-520 · Extractor: pdfminer.six
> Target ≈ 2800 words, Overlap = 200 words · Actual = 201699

> Note: DSP textbook content for SpectralCanvasPro RT-audio knowledge base

---
THE ART OF

VA FILTER DESIGN
ωcT /2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
•(cid:47)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Vadim Zavalishin
rev. 2.1.0 (October 28, 2018)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
ii
About this book: the book covers the theoretical and practical aspects of the
virtual analog ﬁlter design in the music DSP context. Only a basic amount of
DSP knowledge is assumed as a prerequisite. For digital musical instrument
and eﬀect developers.
Front picture: BLT integrator.
DISCLAIMER: THIS BOOK IS PROVIDED “AS IS”, SOLELY AS AN EX-
PRESSION OF THE AUTHOR’S BELIEFS AND OPINIONS AT THE TIME
OF THE WRITING, AND IS INTENDED FOR THE INFORMATIONAL
PURPOSES ONLY.
c(cid:13) Vadim Zavalishin. The right is hereby granted to freely copy this revision of the book in
software or hard-copy form, as long as the book is copied in its full entirety (including this
copyright note) and its contents are not modified.
To the memory of Elena Golushko,
may her soul travel the happiest path. . .
iv
Contents
Preface
1 Fourier theory
1.1
1.2
1.3
1.4
1.5
Complex sinusoids
. . . . . . . . . . . . . . . . . . . . . . . . .
Fourier series
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
Fourier integral
Dirac delta function . . . . . . . . . . . . . . . . . . . . . . . .
Laplace transform . . . . . . . . . . . . . . . . . . . . . . . . . .
2 Analog 1-pole filters
RC ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1
Block diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2
Transfer function . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3
Complex impedances . . . . . . . . . . . . . . . . . . . . . . . .
2.4
Amplitude and phase responses . . . . . . . . . . . . . . . . . .
2.5
Lowpass ﬁltering . . . . . . . . . . . . . . . . . . . . . . . . . .
2.6
Cutoﬀ parameterization . . . . . . . . . . . . . . . . . . . . . .
2.7
Highpass ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.8
2.9
Poles and zeros . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.10 LP to HP substitution . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
2.11 Multimode ﬁlter
. . . . . . . . . . . . . . . . . . . . . . . . . . .
2.12
2.13 Allpass ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.14 Transposed multimode ﬁlter . . . . . . . . . . . . . . . . . . . .
2.15 Transient response . . . . . . . . . . . . . . . . . . . . . . . . .
2.16 Cutoﬀ as time scaling . . . . . . . . . . . . . . . . . . . . . . .
Shelving ﬁlters
3 Time-discretization
3.1
3.2
3.3
3.4
3.5
3.6
3.7
3.8
3.9
3.10
Discrete-time signals . . . . . . . . . . . . . . . . . . . . . . . .
Naive integration . . . . . . . . . . . . . . . . . . . . . . . . . .
Naive lowpass ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . .
Block diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . .
Transfer function . . . . . . . . . . . . . . . . . . . . . . . . . .
Trapezoidal integration . . . . . . . . . . . . . . . . . . . . . . .
Bilinear transform . . . . . . . . . . . . . . . . . . . . . . . . .
Cutoﬀ prewarping
. . . . . . . . . . . . . . . . . . . . . . . . .
Zero-delay feedback . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
Implementations
v
ix
1
1
2
3
4
5
7
7
8
9
12
13
14
15
18
19
24
25
27
28
30
32
40
45
45
47
47
48
50
53
57
59
73
76
vi
CONTENTS
3.11 Direct forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.12 Transient response . . . . . . . . . . . . . . . . . . . . . . . . .
3.13
Instantaneously unstable feedback . . . . . . . . . . . . . . . .
3.14 Other replacement techniques . . . . . . . . . . . . . . . . . . .
78
82
83
90
4 State variable filter
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
Analog model
95
Resonance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
Poles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
Digital model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
Normalized bandpass ﬁlter . . . . . . . . . . . . . . . . . . . . . 111
LP to BP/BS substitutions
. . . . . . . . . . . . . . . . . . . . 114
Further ﬁlter types . . . . . . . . . . . . . . . . . . . . . . . . . 117
Transient response . . . . . . . . . . . . . . . . . . . . . . . . . 122
4.1
4.2
4.3
4.4
4.5
4.6
4.7
4.8
5 Ladder filter
133
Analog model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
5.1
Feedback and resonance . . . . . . . . . . . . . . . . . . . . . . 135
5.2
Digital model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
5.3
5.4
Feedback shaping . . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.5 Multimode ladder ﬁlter . . . . . . . . . . . . . . . . . . . . . . . 141
HP ladder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
5.6
BP ladder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
5.7
Sallen–Key ﬁlters . . . . . . . . . . . . . . . . . . . . . . . . . . 149
5.8
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
5.9
8-pole ladder
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
5.10 Diode ladder
6 Nonlinearities
173
6.1 Waveshaping . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
Saturators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
6.2
Feedback loop saturation . . . . . . . . . . . . . . . . . . . . . . 179
6.3
Nonlinear zero-delay feedback equation . . . . . . . . . . . . . . 183
6.4
Iterative methods . . . . . . . . . . . . . . . . . . . . . . . . . . 184
6.5
. . . . . . . . . . . . . . . . . . . . . . . 191
Approximate methods
6.6
2nd-order saturation curves . . . . . . . . . . . . . . . . . . . . 192
6.7
Tabulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
6.8
6.9
Saturation in 1-pole ﬁlters . . . . . . . . . . . . . . . . . . . . . 199
6.10 Multinonlinear feedback . . . . . . . . . . . . . . . . . . . . . . 204
6.11 Antisaturators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
6.12 Asymmetric saturation . . . . . . . . . . . . . . . . . . . . . . . 217
6.13 Antialiasing of waveshaping . . . . . . . . . . . . . . . . . . . . 220
7 State-space form
237
Diﬀerential state-space form . . . . . . . . . . . . . . . . . . . . 237
Integratorless feedback . . . . . . . . . . . . . . . . . . . . . . . 239
Transfer matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
Transposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
Basis changes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
. . . . . . . . . . . . . . . . . . . . . . . . . 244
Transient response . . . . . . . . . . . . . . . . . . . . . . . . . 245
7.1
7.2
7.3
7.4
7.5
7.6 Matrix exponential
7.7
CONTENTS
vii
Diagonal form . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
7.8
Real diagonal form . . . . . . . . . . . . . . . . . . . . . . . . . 250
7.9
Jordan normal form . . . . . . . . . . . . . . . . . . . . . . . . 256
7.10
Ill-conditioning of diagonal form . . . . . . . . . . . . . . . . . . 259
7.11
. . . . . . . . . . . . . . . . . . . . . . . . . 262
7.12 Time-varying case
7.13 Discrete-time case
. . . . . . . . . . . . . . . . . . . . . . . . . 266
7.14 Trapezoidal integration . . . . . . . . . . . . . . . . . . . . . . . 269
8 Raising the filter order
271
Generalized SVF . . . . . . . . . . . . . . . . . . . . . . . . . . 271
Serial cascade representation . . . . . . . . . . . . . . . . . . . . 273
Parallel representation . . . . . . . . . . . . . . . . . . . . . . . 278
Cascading of identical ﬁlters . . . . . . . . . . . . . . . . . . . . 281
Butterworth transformation . . . . . . . . . . . . . . . . . . . . 283
Butterworth ﬁlters of the 1st kind . . . . . . . . . . . . . . . . . 286
Butterworth ﬁlters of the 2nd kind . . . . . . . . . . . . . . . . 294
Generalized ladder ﬁlters . . . . . . . . . . . . . . . . . . . . . . 305
8.1
8.2
8.3
8.4
8.5
8.6
8.7
8.8
9 Classical signal processing filters
309
Riemann sphere . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
9.1
. . . . . . . . . . . . . . . . . . . . . . . . . . 313
Arctangent scale
9.2
Rotations of Riemann sphere
. . . . . . . . . . . . . . . . . . . 314
9.3
Butterworth ﬁlter revisited . . . . . . . . . . . . . . . . . . . . 319
9.4
Trigonometric functions on complex plane . . . . . . . . . . . . 325
9.5
Chebyshev polynomials . . . . . . . . . . . . . . . . . . . . . . . 332
9.6
Chebyshev type I ﬁlters
. . . . . . . . . . . . . . . . . . . . . . 337
9.7
Chebyshev type II ﬁlters . . . . . . . . . . . . . . . . . . . . . . 344
9.8
9.9
Jacobian elliptic functions . . . . . . . . . . . . . . . . . . . . . 349
9.10 Normalized Jacobian elliptic functions . . . . . . . . . . . . . . 362
. . . . . . . . . . . . . . . . . . . . . . 374
9.11 Landen transformations
9.12 Elliptic rational functions
. . . . . . . . . . . . . . . . . . . . . 383
9.13 Elliptic ﬁlters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
10 Special filter types
407
10.1 Reciprocally symmetric functions . . . . . . . . . . . . . . . . . 407
10.2
Shelving and tilting ﬁlters . . . . . . . . . . . . . . . . . . . . . 410
10.3 Fixed-slope shelving . . . . . . . . . . . . . . . . . . . . . . . . 416
10.4 Variable-slope shelving . . . . . . . . . . . . . . . . . . . . . . . 421
10.5 Higher-order shelving . . . . . . . . . . . . . . . . . . . . . . . . 425
10.6 Band shelving . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
10.7 Elliptic shelving . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
10.8 Crossovers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
10.9 Even/odd allpass decomposition . . . . . . . . . . . . . . . . . . 449
10.10 Analytic ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
10.11 Phase splitter . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
10.12 Frequency shifter . . . . . . . . . . . . . . . . . . . . . . . . . . 468
10.13 Remez algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 472
10.14 Numerical construction of phase splitter . . . . . . . . . . . . . 480
viii
CONTENTS
11 Multinotch filters
487
11.1 Basic multinotch structure . . . . . . . . . . . . . . . . . . . . . 487
1-pole-based multinotches . . . . . . . . . . . . . . . . . . . . . 488
11.2
2-pole-based multinotches . . . . . . . . . . . . . . . . . . . . . 489
11.3
Inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
11.4
11.5 Comb ﬁlters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
11.6 Feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
11.7 Dry/wet mixing . . . . . . . . . . . . . . . . . . . . . . . . . . . 498
11.8 Barberpole notches . . . . . . . . . . . . . . . . . . . . . . . . . 500
History
Index
503
505
Preface
The classical way of presentation of the DSP theory is not very well suitable for
the purposes of virtual analog ﬁlter design. The linearity and time-invariance
of structures are not assumed merely to simplify certain analysis and design
aspects, but are handled more or less as an “ultimate truth”. The connection
to the continuous-time (analog) world is lost most of the time. The key focus
points, particularly the discussed ﬁlter types, are of little interest to a digital
music instrument developer. This makes it diﬃcult to apply the obtained knowl-
edge in the music DSP context, especially in the virtual analog ﬁlter design.
This book attempts to amend this deﬁciency. The concepts are introduced
with the musical VA ﬁlter design in mind. The depth of theoretical explanation
is restricted to an intuitive and practically applicable amount. The focus of the
book is the design of digital models of classical musical analog ﬁlter structures
using the topology-preserving transform approach, which can be considered as
a generalization of bilinear transform, zero-delay feedback and trapezoidal inte-
gration methods. This results in digital ﬁlters having nice amplitude and phase
responses, nice time-varying behavior and plenty of options for nonlinearities.
In a way, this book can be seen as a detailed explanation of the materials pro-
vided in the author’s article “Preserving the LTI system topology in s- to z-plane
transforms.”
The main purpose of this book is not to explain how to build high-quality
emulations of analog hardware (although the techniques explained in the book
can be an important and valuable tool for building VA emulations). Rather it is
about how to build high-quality time-varying digital ﬁlters. The author hopes
that these techniques will be used to construct new digital ﬁlters, rather than
only to build emulations of existing analog structures.
The prerequisites for the reader include familiarity with the basic DSP con-
cepts, complex algebra and the basic ideas of mathematical analysis. Some basic
knowledge of electronics may be helpful at one or two places, but is not critical
for the understanding of the presented materials.
The author apologizes for possible mistakes and messy explanations, as the
book didn’t go through any serious proofreading.
ix
x
PREFACE
Preface to revision 2.0.0alpha
This preface starts with an excuse. With revision 2.0.0 the book receives a major
update, where the new material roughly falls into two diﬀerent categories: the
practical side of VA DSP and a more theoretical part. The latter arose from
the desire to describe theoretical foundations for the subjects which the book
intended to cover. These foundations were not copied from other texts (except
where explicitly noted), but were done from scratch, the author trying to present
the subject in the most intuitive way.1 For that reason, especially in the more
theoretical part, the book possibly contains mistakes.
Certain pieces of information are simply ideas which the author sponta-
neously had and tried to describe,2 not necessarily properly testing all of them.
This is another potential source of mistakes. One option would have been not
rushing the book release and making an exhaustive testing of the presented ma-
terial. During the same time the book text could have gone through a few more
polishing runs, possibly restructuring some of the material in an easier to grasp
way. However, this probably would have delayed the book’s release by half a
year or, likely, much more, as after ﬁve months of overly intensive work on the
book the author (hopefully) deserves some relaxing. On the other hand, the
main intention of the book is not to provide a collection of ready to use recipes,
but rather to describe one possible way to think about the respective matters
and give some key pieces of information. Thus, readers, who understood the
text, should be able to correct the respective mistakes, if any, on their own.
From that perspective, the book in the present state should fulﬁll its goal.
Therefore the author decided to release the book in an alpha state with
the above reservations.3 Readers looking for a collection of time-proven recipes
might want to check other sources.
The author also has recieved a number of complaints in regards to the book
having too high requirements on the math side. It just so happens that certain
things simply need advanced math to be properly understood. Sacriﬁcing the
exactness and the amount of information for the sake of a more accessible text
could have deﬁnitely been an option, but. . . that would have been a completely
diﬀerent book. In that regard the new revision contains parts which are even
harder on the math side than the previous revisions, the math prerequisites for
these parts respectively being generally higher than for the rest of the book.
Such parts, however, may simply be skipped by the readers.
In regards to the usage of the math in the book, the author would like to
make one more remark. The book uses math notation not simply to provide
some calculation formulas or to do formal transformations. The math notation
is also used to express information, since quite in some cases it can do this
much more exactly than words. In that sense the respective formulas become
an integral part of the book’s text, rather than some kind of a parallel stream
of information. E.g. the formula (2.4), which some readers ﬁnd daunting, is
1“Intuitive” here doesn’t mean “easy to understand”, but rather “when understood, it
becomes easy”.
2It is possible that some of these ideas are not new, but the author at the time of the
writing was not aware of that. This might result in a lack of respective credits and in a
different terminology, for which, should that happen to be the case, the author apologizes.
3The alpha state has been dropped in rev.2.1.0, as the author did some additional verifi-
cation of the new materials.
xi
simply providing a detailed explanation to the statement that each partial can
be integrated independently.
Certain readers, being initially daunted by the look of the text, also believe
that they need to read some other ﬁlter DSP text before attempting this one.
This is not necessarily so, since this book strongly deviates in its presentation
from the classical DSP texts and this might create a collision in the beginner’s
mind between two very diﬀerent approaches to the material. Also, chances are,
after reading some other classical DSP text ﬁrst, the reader will only ﬁnd out
that this didn’t help much in regards to understanding this book and was simply
an additional investment of time.
The part of DSP knowledge which is more or less required (although a pretty
surface level should suﬃce) is a basic understanding of discrete time sampling.
Also basic knowledge of Fourier theory could be helpful, but probably even that
is not a must, as the book introduces it in a, however condensed, but suﬃcient
for the understanding of the the further text form. No preliminary knowledge
of ﬁlters is needed. Also, in author’s impression, often the real problem is pos-
sibly an insuﬃcient level of math knowledge or experience, which then leads to
a reader believing that some additional ﬁlter knowledge is needed ﬁrst, whereas
what’s lacking is rather purely the math skills. In this case, if the gap is not
very large, one could try to simply read through anyway, it might become pro-
gressively better, or the part of the math which is not being understood may
happen to be not essential for practical application of the materials.
xii
Acknowledgements
PREFACE
The author would like to express his gratitude to a number of people who helped
him with the matters related to the creation of this book in one or another way:
Daniel Haver, Mate Galic, Tom Kurth, Nicolas Gross, Maike Weber, Martijn
Zwartjes, Mike Daliot and Jelena Miˇceti´c Krowarz. Special thanks to Stephan
Schmitt, Egbert J¨urgens, Tobias Baumbach, Steinunn Arnardottir, Eike Jonas,
Maximilian Zagler, Marin Vrbica and Philipp Dransfeld.
The author is also grateful to a number of people on the KVR Audio DSP
forum and the music DSP mailing list for productive discussions regarding the
matters discussed in the book. Particularly to Martin Eisenberg for the detailed
and extensive discussion of the delayless feedback, to Dominique Wurtz for the
idea of the full equivalence of diﬀerent BLT integrators, to Ren´e Jeschke for
the introduction of the transposed direct form II BLT integrator in the TPT
context, to Teemu Voipio and Max Mikhailov for their active involvement into
the related discussions and research and to Urs Heckmann for being an active
proponent of the ZDF techniques and actually (as far as the author knows)
starting the whole avalanche of their usage. Thanks to Robin Schmidt, Richard
Hoﬀmann and Francisco Garcia for reporting a number of mistakes in the book
text.
One shouldn’t underestimate the small but invaluable contribution by Helene
Kolpakova, whose questions and interest in the VA ﬁlter design matters have
triggered the initial idea of writing this book. Thanks to Julian Parker for
productive discussions, which stimulated the creation of the book’s next revision.
Last, but most importantly, big thanks to Bob Moog for inventing the
voltage-controlled transistor ladder ﬁlter.
Prior work credits
Various ﬂavors and applications of delayless feedback techniques were in prior
use for quite a while. Particularly there are works by A.H¨arm¨a, F.Avancini,
G.Borin, G.De Poli, F.Fontana, D.Rocchesso, T.Seraﬁni and P.Zamboni, al-
though reportedly this subject has been appearing as far ago as in the 70s of
the 20th century.
Chapter 1
Fourier theory
When we are talking about ﬁlters we say that ﬁlters modify the frequency
content of the signal. E.g. a lowpass ﬁlter lets the low frequencies through,
while suppressing the high frequencies, a highpass ﬁlter does vice versa etc.
In this chapter we are going to develop a formal deﬁnition1 of the concept of
frequencies “contained” in a signal. We will later use this concept to analyse
the behavior of the ﬁlters.
1.1 Complex sinusoids
In order to talk about the ﬁlter theory we need to introduce complex sinusoidal
signals. Consider the complex identity:
ejt = cos t + j sin t
(t ∈ R)
(notice that, if t is the time, then the point ejt is simply moving along a unit
circle in the complex plane). Then
and
cos t =
ejt + e−jt
2
sin t =
ejt − e−jt
2j
Then a real sinusoidal signal a cos(ωt + ϕ) where a is the real amplitude and
ϕ is the initial phase can be represented as a sum of two complex conjugate
sinusoidal signals:
a cos(ωt + ϕ) =
a
2
(cid:16)
ej(ωt+ϕ) + e−j(ωt+ϕ)(cid:17)
=
ejϕ(cid:17)
(cid:16) a
2
ejωt +
(cid:16) a
2
e−jϕ(cid:17)
e−jωt
Notice that we have a sum of two complex conjugate sinusoids e±jωt with re-
spective complex conjugate amplitudes (a/2)e±jϕ. So, the complex amplitude
simultaneously encodes both the amplitude information (in its absolute magni-
tude) and the phase information (in its argument). For the positive-frequency
component (a/2)ejϕ · ejωt, the complex “amplitude” a/2 is a half of the real
amplitude and the complex “phase” ϕ is equal to the real phase.
1More precisely we will develop a number of definitions.
1
2
CHAPTER 1. FOURIER THEORY
1.2 Fourier series
Let x(t) be a real periodic signal of a period T:
x(t) = x(t + T )
Let ω = 2π/T be the fundamental frequency of that signal. Then x(t) can
be represented2 as a sum of a ﬁnite or inﬁnite number of sinusoidal signals of
harmonically related frequencies jnω plus the DC offset term3 a0/2:
x(t) =
a0
2
+
∞
(cid:88)
n=1
an cos(jnωt + ϕn)
(1.1)
The representation (1.1) is referred to as real-form Fourier series. The respective
sinusoidal terms are referred to as the harmonics or the harmonic partials of
the signal.
The set of partials contained in a signal (including the DC term) is referred
to as the signal’s spectrum. Respectively, a periodic signal can be speciﬁed by
specifying its spectrum.
Using the complex sinusoid notation the same can be rewritten as
x(t) =
∞
(cid:88)
n=−∞
Xnejnωt
(1.2)
where each harmonic term an cos(jnωt + ϕn) will be represented by a sum of
Xnejnωt and X−ne−jnωt, where Xn and X−n are mutually conjugate: Xn =
X ∗
−n. The representation (1.2) is referred to as complex-form Fourier series
and respectively we can talk of a complex spectrum. Note that we don’t have
an explicit DC oﬀset partial in this case, it is implicitly contained in the series
as the term for n = 0.
It can be easily shown that the real- and complex-form coeﬃcients are related
as
ejϕn
(n > 0)
Xn =
X0 =
an
2
a0
2
This means that intuitively we can use the absolute magnitude and the argument
of Xn (for positive-frequency terms) as the amplitudes and phases of the real
Fourier series partials.
Complex-form Fourier series can also be used to represent complex (rather
than real) periodic signals in exactly the same way, except that the equality
Xn = X ∗
−n doesn’t hold anymore.
Thus, any real periodic signal can be represented as a sum of harmonically
related real sinusoidal partials plus the DC offset. Alternatively, any periodic
signal can be represented as a sum of harmonically related complex sinusoidal
partials.
2Formally speaking, there are some restrictions on x(t). It would be sufficient to require
that x(t) is bounded and continuous, except for a finite number of discontinuous jumps per
period.
3The reason the DC offset term is notated as a0/2 and not as a0 has to do with simplifying
the math notation in other related formulas.
1.3. FOURIER INTEGRAL
3
1.3 Fourier integral
While periodic signals are representable as a sum of a countable number of
sinusoidal partials, a nonperiodic real signal can be represented4 as a sum of an
uncountable number of sinusoidal partials:
x(t) =
(cid:90) ∞
0
a(ω) cos(cid:0)ωt + ϕ(ω)(cid:1) dω
2π
(1.3)
The representation (1.3) is referred to as Fourier integral.5 The DC oﬀset term
doesn’t explicitly appear in this case.
Even though the set of partials is uncountable this time, we still refer to it
as a spectrum of the signal. Thus, while periodic signals had discrete spectra
(consisting of a set of discrete partials at the harmonically related frequencies),
nonperiodic signals have continuous spectra.
The complex-form version of Fourier integral6 is
x(t) =
(cid:90) ∞
−∞
X(ω)ejωt dω
2π
(1.4)
For real x(t) we have a Hermitian X(ω): X(ω) = X ∗(−ω), for complex x(t)
there is no such restriction. The function X(ω) is referred to as Fourier trans-
form of x(t).7
It can be easily shown that the relationship between the parameters of the
real and complex forms of Fourier transform is
X(ω) =
a(ω)
2
ejϕ(ω)
(ω > 0)
This means that intuitively we can use the absolute magnitude and the argument
of X(ω) (for positive frequencies) as the amplitudes and phases of the real
Fourier integral partials.
Thus, any timelimited signal can be represented as a sum of an uncountable
number of sinusoidal partials of infinitely small amplitudes.
4As with Fourier series, there are some restrictions on x(t). It is sufficient to require x(t) to
be absolutely integrable, bounded and continuous (except for a finite number of discontinuous
jumps per any finite range of the argument value). The most critical requirement here is
probably the absolute integrability, which is particularly fulfilled for the timelimited signals.
5The 1/2π factor is typically used to simplify the notation in the theoretical analysis
involving the computation. Intuitively, the integration is done with respect to the ordinary,
rather than circular frequency:
x(t) =
(cid:90) ∞
0
a(f ) cos(cid:0)2πf t + ϕ(f )(cid:1)df
Some texts do not use the 1/2π factor in this position, in which case it appears in other places
instead.
6A more common term for (1.4) is inverse Fourier transform. However the term inverse
Fourier transform stresses the fact that x(t) is obtained by computing the inverse of some
transform, whereas in this book we are more interested in the fact that x(t) is representable
as a combination of sinusoidal signals. The term Fourier integral better reflects this aspect.
It also suggests a similarity to the Fourier series representation.
7The notation X(ω) for Fourier transform shouldn’t be confused with the notation X(s)
for Laplace transform. Typically one can be told from the other by the semantics and the
notation of the argument. Fourier transform has a real argument, most commonly denoted as
ω. Laplace transform has a complex argument, most commonly denoted as s.
4
CHAPTER 1. FOURIER THEORY
1.4 Dirac delta function
The Dirac delta function δ(t) is intuitively deﬁned as a very high and a very
short symmetric impulse with a unit area (Fig. 1.1):
δ(t) =
(cid:40)
+∞ if t = 0
if t (cid:54)= 0
0
δ(−t) = δ(t)
(cid:90) ∞
δ(t) dt = 1
−∞
δ(t)
+∞
0
t
Figure 1.1: Dirac delta function.
Since the impulse is inﬁnitely narrow and since it has a unit area,
(cid:90) ∞
−∞
f (τ )δ(τ ) dτ = f (0)
∀f
from where it follows that a convolution of any function f (t) with δ(t) doesn’t
change f (t):
(cid:90) ∞
(f ∗ δ)(t) =
f (τ )δ(t − τ ) dτ = f (t)
Dirac delta can be used to represent Fourier series by a Fourier integral. If
−∞
we let
then
X(ω) =
∞
(cid:88)
n=−∞
2πδ(ω − nωf )Xn
∞
(cid:88)
n=−∞
Xnejnωf t =
(cid:90) ∞
−∞
X(ω)ejωt dω
2π
1.5. LAPLACE TRANSFORM
5
Notice that thereby the spectrum X(ω) is discrete, even though being formally
notated as a continuous function. From now on, we’ll not separately mention
Fourier series, assuming that Fourier integral can represent any necessary signal.
Thus, most signals can be represented as a sum of (a possibly infinite number
of ) sinusoidal partials.
1.5 Laplace transform
Let s = jω. Then, a complex-form Fourier integral can be rewritten as
x(t) =
(cid:90) +j∞
−j∞
X(s)est ds
2πj
where the integration is done in the complex plane along the straight line from
−j∞ to +j∞ (apparently X(s) is a diﬀerent function than X(ω)).8 For time-
limited signals the function X(s) can be deﬁned on the entire complex plane in
such a way that the integration can be done along any line which is parallel to
the imaginary axis:
x(t) =
(cid:90) σ+j∞
σ−j∞
X(s)est ds
2πj
(σ ∈ R)
(1.5)
In many other cases such X(s) can be deﬁned within some strip σ1 < Re s < σ2.
Such function X(s) is referred to as bilateral Laplace transform of x(t), whereas
the representation (1.5) can be referred to as Laplace integral.9 10
Notice that the complex exponential est is representable as
est = eRe s·teIm s·t
Considering eRe s·t as the amplitude of the complex sinusoid eIm s·t we notice
that est is:
- an exponentially decaying complex sinusoid if Re s < 0,
- an exponentially growing complex sinusoid if Re s > 0,
- a complex sinusoid of constant amplitude if Re s = 0.
Thus, most signals can be represented as a sum of (a possibly infinite number
of ) complex exponential partials, where the amplitude growth or decay speed of
these partials can be relatively arbitrarily chosen.
8As already mentioned, the notation X(ω) for Fourier transform shouldn’t be confused
with the notation X(s) for Laplace transform. Typically one can be told from the other by
the semantics and the notation of the argument. Fourier transform has a real argument, most
commonly denoted as ω. Laplace transform has a complex argument, most commonly denoted
as s.
9A more common term for (1.5) is inverse Laplace transform. However the term inverse
Laplace transform stresses the fact that x(t) is obtained by computing the inverse of some
transform, whereas is this book we are more interested in the fact that x(t) is representable
as a combination of exponential signals. The term Laplace integral better reflects this aspect.
10The representation of periodic signals by Laplace integral (using Dirac delta function) is
problematic for σ (cid:54)= 0. Nevertheless, we can represent them by a Laplace integral if we restrict
σ to σ = 0 (that is Re s = 0 for X(s)).
6
CHAPTER 1. FOURIER THEORY
SUMMARY
The most important conclusion of this chapter is: any signal occurring in prac-
tice can be represented as a sum of sinusoidal (real or complex) components.
The frequencies of these sinusoids can be referred to as the “frequencies con-
tained in the signal”. The full set of these sinusoids, including their amplitudes
and phases, is refereed to as the spectrum of the signal.
For complex representation, the real amplitude and phase information is
encoded in the absolute magnitude and the argument of the complex amplitudes
of the positive-frequency partials (where the absolute magnitude of the complex
amplitude is a half of the real amplitude). It is also possible to use complex
exponentials instead of sinusoids.
Chapter 2
Analog 1-pole filters
In this chapter we are going to introduce the basic analog RC-ﬁlter and use it
as an example to develop the key concepts of the analog ﬁlter analysis.
2.1 RC filter
Consider the circuit in Fig. 2.1, where the voltage x(t) is the input signal and the
capacitor voltage y(t) is the output signal. This circuit represents the simplest
1-pole lowpass filter, which we are now going to analyse.
(cid:128)(cid:129)(cid:129)(cid:129) (cid:130)(cid:240)x(t)
R
(cid:1)
y(t)
(cid:128)(cid:129)(cid:129)(cid:129)(cid:129) (cid:130)(cid:242)
(cid:255)
C
(cid:129)
(cid:254)
Figure 2.1: A simple RC lowpass ﬁlter.
Writing the equations for that circuit we have:
x = UR + UC
y = UC
UR = RI
I = ˙qC
qC = CUC
(2.1)
where UR is the resistor voltage, UC is the capacitor voltage, I is the current
through the circuit and qC is the capacitor charge. Reducing the number of
variables, we can simplify the equation system to:
or
x = RC ˙y + y
˙y =
1
RC
(x − y)
7
(2.2)
(cid:131)
(cid:133)
(cid:131)
(cid:133)
(cid:131)
(cid:133)
(cid:131)
(cid:133)
8
CHAPTER 2. ANALOG 1-POLE FILTERS
or, integrating with respect to time:
y = y(t0) +
(cid:90) t
t0
1
RC
(cid:0)x(τ ) − y(τ )(cid:1) dτ
where t0 is the initial time moment. Introducing the notation ωc = 1/RC we
have
y = y(t0) +
(cid:0)x(τ ) − y(τ )(cid:1) dτ
ωc
(2.3)
(cid:90) t
t0
We will reintroduce ωc later as the cutoff of the ﬁlter.
Notice that we didn’t factor 1/RC (or ωc) out of the integral for the case
when the value of R is varying with time. The varying R corresponds to the
varying cutoﬀ of the ﬁlter, and this situation is highly typical in the music DSP
context.1
2.2 Block diagrams
The integral equation (2.3) can be expressed in the block diagram form (Fig. 2.2).
x(t)
ωc
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82)(cid:47)
•(cid:47)
y(t)
Figure 2.2: A 1-pole RC lowpass ﬁlter in the block diagram form.
The meaning of the elements of the diagram should be intuitively clear.
The gain element (represented by a triangle) multiplies the input signal by ωc.
Notice the inverting input of the summator, denoted by “−”. The integrator
simply integrates the input signal:
output(t) = output(t0) +
(cid:90) t
t0
input(τ ) dτ
The representation of the system by the integral (rather than diﬀerential)
equation and the respective usage of the integrator element in the block diagram
has an important intuitive meaning.
Intuitively, the capacitor integrates the
current ﬂowing through it, accumulating it as its own charge:
or, equivalently
qC(t) = qC(t0) +
(cid:90) t
t0
I(τ ) dτ
UC(t) = UC(t0) +
1
C
(cid:90) t
t0
I(τ ) dτ
One can observe from Fig. 2.2 that the output signal is always trying to
“reach” the input signal. Indeed, the diﬀerence x − y is always “directed” from
1We didn’t assume the varying C because then our simplification of the equation system
doesn’t hold anymore, since ˙qC (cid:54)= C ˙UC in this case.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
2.3. TRANSFER FUNCTION
9
y to x. Since ωc > 0, the integrator will respectively increase or decrease its
output value in the respective direction. This corresponds to the fact that the
capacitor voltage in Fig. 2.1 is always trying to reach the input voltage. Thus,
the circuit works as a kind of smoother of the input signal.
2.3 Transfer function
Consider the integrator:
x(t)
(cid:82)(cid:47)
y(t)
Suppose x(t) = est (where s = jω or, possibly, another complex value). Then
y(t) = y(t0) +
(cid:90) t
t0
esτ dτ = y(t0) +
esτ (cid:12)
(cid:12)
(cid:12)
1
s
t
τ =t0
=
1
s
(cid:18)
est +
y(t0) −
(cid:19)
1
s
est0
Thus, a complex sinusoid (or exponential) est sent through an integrator comes
out as the same signal est just with a diﬀerent amplitude 1/s plus some DC
term y(t0) − est0 /s. Similarly, a signal X(s)est (where X(s) is the complex
amplitude of the signal) comes out as (X(s)/s)est plus some DC term. That
is, if we forget about the extra DC term, the integrator simply multiplies the
amplitudes of complex exponential signals est by 1/s.
Now, the good news is:
for our purposes of ﬁlter analysis we can simply
forget about the extra DC term. The reason for this is the following. Suppose
the initial time moment t0 was quite long ago (t0 (cid:28) 0). Suppose further that
the integrator is contained in a stable ﬁlter2. It can be shown that in this case
the eﬀect of the extra DC term on the output signal is negligible.3 Since the
initial state y(t0) is incorporated into the same DC term, it also means that the
eﬀect of the initial state is negligible!4
Thus, we simply write (for an integrator):
(cid:90)
esτ dτ =
1
s
est
This means that est is an eigenfunction of the integrator with the respective
eigenvalue 1/s.
Since the integrator is linear,5 not only are we able to factor X(s) out of the
integration:
(cid:90)
X(s)esτ dτ = X(s)
(cid:90)
esτ dτ =
1
s
X(s)est
2We will discuss the filter stability later, for now we’ll simply mention that we’re mostly
interested in the stable filters for the purposes of the current discussion
3We will discuss the mechanisms behind that fact when we talk about transient response.
4In practice, typically, a zero initial state is assumed. Then, particularly, in the case of
absence of the input signal, the output signal of the filter is zero from the very beginning
(rather than for t (cid:29) t0).
5The linearity here is understood in the sense of the operator linearity. An operator ^H is
linear, if
^H (λ1f1(t) + λ2f2(t)) = λ1 ^Hf1(t) + λ2 ^Hf2(t)
(cid:47)
(cid:47)
(cid:47)
10
CHAPTER 2. ANALOG 1-POLE FILTERS
but we can also apply the integration independently to all Fourier (or Laplace)
partials of an arbitrary signal x(t):
(cid:90) (cid:18)(cid:90) σ+j∞
σ−j∞
(cid:19)
X(s)esτ ds
2πj
dτ =
(cid:90) σ+j∞
(cid:18)(cid:90)
σ−j∞
X(s)esτ dτ
(cid:19) ds
2πj
=
=
(cid:90) σ+j∞
σ−j∞
X(s)
s
esτ ds
2πj
(2.4)
That is, the integrator changes the complex amplitude of each partial by a 1/s
factor.
Consider again the structure in Fig. 2.2. Assuming the input signal x(t) has
the form est we can replace the integrator by a gain element with a 1/s factor.
We symbolically reﬂect this by replacing the integrator symbol in the diagram
with the 1/s fraction (Fig. 2.3).6
ωc
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
1
s
•(cid:47)
y(t)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 2.3: A 1-pole RC lowpass ﬁlter in the block diagram form
with a 1/s notation for the integrator.
So, suppose x(t) = X(s)est and suppose we know y(t). Then the input signal
for the integrator is ωc(x − y). We now will further take for granted the knowl-
edge that y(t) will be the same signal est with some diﬀerent complex amplitude
Y (s), that is y(t) = Y (s)est (notably, this holds only if ωc is constant, that is,
if the system is time-invariant!!!)7 Then the input signal of the integrator is
ωc(X(s) − Y (s))est and the integrator simply multiplies its amplitude by 1/s.
Thus the output signal of the integrator is ωc(x − y)/s. But, on the other hand
y(t) is the output signal of the integrator, thus
or
or
from where
y(t) = ωc
x(t) − y(t)
s
Y (s)est = ωc
X(s) − Y (s)
s
est
Y (s) = ωc
X(s) − Y (s)
s
sY (s) = ωcX(s) − ωcY (s)
6Often in such cases the input and output signal notation for the block diagram is replaced
with X(s) and Y (s). Such diagram then “works” in terms of Laplace transform, the input of
the diagram is the Laplace transform X(s) of the input signal x(t), the output is respectively
the Laplace transform Y (s) of the output signal y(t). The integrators can then be seen as
s-dependent gain elements, where the gain coefficient is 1/s.
7In other words, we take for granted the fact that est is an eigenfunction of the entire
circuit.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
2.3. TRANSFER FUNCTION
11
and
Y (s) =
ωc
s + ωc
X(s)
Thus, the circuit in Fig. 2.3 (or in Fig. 2.2) simply scales the amplitude of the
input sinusoidal (or exponential) signal X(s)est by the ωc/(s + ωc) factor.
Let’s introduce the notation
Then
H(s) =
ωc
s + ωc
Y (s) = H(s)X(s)
(2.5)
(2.6)
H(s) is referred to as the transfer function of the structure in Fig. 2.3 (or
Fig. 2.2). Notice that H(s) is a complex function of a complex argument.
For an arbitrary input signal x(t) we can use the Laplace transform repre-
sentation
x(t) =
(cid:90) σ+j∞
σ−j∞
X(s)est ds
2πj
From the linearity 8 of the circuit in Fig. 2.3, it follows that the result of the
application of the circuit to a linear combination of some signals is equal to
the linear combination of the results of the application of the circuit to the
individual signals. That is, for each input signal of the form X(s)est we obtain
the output signal H(s)X(s)est. Then for an input signal which is an integral sum
of X(s)est, we obtain the output signal which is an integral sum of H(s)X(s)est.
That is
y(t) =
(cid:90) σ+j∞
σ−j∞
H(s)X(s)est ds
2πj
(2.7)
So, the circuit in Fig. 2.3 independently modiﬁes the complex amplitudes of the
sinusoidal (or exponential) partials est by the H(s) factor!
Notably, the transfer function can be introduced for any system which is
linear and time-invariant. For the diﬀerential systems, whose block diagrams
consist of integrators, summators and ﬁxed gains, the transfer function is always
a non-strictly proper 9 rational function of s. Particularly, this holds for the
electronic circuits, where the diﬀerential elements are capacitors and inductors,
since these types of elements logically perform integration (capacitors integrate
the current to obtain the voltage, while inductors integrate the voltage to obtain
the current).
It is important to realize that in the derivation of the transfer function con-
cept we used the linearity and time-invariance (the absence of parameter mod-
ulation) of the structure. If these properties do not hold, the transfer function
can’t be introduced! This means that all transfer function-based analysis holds
only in the case of fixed parameter values. In practice, if the parameters are
not changing too quickly, one can assume that they are approximately constant
8Here we again understand the linearity in the operator sense:
^H (λ1f1(t) + λ2f2(t)) = λ1 ^Hf1(t) + λ2 ^Hf2(t)
The operator here corresponds to the circuit in question: y(t) = ^Hx(t) where x(t) and y(t)
are the input and output signals of the circuit.
9A rational function is nonstrictly proper, if the order of its numerator doesn’t exceed the
order of its denominator.
12
CHAPTER 2. ANALOG 1-POLE FILTERS
during a certain time range. That is we can “approximately” apply the transfer
function concept (and the discussed later derived concepts, such as amplitude
and phase responses, poles and zeros, stability criterion etc.) if the modulation
of the parameter values is “not too fast”.
2.4 Complex impedances
Actually, we could have obtained the transfer function of the circuit in Fig. 2.1
using the concept of complex impedances.
Consider the capacitor equation:
If
I = C ˙U
I(t) = I(s)est
U (t) = U (s)est
(where I(t) and I(s) are obviously two diﬀerent functions, the same for U (t)
and U (s)), then
˙U = sU (s)est = sU (t)
and thus
that is
or
I(t) = I(s)est = C ˙U = CsU (s)est = sCU (t)
I = sCU
U =
1
sC
I
Now the latter equation looks almost like Ohm’s law for a resistor: U = RI. The
complex value 1/sC is called the complex impedance of the capacitor. The same
equation can be written in the Laplace transform form: U (s) = (1/sC)I(s).
For an inductor we have U = L ˙I and respectively, for I(t) = I(s)est and
U (t) = U (s)est we obtain U (t) = sLI(t) or U (s) = sLI(s). Thus, the complex
impedance of the inductor is sL.
Using the complex impedances as if they were resistances (which we can do,
assuming the input signal has the form X(s)est), we simply write the voltage
division formula for the circuit in in Fig. 2.1:
y(t) =
UC
UR + UC
x(t)
or, cancelling the common current factor I(t) from the numerator and the de-
nominator, we obtain the impedances instead of voltages:
y(t) =
1/sC
R + 1/sC
x(t)
from where
H(s) =
y(t)
x(t)
=
1/sC
R + 1/sC
=
1
1 + sRC
=
1/RC
s + 1/RC
=
ωc
s + ωc
which coincides with (2.5).
2.5. AMPLITUDE AND PHASE RESPONSES
13
2.5 Amplitude and phase responses
Consider again the structure in Fig. 2.3. Let x(t) be a real signal and let
x(t) =
(cid:90) σ+j∞
σ−j∞
X(s)est ds
2πj
be its Laplace integral representation. Let y(t) be the output signal (which is
obviously also real) and let
y(t) =
(cid:90) σ+j∞
σ−j∞
Y (s)est ds
2πj
be its Laplace integral representation. As we have shown, Y (s) = H(s)X(s)
where H(s) is the transfer function of the circuit.
The respective Fourier integral representation of x(t) is apparently
x(t) =
(cid:90) +∞
−∞
X(jω)ejωt dω
2π
where X(jω) is the Laplace transform X(s) evaluated at s = jω. The real
Fourier integral representation is then obtained as
ax(ω) = 2 · |X(jω)|
ϕx(ω) = arg X(jω)
For y(t) we respectively have10 11
ay(ω) = 2 · |Y (jω)| = 2 · |H(jω)X(jω)| = |H(jω)| · ax(ω)
ϕy(ω) = arg Y (jω) = arg (H(jω)X(jω)) = ϕx(ω) + arg H(jω)
(ω ≥ 0)
Thus, the amplitudes of the real sinusoidal partials are magniﬁed by the |H(jω)|
factor and their phases are shifted by arg H(jω) (ω ≥ 0). The function |H(jω)|
is referred to as the amplitude response of the circuit and the function arg H(jω)
is referred to as the phase response of the circuit. Note that both the amplitude
and the phase response are real functions of a real argument ω.
The complex-valued function H(jω) of the real argument ω is referred to
as the frequency response of the circuit. Simply put, the frequency response is
equal to the transfer function evaluated on the imaginary axis.
Since the transfer function concept works only in the linear time-invariant
case, so do the concepts of the amplitude, phase and frequency responses!
10This relationship holds only if H(jω) is Hermitian: H(jω) = H ∗(−jω). If it weren’t the
case, the Hermitian property wouldn’t hold for Y (jω) and y(t) couldn’t have been a real signal
(for a real input x(t)). Fortunately, for real systems H(jω) is always Hermitian. Particularly,
rational transfer functions H(s) with real coefficients obviously result in Hermitian H(jω).
11Formally, ω = 0 requires special treatment in case of a Dirac delta component at ω = 0
(arising particularly if the Fourier series is represented by a Fourier integral and there is a
nonzero DC offset). Nevertheless, the resulting relationship between ay(0) and ax(0) is exactly
the same as for ω > 0, that is ay(0) = H(0)ax(0). A more complicated but same argument
holds for the phase.
14
CHAPTER 2. ANALOG 1-POLE FILTERS
2.6 Lowpass filtering
Consider again the transfer function of the structure in Fig. 2.2:
H(s) =
ωc
s + ωc
The respective amplitude response is
|H(jω)| =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ωc
ωc + jω
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Apparently at ω = 0 we have H(0) = 1. On the other hand, as ω grows, the
magnitude of the denominator grows as well and the function decays to zero:
H(+j∞) = 0. This suggests the lowpass ﬁltering behavior of the circuit: it lets
the partials with frequencies ω (cid:28) ωc pass through and stops the partials with
frequencies ω (cid:29) ωc. The circuit is therefore referred to as a lowpass filter, while
the value ωc is deﬁned as the cutoff frequency of the circuit.
It is convenient to plot the amplitude response of the ﬁlter in a fully log-
arithmic scale. The amplitude gain will then be plotted in decibels, while the
frequency axis will have a uniform spacing of octaves. For H(s) = ωc/(s + ωc)
the plot looks like the one in Fig. 2.4.
Passband
Transition
band
Stopband
|H(jω)|, dB
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 2.4: Amplitude response of a 1-pole lowpass ﬁlter.
The frequency range where |H(jω)| ≈ 1 is referred to as the ﬁlter’s passband.
The frequency range where |H(jω)| ≈ 0 is referred to as the ﬁlter’s stopband.
The frequency range between the passand and the stopband where |H(jω)| is
changing from approximately 1 to approximately 0 is referred to as the ﬁlter’s
transition band.12
Notice that the plot falls oﬀ in an almost straight line as ω → ∞. Apparently,
at ω (cid:29) ωc and respectively |s| (cid:29) ωc we have H(s) ≈ ωc/s and |H(s)| ≈ ωc/ω.
This is a hyperbola in the linear scale and a straight line in a fully logarithmic
scale. If ω doubles (corresponding to a step up by one octave), the amplitude
12We introduce the concepts of pass-, stop- and transition bands only qualitatively, without
attempting to give more exact definitions of the positions of the boundaries between the bands.
2.7. CUTOFF PARAMETERIZATION
15
gain is approximately halved (that is, drops by approximately 6 decibel). We
say that this lowpass ﬁlter has a rolloff of 6dB/oct.
Another property of this ﬁlter is that the amplitude drop at the cutoﬀ is
−3dB. Indeed
|H(jωc)| =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ωc
ωc + jωc
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
1 + j
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
1
√
2
≈ −3dB
The phase response of the 1-pole lowpass is respectively
arg H(jω) = arg
ωc
ωc + jω
giving 0 at ω = 0, −π/4 at the cutoﬀ and −π/2 at ω → +∞. With phase
response plots we don’t want a logarithmic phase axis, but the logarithmic
frequency scale is usually desired. Fig. 2.5 illustrates.
arg H(jω)
0
−π/4
−π/2
ωc/8
ωc
8ωc
ω
Figure 2.5: Phase response of a 1-pole lowpass ﬁlter.
Note that the phase response is close to zero in the passband, this will be a
property encountered in most of the ﬁlters that we deal with.
2.7 Cutoff parameterization
Suppose ωc = 1. Then the lowpass transfer function (2.5) turns into
H(s) =
1
s + 1
Now perform the substitution s ← s/ωc. We obtain
H(s) =
1
s/ωc + 1
=
ωc
s + ωc
which is again our familiar transfer function of the lowpass ﬁlter.
Consider the amplitude response graph of 1/(s + 1) in a logarithmic scale.
The substitution s ← s/ωc simply shifts this graph to the left or to the right
16
CHAPTER 2. ANALOG 1-POLE FILTERS
(depending on whether ωc < 1 or ωc > 1) without changing its shape. Thus,
the variation of the cutoﬀ parameter doesn’t change the shape of the ampli-
tude response graph (Fig. 2.6), or of the phase response graph, for that matter
(Fig. 2.7).
|H(jω)|, dB
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 2.6: 1-pole lowpass ﬁlter’s amplitude response shift by a
cutoﬀ change.
arg H(jω)
0
−π/4
−π/2
ωc/8
ωc
8ωc
ω
Figure 2.7: 1-pole lowpass ﬁlter’s phase response shift by a cutoﬀ
change.
The substitution s ← s/ωc is a generic way to handle cutoﬀ parameterization
for analog ﬁlters, because it doesn’t change the response shapes. This has a nice
counterpart on the block diagram level. For all types of ﬁlters we simply visually
2.7. CUTOFF PARAMETERIZATION
17
combine an ωc gain and an integrator into a single block:13
ωc
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:82)(cid:47)
→
ωc
s
Apparently, the reason for the ωc/s notation is that this is the transfer function
of the serial connection of an ωc gain and an integrator. Alternatively, we simply
assume that the cutoﬀ gain is contained inside the integrator:
ωc
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:82)(cid:47)
→
(cid:82)(cid:47)
The internal representation of such integrator block is of course still a cutoﬀ
gain followed by an integrator. Whether the gain should precede the integrator
or follow it may depend on the details of the analog prototype circuit. In the
absence of the analog prototype it’s better to put the gain before the integrator,
because then the integrator will smooth the jumps and further artifacts arising
out of the cutoﬀ modulation. Another reason to put the cutoﬀ gain before the
integrator is that it has an important impact on the behavior of the ﬁlter in the
time-varying case. We will discuss this aspect in Section 2.16.
With the cutoﬀ gain implied inside the integrator block, the structure from
Fig. 2.2 is further simpliﬁed to the one in Fig. 2.8:
(cid:82)(cid:47)
•(cid:47)
y(t)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 2.8: A 1-pole RC lowpass ﬁlter with an implied cutoﬀ.
Unit-cutoff notation
As a further shortcut arising out of the just discussed facts, it is common to
assume ωc = 1 during the ﬁlter analysis. Particularly, the transfer function of
a 1-pole lowpass ﬁlter is often written as
H(s) =
1
s + 1
It is assumed that the reader will perform the s ← s/ωc substitution as neces-
sary.
13Notice that including the cutoff gain into the integrator makes the integrator block in-
variant to the choice of the time units:
y(t) = y(t0) +
(cid:90) t
t0
ωcx(τ ) dτ
because the product ωc dτ is invariant to the choice of the time units. This will become
important once we start building discrete-time models of filters, where we would often assume
unit sampling period.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
18
CHAPTER 2. ANALOG 1-POLE FILTERS
To illustrate the convenience of the unit cutoﬀ notation we will obtain the
explicit expression for the 1-pole lowpass phase response shown in Fig. 2.5:
arg H(jω) = arg
1
1 + jω
= − arg(1 + jω) = − arctan ω
(2.8)
The formula (2.8) explains the apparent from Fig. 2.5 symmetry (relative to the
point at ω = ωc) of the phase response in the logarithmic frequency scale, as
this symmetry is simply due to the property of the arctangent function:
arctan x + arctan
1
x
=
π
2
(2.9)
2.8 Highpass filter
If instead of the capacitor voltage in Fig. 2.1 we pick up the resistor voltage as
the output signal, we obtain the block diagram representation as in Fig. 2.9.
y(t)
•(cid:47)
(cid:82)(cid:47)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 2.9: A 1-pole highpass ﬁlter.
Obtaining the transfer function of this ﬁlter we get
or, in the unit-cutoﬀ form,
H(s) =
s
s + ωc
H(s) =
s
s + 1
It’s easy to see that H(0) = 0 and H(+j∞) = 1, whereas the biggest change in
the amplitude response occurs again around ω = ωc. Thus, we have a highpass
filter here. The amplitude response of this ﬁlter is shown in Fig. 2.10 (in the
logarithmic scale).
It’s not diﬃcult to observe or show that this response is a mirrored version of
the one in Fig. 2.4. Particularly, at ω (cid:28) ωc we have H(s) ≈ s/ωc, so when the
frequency is halved (dropped by an octave), the amplitude gain is approximately
halved as well (drops by approximately 6dB). Again, we have a 6dB/oct rolloﬀ.
The phase response of the highpass is a 90◦ shifted version of the lowpass
phase response:
arg
jω
1 + jω
=
π
2
+
1
1 + jω
Fig. 2.11 illustrates. Note that the phase response in the passband is close to
zero, same as we had for the lowpass.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
2.9. POLES AND ZEROS
19
Stopband
Transition
band
Passband
|H(jω)|, dB
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 2.10: Amplitude response of a 1-pole highpass ﬁlter.
arg H(jω)
π/2
π/4
0
ωc/8
ωc
8ωc
ω
Figure 2.11: Phase response of a 1-pole highpass ﬁlter.
2.9 Poles and zeros
Poles and zeros are two very important concepts used in connection with ﬁlters.
Now might be a good time to introduce them.
Consider the lowpass transfer function:
H(s) =
ωc
s + ωc
Apparently, this function has a pole in the complex plane at s = −ωc. Similarly,
the highpass transfer function
H(s) =
s
s + ωc
also has a pole at s = −ωc, but it also has a zero at s = 0.
Recall that the transfer functions of linear time-invariant diﬀerential systems
are nonstrictly proper rational functions of s. Writing any such function in the
20
CHAPTER 2. ANALOG 1-POLE FILTERS
multiplicative form we obtain
H(s) = g ·
Nz(cid:89)
(s − zn)
n=1
Np
(cid:89)
(s − pn)
n=1
(Np ≥ Nz ≥ 0, Np ≥ 1)
(2.10)
where Np stands for the order of the denominator, simultaneously being the
number of poles, and Nz stands for the order of the numerator, simultaneously
being the number of zeros. Thus such transfer functions always have poles and
often have zeros. The poles and zeros of transfer function (especially the poles)
play an important role in the ﬁlter analysis. For simplicity they are referred to
as the poles and zeros of the ﬁlters.
The transfer functions of real linear time-invariant diﬀerential systems have
real coeﬃcients in the numerator and denominator polynomials. Apparently,
this doesn’t prevent them from having complex poles and zeros, however, being
roots of real polynomials, those must come in complex conjugate pairs. E.g. a
transfer function with a 3rd order denominator can have either three real poles,
or one real and two complex conjugate poles.
The 1-pole lowpass and highpass ﬁlters discussed so far, each have one pole.
For that reason they are referred to as 1-pole ﬁlters. Actually, the number of
poles is always equal to the order of the ﬁlter or (which is the same) to the
number of integrators in the ﬁlter.14 Therefore it is common, instead of e.g. a
“4th-order ﬁlter” to say a “4-pole ﬁlter”.
The number of poles therefore provides one possible way of classiﬁcation of
ﬁlters. It allows to get an approximate idea of how complex the ﬁlter is and
also often allows to estimate some other ﬁlter properties without knowing lots
of extra detail. The number of zeros in the ﬁlter is usually less important and
therefore typically is not used for classiﬁcation.
Finite and infinite zeros/poles
Equation (2.10) assumes that all pn and zn are ﬁnite. However often (especially
when dealing with complex numbers) it is convenient to include the inﬁnity into
the set of “allowed” values. Respectively, if Nz < Np we will say that H(s) has
a zero of order Np − Nz at the inﬁnity. E.g. the 1-pole lowpass transfer function
has a zero of order 1 at the inﬁnity.
Conversely, if Np > Nz we could say that H(s) has a pole of order Np − Nz
at the inﬁnity, however this situation won’t occur for a transfer function of a
diﬀerential ﬁlter, since Nz cannot exceed Np.
Apparently, zeros at the inﬁnity are not a part of the explicit factoring (2.10)
and occur implicity simply due to the diﬀerence of the numerator and denomi-
nator orders. Even though they don’t show up in (2.10) they may occasionally
show up in other formulas or transformations. Thus, whether the inﬁnite zeros
(or also poles, if we deal with other rational functions) are included into the
set of zeros/poles under consideration depends on the context. Unless explicitly
14In certain singular cases, depending on the particular definition details, these numbers
might be not equal to each other.
2.9. POLES AND ZEROS
21
mentioned, usually only ﬁnite zeros and poles are meant, however the readers
are encouraged to use their own judgement in this regard.
Notice that if zeros/poles at the inﬁnity are included, the total number of
zeros is always equal to the total number of poles.
Rolloff
In (2.10) let ω → +∞. Apparently, this is the same as simply letting s → ∞
and therefore we obtain
H(s) ∼
g
sNp−Nz
(s → ∞)
as the asymptotic behavior, which means that the amplitude response rolloﬀ
speed at ω → +∞ is 6(Np − Nz)dB/oct.
Now suppose some of the zeros of H(s) are located at s = 0 and let Nz0 be
the number of such zeros. Then, for ω → 0 we obtain
H(s) ∼ g · sNz0
(s → 0)
(assuming there are no poles at s = 0). Therefore the amplitude response rolloﬀ
speed at ω → 0 is 6Nz0dB/oct. Considering that 0 ≤ Nz0 ≤ Nz ≤ Np, the
rolloﬀ speed at ω → +∞ or at ω → 0 can’t exceed 6NpdB/oct. Also, if all zeros
of a ﬁlter are at s = 0 (that is Nz0 = Nz) then the sum of the rolloﬀ speeds at
ω → 0 and ω → +∞ is exactly 6NpdB/oct.
The case of 0dB/oct rolloﬀ deserves a special attention. The 0dB/oct at
ω → +∞ occurs when Np = Nz. Respectively H(s) → g as s → ∞. Since
g must be real, it follows that so is H(∞), thus we arrive at the following
statement: if H(∞) (cid:54)= 0, then the phase response at the inﬁnity is either 0◦ or
180◦. The same statement applies for ω → 0 if Nz0 > 0, where we simply notice
that H(0) must be real due to H(jω) being Hermitian.15 The close-to-zero
phase response in the passbands of 1-pole low- and high-passes is a particular
case of this property.
Stability
The other, probably even more important property of the poles (but not zeros)
is that they determine the stability of the ﬁlter. A ﬁlter is said to be stable (or,
more exactly, BIBO-stable, where BIBO stands for “bounded input bounded
output”) if for any bounded input signal the resulting output signal is also
bounded. In comparison, unstable ﬁlters “explode”, that is, given a bounded
input signal (e.g. a signal with the amplitude not exceeding unity), the output
signal of such ﬁlter will grow indeﬁnitely.
It is known that a ﬁlter16 is stable if and only if all its poles are located
15Of course, H(0) and H(∞) are real regardless of the rolloff speeds. However zero values
of H do not have a defined phase response and can be approached from any direction on the
complex plane of values of H. On the other hand a nonzero real value H(0) or H(∞) means
that H(s) must be almost real in some neightborhood of s = 0 or s = ∞ respectively.
16More precisely a linear time-invariant system, which particularly implies fixed parameters.
This remark is actually unnecessary in the context of the current statement, since, as we
mentioned, the transfer function (and respectively the poles) are defined only for the linear
time-invariant case.
22
CHAPTER 2. ANALOG 1-POLE FILTERS
in the left complex semiplane (that is to the left of the imaginary axis).17 For
our lowpass and highpass ﬁlters this is apparently true, as long as ωc > 0. If
ωc < 0, the pole is moved to the right semiplane, the ﬁlter becomes unstable
and will “explode”. This behavior can be conveniently explained in terms of
the transient response of the ﬁlters and we will do so later.
We have established by now that if we put a sinusoidal signal through a
stable ﬁlter we will obtain an amplitude-modiﬁed and phase-shifted sinusoidal
signal of the same frequency (after the eﬀects of the initial state, if such were
initially present, disappear). In an unstable ﬁlter the eﬀects of the initial state
do not decay with time, but, on the opposite, inﬁnitely grow, thus the output
will not be the same kind of a sinusoidal signal and it doesn’t make much sense
to take of amplitude and phase responses, except maybe formally.
It is possible to obtain an intuitive understanding of the eﬀect of the pole
position on the ﬁlter stability. Consider a transfer function of the form (2.10)
and suppose all poles are initially in the left complex semiplane. Now imagine
one of the poles (let’s say p1) starts moving towards the imaginary axis. As
the pole gets closer to the axis, the (s − p1) factor in the denominator becomes
smaller around ω = Im p1 and thus the amplitude response at ω = Im p1 grows.
When p1 gets onto the axis, the amplitude response at ω = Im p1 is inﬁnitely
large (since jω = p1, we have H(jω) = H(p1) = ∞). This corresponds to the
ﬁlter getting unstable.18
It should be stressed once again, that the concepts of poles and zeros are
bound to the concept of the transfer function and thus are properly deﬁned only
if the ﬁlter’s parameters are not modulated. Sometimes one could talk about
poles and/or zeros moving with time, but this is rather a convenient way to
describe particular aspects of the change in the ﬁlter’s parameters rather than
a formally correct way. Although, if the poles and zeros are moving “slowly
enough”, this way of thinking could provide a good approximation of what’s
going on.
Cutoff
The cutoﬀ control is deﬁned as s ← s/ωc substitution. Given a transfer function
denominator factor (s − p), after the cutoﬀ substitution it becomes (s/ωc − p).
The pole associated with this factor becomes deﬁned by the equation
s/ωc − p = 0
which gives s = ωcp. This means that the pole position is changed from p to
ωcp.
Obviously, the same applies for zeros.
17The case when some of the poles are exactly on the imaginary axis, while the remaining
poles are in the left semiplane is referred to as marginally stable case. For some of the
marginally stable filters the BIBO property may still theoretically hold. However since in
practice (due to noise in analog systems or precision losses in their digital emulations) it’s
usually impossible to have the pole locations exactly defined and we will not concern ourselves
with this boundary case. One additional property of filters with all poles in the left semiplane
is that their state decays to zero in the absence of the input signal. Marginally stable filters
do not have this property.
18The reason, why the stable area is the left (and not the right) complex semiplane, is
discussed later in connection with transient response.
2.9. POLES AND ZEROS
23
Minimum and maximum phase
Consider a change to a ﬁlter’s transfer function (2.10) where we ﬂip one of
the poles or zeros symmetrically with respect to the imaginary axis.19 E.g. we
replace p1 with −p∗
1 . Apparently, such change doesn’t aﬀect the
amplitude response of the ﬁlter.
1 or z1 with −z∗
Indeed, a pole’s contribution to the amplitude response is, according to
(2.10), |jω−pn|, which is the distance from the pole pn to the point jω. However
the distance from the point −p∗
n to jω is exactly the same, thus replacing pn
with −p∗
n doesn’t change the amplitude response (Fig. 2.12). The same applies
to the situation when we change a zero from zn to −z∗
n.
Im s
jω
jω−(−p∗
n)
−p∗
n
jω−pn
pn
0
Re s
Figure 2.12: Contribution to the amplitude response from two sym-
metric points.
Flipping a pole symmetrically with respect to the imaginary axis normally
doesn’t make much sense, since this would turn a previously stable ﬁlter into
an unstable one. Even though sometimes we will be speciﬁcally interested in
using unstable ﬁlters (particularly if the ﬁlter is nonlinear), such ﬂipping is not
very useful. The point of the ﬂipping is preserving the amplitude response and,
as we mentioned, the concept of the amplitude response doesn’t really work in
the case of an unstable ﬁlter.
The situation is very diﬀerent with zeros, though. Zeros can be located in
both left and right semiplanes without endangering ﬁlter’s stability. Therefore
we could construct ﬁlters with identical amplitude responses, diﬀering only in
which of the zeros are positioned to the left and which to the right of the
imaginary axis. Even though the amplitude response is not aﬀected by this, the
phase response apparently is, and this could be the reason to chose between the
two possible positions of each (or all) of the zeros.
Qualitatively comparing the eﬀect of the positioning a zero to the left or to
the right, consider the following. A zero located to the left of the imaginary axis
makes a contribution to the phase response which varies from −90◦ to +90◦ as ω
goes from −∞ to +∞. A zero located on the right makes a contribution which
varies from +90◦ to −90◦. That is, in the ﬁrst case the phase is increasing by
19Conjugation p∗ flips the pole p symmetrically with respect to the real axis. Now if we
additionally flip the result symmetrically with respect to the origin, the result −p∗ will be
located symmetrically to p with respect to the imaginary axis.
24
CHAPTER 2. ANALOG 1-POLE FILTERS
180◦ as ω goes from −∞ to +∞, in the second case it is decreasing by 180◦.
The phase is deﬁned modulo 360◦ and generally we cannot compare two
diﬀerent values of the phase. E.g. if we have two values ϕ1 = +120◦ and
ϕ2 = −90◦, we can’t say for sure, whether ϕ1 is larger than ϕ2 by 210◦, or
whether ϕ1 is smaller than ϕ2 by 150◦. So, we only can reliably compare
continuous changes to the phase. In the case of comparing the positioning of
a zero in the left or right complex semiplane, we can say that in one case the
phase will be growing and in the other it will be decreasing.
If all zeros are in the left semiplane, then the phase will be increasing as
much as possible, the total contribution of all zeros to the phase variation on ω ∈
(−∞, +∞) being equal to +180◦ ·Nz. If all zeros are in the right semiplane, then
the phase will be decreasing as much as possible, the total contribution being
−180◦ · Nz. Assuming the ﬁlter is stable, all its poles are in the left semiplane.
The factors corresponding to the poles are contained in the denominator of the
transfer function, therefore left-semiplane poles contribute to the decreasing of
the phase, the total contribution being −180◦ · Np.
If all zeros are positioned in the left semiplane, the total phase variation is
−180◦ · (Np − Nz). If all zeros are positioned in the right semiplane, the total
phase variation is −180◦ · (Np + Nz). Since 0 ≤ Nz ≤ Np, the absolute total
phase variation in the second case is as large as possible, whereas in the ﬁrst case
it is as small as possible. For that reason the ﬁlters and/or transfer functions
having all zeros in the left semiplane are referred to as minimum phase, and
respectively the ﬁlters and/or transfer functions having all zeros in the right
semiplane are referred to as maximum phase.20
2.10 LP to HP substitution
The symmetry between the lowpass and the highpass 1-pole amplitude responses
has an algebraic explanation. The 1-pole highpass transfer function can be
obtained from the 1-pole lowpass transfer function by the LP to HP (lowpass
to highpass) substitution:
s ← 1/s
Applying the same substitution to a highpass 1-pole we obtain a lowpass 1-pole.
The name “LP to HP substitution” originates from the fact that a number of
ﬁlters are designed as lowpass ﬁlters and then are being transformed to their
highpass versions. Occasionally we will also refer to the LP to HP substitution as
LP to HP transformation, where essentially there won’t be a diﬀerence between
the two terms.
Recalling that s = jω, the respective transformation of the imaginary axis
is jω ← 1/jω or, equivalently
ω ← −1/ω
Recalling that the amplitude responses of real systems are symmetric between
positive and negative frequencies (|H(jω)| = |H(−jω)|) we can also write
ω ← 1/ω
(for amplitude response only)
20The only filter which we discussed so far which was having a zero was the 1-pole highpass.
It has the zero right on the imaginary axis and thus we can’t really say whether it’s minimum
or maximum phase or “something in between”. However later we will encounter some filters
with zeros located off the imaginary axis and in some cases the choice between minimum and
maximum phase will become really important.
2.11. MULTIMODE FILTER
25
Taking the logarithm of both sides gives:
log ω ← − log ω
(for amplitude response only)
Thus, the amplitude response is ﬂipped around ω = 1 in the logarithmic scale.
The LP to HP substitutions also transforms the ﬁlter’s poles and zeros by
the same formula:
s(cid:48) = 1/s
where we substitute pole and zero positions for s. Clearly this transformation
maps the complex values in the left semiplane to the values in the left semiplane
and the values in the right semiplane to the right semiplane. Thus, the LP to
HP substitution exactly preserves the stability of the ﬁlters.
Notice that thereby a zero occuring at s = 0 will be transformed into a zero
at the inﬁnity and vice versa (this is the main example of why we sometimes
need to consider zeros at the inﬁnity). Particularly, the zero at s = ∞ of the
1-pole lowpass ﬁlter is transformed into the zero at s = 0 of the 1-pole highpass
ﬁlter.
The LP to HP substitution can be performed not only algebraically (on a
transfer function), but also directly on a block diagram, if we allow the usage
of diﬀerentiators. Since the diﬀerentiator’s transfer function is H(s) = s, re-
placing all integrators by diﬀerentiators will eﬀectively perform the 1/s ← s
substitution, which apparently is the same as the s ← 1/s substitution. Shall
the usage of the diﬀerentiators be forbidden, it might still be possible to convert
diﬀerentiation to the integration by analytical transformations of the equations
expressed by the block diagram.
2.11 Multimode filter
Actually, we can pick up the lowpass and highpass signals simultaneously from
the same structure (Fig. 2.13). This is referred to as a multimode filter.
yHP(t)
•(cid:47)
(cid:82)(cid:47)
•(cid:47)
yLP(t)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 2.13: A 1-pole multimode ﬁlter.
It’s easy to observe that yLP(t) + yHP(t) = x(t), that is the input signal is
split by the ﬁlter into the lowpass and highpass components. In the transfer
function form this corresponds to
HLP(s) + HHP(s) =
ωc
s + ωc
+
s
s + ωc
= 1
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
26
CHAPTER 2. ANALOG 1-POLE FILTERS
The multimode ﬁlter can be used to implement almost any 1st-order stable
diﬀerential ﬁlter by simply mixing its outputs. Indeed, let
H(s) =
b1s + b0
s + a0
where we assume a0 (cid:54)= 0.21 Letting ωc = a0 we obtain
H(s) =
b1s + b0
s + ωc
= b1
s
s + ωc
+
b0
ωc
·
ωc
s + ωc
= b1HHP(s) +
(cid:19)
(cid:18) b0
ωc
HLP(s)
Thus we simply need to set the ﬁlter’s cutoﬀ to a0 and take the sum
y = b1yHP(t) +
(cid:19)
(cid:18) b0
ωc
yLP(t)
as the output signal.
Normally (although not always) we are interested in the ﬁlters whose re-
sponses do not change the shape under cutoﬀ variation, but are solely shifted to
the left or to the right in the logarithmic frequency scale. Such modal mixtures
are easiest written in the unit-cutoﬀ form:
H(s) =
b1s + b0
s + 1
= b1
s
s + 1
+ b0
1
s + 1
where we actually imply
H(s) =
b1(s/ωc) + b0
(s/ωc) + 1
Respectively, the mixing coeﬃcients become independent of the cutoﬀ:
y = b1yHP(t) + b0yLP(t)
Fig. 2.14 illustrates.
•(cid:47)
(cid:82)(cid:47)
•(cid:47)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
b1
b0
y(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 2.14: Modal mixture with 1-pole multimode ﬁlter imple-
menting H(s) = (b1s + b0)/(s + 1).
21If a0 = 0, it means that the pole of the filter is exactly at s = 0, which is a rather exotic
situation to begin with. Even then, chances are that b0 = 0 as well, in which case the filter
either reduces to a multiplication by a gain (H(s) = b1) or, if the coefficients vary, we can
take the limiting value of b0/ωc in the respective formulas.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
2.12. SHELVING FILTERS
27
2.12 Shelving filters
By adding/subtracting the lowpass-ﬁltered signal to/from the unmodiﬁed input
signal one can build a low-shelving ﬁlter:
The transfer function of the low-shelving ﬁlter is respectively:
y(t) = x(t) + K · yLP(t)
H(s) = 1 + K
1
s + 1
The amplitude response is plotted Fig. 2.15. Typically K ≥ −1. At K = 0 the
signal is unchanged. At K = −1 the ﬁlter turns into a highpass.
|H(jω)|, dB
+6
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 2.15: Amplitude response of a 1-pole low-shelving ﬁlter (for
various K).
The high-shelving ﬁlter is built in a similar way:
and
y(t) = x(t) + K · yHP(t)
H(s) = 1 + K
s
s + 1
The amplitude response is plotted Fig. 2.16.
Actually, it would be more convenient to specify with the fact that the
amplitude boost or drop for the “shelf” in decibels. It’s not diﬃcult to realize
that the decibel boost is
Indeed, e.g. for the low-shelving ﬁlter at ω = 0 (that is s = 0) we have22
GdB = 20 log10(K + 1)
H(0) = 1 + K
22H(0) = 1 + K is not a fully trivial result here. We have it only because the lowpass filter
doesn’t change the signal’s phase at ω = 0. If instead it had e.g. inverted the phase, then we
would have obtained 1 − K here.
CHAPTER 2. ANALOG 1-POLE FILTERS
28
|H(jω)|, dB
+6
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 2.16: Amplitude response of a 1-pole high-shelving ﬁlter
(for various K).
We also obtain H(+j∞) = 1 + K for the high-shelving ﬁlter.
There is, however, a problem with the shelving ﬁlters built this way. Even
though these ﬁlters do work as a shelving ﬁlters, the deﬁnition of the cutoﬀ at
ω = 1 for such ﬁlters is not really convenient. Indeed, looking at the amplitude
response graphs in Figs. 2.15 and 2.16 we would rather wish to have the cutoﬀ
point positioned exactly at the middle of the respective slopes. A solution to
this problem will be described in Chapter 10.
2.13 Allpass filter
The ideas explained in the discussion of the minimum and maximum phase
properties of a ﬁlter can be used to construct an allpass ﬂter. Since in this
chapter our focus is on 1-poles, we will construct a 1-pole allpass but the same
approach generalizes to an allpass of an arbitrary order.
Starting with an identity 1-pole transfer function
H(s) =
s + 1
s + 1
≡ 1
and noticing that this is a minimum phase ﬁlter, let’s ﬂip its zero symmetrically
with respect to the imaginary axis, thereby turning it into a maximum phase
ﬁlter:
H(s) =
s − 1
s + 1
(2.11)
As we discussed before, such change can’t aﬀect the amplitude response of the
ﬁlter and thus
|H(jω)| =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
jω − 1
jω + 1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≡ 1
On the other hand the phase response has changed from arg H(jω) ≡ 0 to some
decreasing function of ω (Fig. 2.17).
2.13. ALLPASS FILTER
29
arg H(jω)
π/2
0
−π/2
ωc/8
ωc
8ωc
ω
Figure 2.17: Phase response of the 1-pole allpass ﬁlter (2.11).
The ﬁlters whose purpose is to aﬀect only the phase of the signal, not touch-
ing the amplitude part at all, are referred to as allpass filters.23 Obviously,
(2.11) is a 1-pole allpass. However it’s not the only possible one.
Apparently, multiplying a transfer function by −1 doesn’t change the am-
plitude response. Therefore, multiplying the right-hand side of (2.11) by −1 we
obtain another 1-pole allpass.
H(s) =
1 − s
1 + s
(2.12)
This one diﬀers from the one in (2.11) by the fact that the phase response
of (2.12) is changing from 0 to −π (Fig. 2.18) whereas the phase of (2.11) is
changing from +π/2 to −π/2. Often it’s more convenient, if the allpass ﬁlter’s
phase response starts at zero, which could be a reason for preferring (2.12) over
(2.11).
arg H(jω)
0
−π/2
−π
ωc/8
ωc
8ωc
ω
Figure 2.18: Phase response of the 1-pole allpass ﬁlter (2.12).
23The most common VA use for the allpass filters is probably in phasers.
30
CHAPTER 2. ANALOG 1-POLE FILTERS
Notably, the phase response of the allpass (2.12) (Fig. 2.18) is the doubled
phase response of the 1-pole lowpass (Fig. 2.7). It is easy to realize that the
reason for this is that the numerator (1−s) contributes exactly the same amount
to the phase response as the denominator (1 + s):
arg
1 − jω
1 + jω
= arg(1 − jω) − arg(1 + jω) = −2 arg(1 + jω) = −2 arctan ω (2.13)
where the symmetry of the phase response in Fig. 2.18 is due to (2.9).
Noticing that
H(s) =
1 − s
1 + s
=
1
1 + s
−
s
1 + s
= HLP(s) − HHP(s)
we ﬁnd that the allpass (2.12) can be obtained by simply subtracting the high-
pass output from the lowpass output of the multimode ﬁlter, the opposite order
of subtraction creating the (2.11) allpass.
As mentioned earlier, the same approach can in principle be used to construct
arbitrary allpasses. Starting with a stable ﬁlter
H(s) =
N
(cid:89)
(s − pn)
n=1
N
(cid:89)
(s − pn)
n=1
≡ 1
we ﬂip all zeros over to the right complex semiplane, turning H(s) into a max-
imum phase ﬁlter:
H(s) =
N
(cid:89)
(s + p∗
n)
n=1
N
(cid:89)
(s − pn)
n=1
where we might invert the result to make sure that H(0) = 1
H(s) = (−1)N ·
N
(cid:89)
(s + p∗
n)
n=1
N
(cid:89)
(s − pn)
n=1
In practice, however, high order allpasses are often created by simply connecting
several of 1- and 2-pole allpasses in series.
2.14 Transposed multimode filter
We could apply the transposition to the block diagram in Fig. 2.13. The trans-
position process is deﬁned as reverting the direction of all signal ﬂow, where
2.14. TRANSPOSED MULTIMODE FILTER
31
forks turn into summators and vice versa (Fig. 2.19).24 The transposition keeps
the transfer function relationship within each pair of an input and an output
(where the input becomes the output and vice versa). Thus in Fig. 2.19 we have
a lowpass and a highpass input and a single output.
y(t)
•
xHP(t)
(cid:82)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
xLP(t)
Figure 2.19: A 1-pole transposed multimode ﬁlter.
Looking carefully at Fig. 2.19 we would notice that the lowpass part of the
structure is fully identical to the non-transposed lowpass. The highpass part
diﬀers solely by the relative order of the signal inversion and the integrator in the
feedback loop. It might seem therefore that the ability to accept multiple inputs
with diﬀerent corresponding transfer functions is the only essential diﬀerence of
the transposed ﬁlter from the non-transposed one.
This is not fully true, if time-varying usage of the ﬁlter is concerned. Note
that if the modal mixture is involved, the gains corresponding to the transfer
function numerator coeﬃcients will precede the ﬁlter (Fig. 2.20). Thus, if the
mixing coeﬃcients vary with time, the coeﬃcient variations will be smoothed
down by the ﬁlter (especially the lowpass coeﬃcient, but also to an extent the
highpass one), in a similar way to how the cutoﬀ placement prior to the inte-
grator helps to smooth down cutoﬀ variations. Compare Fig. 2.20 to Fig. 2.14.
x(t)
•(cid:47)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
b1
b0
(cid:82)(cid:47)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
y(t)
Figure 2.20: 1-pole transposed multimode ﬁlter implementing
H(s) = (b1s + b0)/(s + 1).
One particularly useful case of the transposed 1-pole’s multi-input feature, is
feedback shaping. Imagine we are mixing an input signal xin(t) with a feedback
signal xfbk(t), and we wish to ﬁlter each one of those by a 1-pole ﬁlter, and the
cutoﬀs of these 1-pole ﬁlters are identical. That is, the transfer functions of those
ﬁlters share a common denominator. Then we could use a single transposed 1-
24The inverting input of the summator in the transposed version was obtained from the
respective inverting input of the summator in the non-transposed version as follows. First the
inverting input is replaced by an explicit inverting gain element (gain factor −1), then the
transposition is performed, then the inverting gain is merged into the new summator.
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
32
CHAPTER 2. ANALOG 1-POLE FILTERS
pole multimode ﬁlter as in Fig. 2.21. The mixing coeﬃcients A, B, C and D
deﬁne the numerators of the respective two transfer functions.
xin(t)
•
A
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
B
TMMF
y(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP
HP
+
(cid:13)(cid:13)(cid:13)(cid:49)(cid:49)(cid:49)(cid:79)
C
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)(cid:49)(cid:49)(cid:49)
D
•
xfbk(t)
Figure 2.21: A transposed multimode ﬁlter (TMMF) used for feed-
back signal mixing.
2.15 Transient response
For a 1-pole ﬁlter it is not diﬃcult to obtain an explicit expression for the ﬁlter’s
output, given the ﬁlter’s input. Indeed, let’s rewrite (2.2) in terms of ωc:
˙y(t) = ωc · (x(t) − y(t))
We can further express ωc in terms of the system pole p = −ωc:
˙y = p · (y − x)
(2.14)
Writing the system equation in terms of the pole will prove to be useful, when
we reuse the results obtained in this section in later chapters of the book.
Rewriting (2.14) in a slightly diﬀerent way we obtain
˙y − py = −px
(2.15)
Multiplying both sides by e−pt:
e−pt ˙y − pe−pty = −pe−ptx
and noticing that the left-hand size is a derivative of e−pty(t) we have
d
dt
(e−pty) = −pe−ptx
Integrating both sides from 0 to t with respect to t:
e−pty(t) − y(0) = −p
(cid:90) t
0
e−pτ x(τ ) dτ
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
2.15. TRANSIENT RESPONSE
33
e−pty(t) = y(0) − p
(cid:90) t
0
e−pτ x(τ ) dτ
Multiplying both sides by ept:
y(t) = y(0)ept − p
(cid:90) t
0
ep(t−τ )x(τ ) dτ
(2.16)
we obtain a formula which allows us to explicitly compute the ﬁlter’s output,
knowing the ﬁlter’s input and initial state.
Now suppose x(t) = X(s)est. Then (2.16) implies
y(t) = y(0)ept − peptX(s)
(cid:90) t
e(s−p)τ dτ =
= y(0)ept − peptX(s) ·
= y(0)ept − peptX(s) ·
t
(cid:12)
(cid:12)
(cid:12)
(cid:12)
0
e(s−p)τ
s − p
τ =0
e(s−p)t − 1
s − p
=
=
(cid:18)
=
y(0) −
(cid:19)
X(s)
ept +
−p
s − p
−p
s − p
X(s)est =
= (y(0) − H(s)X(s)) ept + H(s)X(s)est =
= (cid:0)y(0) − H(s)x(0)(cid:1)ept + H(s)x(t) =
= H(s)x(t) + (cid:0)y(0) − H(s)x(0)(cid:1)ept
(2.17)
where
H(s) =
−p
s − p
=
ωc
s + ωc
is the ﬁlter’s transfer function.
Now look at the last expression of (2.17). The ﬁrst term corresponds to
(2.6). This is the output of the ﬁlter which we would expect according to our
previous discussion. The second term looks new, but, since normally p < 0, this
term is exponentially decaying with time. Thus at some moment the second
term becomes negligible and only the ﬁrst term remains. We say that the ﬁlter
has entered a steady state and refer to H(s)x(t) as the steady-state response of
the ﬁlter (for the complex exponential input signal x(t) = X(s)est). The other
term, which is exponentially decaying and exists only for a certain period of
time is called the transient response.
Now we would like to analyse the general case, when the input signal is a
sum of such exponential signals:
x(t) =
(cid:90) σ+j∞
σ−j∞
X(s)est ds
2πj
First, assuming y(0) = 0 and using the linearity of (2.16), we apply (2.17)
independently to each partial X(s)est of x(t), obtaining
(cid:90)
y(t) =
H(s)X(s)est ds
2πj
(cid:90)
− ept
H(s)X(s)
ds
2πj
(2.18)
Again, the ﬁrst term corresponds to (2.6) and is the steady-state response.
Respectively, the second term, which is exponentially decaying (notice that the
34
CHAPTER 2. ANALOG 1-POLE FILTERS
integral in the second term is simply a constant, not changing with t), is the
transient response.
Comparing (2.18) to (2.16) we can realize that the diﬀerence between y(0) =
0 and y(0) (cid:54)= 0 is simply the addition of the term y(0)ept. Thus we simply add
the missing term to (2.18) obtaining
(cid:90)
y(t) =
H(s)X(s)est ds
2πj
(cid:18)
+
y(0) −
(cid:90)
H(s)X(s)
(cid:19)
ds
2πj
· ept =
= ys(t) + (y(0) − ys(0)) · ept = ys(t) + yt(t)
(2.19)
where
(cid:90)
ys(t) =
H(s)X(s)est ds
2πj
yt(t) = (y(0) − ys(0)) · ept
(2.20a)
(2.20b)
are the steady-state and transient responses.
Looking at (2.20) we can give the following interpretation to the steady-state
and transient responses. Steady-state response is the “expected” response of the
ﬁlter in terms of the spectrum of x(t) and the transfer function H(s), this is the
part of the ﬁlter’s output that we have been exclusively dealing with until now
and this is the part that we will continue being interested in most of the time.
Particularly, this is the part of the ﬁlter’s output for which the terms amplitude
and phase response are making sense. However, at the initial time moment the
ﬁlter’s output will usually not match the expected response (y(0) (cid:54)= ys(0)), since
the initial ﬁlter state may be arbitrary. Even if y(0) = 0, we still usually have
ys(0) (cid:54)= 0. But the integrator’s state cannot change abruptly25 and therefore
there will be a diﬀerence between the actual and “expected” outputs. This
diﬀerence however decays exponentially as ept. This exponentially decaying
part, caused by a discrepancy between the “expected” output and the actual
state of the ﬁlter is the transient response (Fig. 2.22).
The origin of the term “steady-state response” should be obvious by now.
As for the term “transient response” things might be a bit more subtle, but
actually it’s also quite simple.
Suppose the input of the ﬁlter is receiving a steady signal, e.g. a periodic
wave and suppose the ﬁlter has entered the steady state by t = t0 (meaning
that the transient response became negligibly small). Suppose that at t = t0 a
transient occurs in the input signal: the ﬁlter’s input suddenly changes to some
other steady signal, e.g. it has a new waveform, or amplitude, or frequency, or
all of that. This means that at this moment the deﬁnition of the steady state
also changes and the ﬁlter’s output does no longer match the “expected” signal.
Thus, at t = t0 we suddenly have ys(t) (cid:54)= y(t) and a decaying transient response
impulse is generated. The transient response turns a sudden jump, which would
have occured in the ﬁlter’s output due to the switching of the input signal, into
a continuous exponential “crossfade”.
25Assuming the input signal is finite. In theoretical filter analysis sometimes infinitely large
input signals (most commonly x(t) = δ(t)) are used. In such cases the filter state may change
abruptly (and this is the whole purposes of using input signals such as δ(t).
2.15. TRANSIENT RESPONSE
35
yt(t)
y(0)−ys(0)
0
t
Figure 2.22: Transient response of a 1-pole lowpass ﬁlter (dashed
line depicts the unstable case).
Highpass transient response
For a highpass, since yHP(t) = x(t) − yLP(t) = x(t) − y, equation (2.19) converts
into
yHP(t) = x(t) − (ys(t) + yt(t)) = (x(t) − ys(t)) − y(t) = yHPs(t) + yHPt(t)
where the highpass steady-state response is
yHPs(t) = x(t) − ys(t) = x(t) −
(cid:90)
H(s)X(s)est ds
2πj
=
(cid:90)
(cid:90)
=
=
(cid:90)
−
X(s)est ds
2πj
(1 − H(s))X(s)est ds
2πj
H(s)X(s)est ds
2πj
(cid:90)
=
=
HHP(s)X(s)est ds
2πj
and the highpass transient response is
yHPt(t) = −yt(t) = − (y(0) − ys(0)) · ept =
= ((x(t) − y(0)) − (x(t) − ys(0))) · ept = (yHP(0) − yHPs(0)) · ept
That is we are having the same kind of exponentially decaying discrepancy
between the output signal and the steady-state signal, where the exponent ept
itself is identical to the one in the lowpass transient response.
Poles and stability
At this point we could get a ﬁrst hint at the mechanism behind the relationship
between the ﬁlter poles and ﬁlter stability. The transient response of the 1-pole
ﬁlter decays as ept (this means it it takes longer time to reach a steady state
at lower cutoﬀs). However, if p > 0, the transient response doesn’t decay, but
instead inﬁnitely grows with time (as shown by the dashed line in Fig. 2.22),
and we say that the ﬁlter “explodes”.
36
CHAPTER 2. ANALOG 1-POLE FILTERS
At p = 0 the 1-pole lowpass ﬁlter doesn’t explode, but stays at the same
value (since p = 0 implies ˙y = 0 for this ﬁlter), corresponding to the marginally
stable case. But this actually happens because of the speciﬁc form of the transfer
function we are using: H(s) = −p/(s − p). Thus, p = 0 simultaneously implies
a zero total gain, which prevents the explosion.
However, in a more general case, a marginally stable 1-pole ﬁlter can explode.
We are going to discuss this using Jordan 1-poles.
Steady state
The steady-state response is actually not a precisely deﬁned concept, as it has
a subjective element. A bit earler we have been analysing the situation of an
abrupt change of the input signal causing a discrepancy between the steady-
state response and the actual output signal, this discrepancy being responsible
for the appearance of the transient response term. However we don’t have to
understand this case as an abrupt change of the input signal. Instead we could
consider the input signal over the entire time duration as a whole incorporating
the abrupt change as an integral part of the signal. E.g. instead of considering
the input signal changing from sin t to 2 sin(4t + 1) at some moment t = t0, we
would formally consider a non-periodic signal x(t) deﬁned as
x(t) =
(cid:40)
sin t
2 sin(4t + 1)
if t < t0
if t ≥ t0
In that sense there would be just some non-periodic input signal x(t) which
doesn’t change to some other input signal. Then we would have a diﬀerent
deﬁnition of the signal’s spectrum, the spectrum being constant all the time,
rather than suddenly changing at t = t0, which would mean there is no transient
at t = t0. Thus we would also be having a diﬀerent deﬁnition of the steady
state response, which wouldn’t have a discrepancy with the ﬁlter’s output signal
at t = t0 either. Therefore there wouldn’t be a transient response impulse
appearing at t = t0. Thus, the deﬁnition of the input signal has a subjective
element, which results in the same subjectivity of the deﬁnition of the steady-
state response signal.
The formal deﬁnition of the steady-state response is the formula (2.20a).
Careful readers who are also familiar with Laplace transform theory might be
by now asking themselves the question, whether the multiplication of X(s) by
H(s) has any eﬀect on the region of convergence and, if yes, what are the
implications of this eﬀect. Surprisingly, this question has a connection to the
subjectivity of the steady-state response.
The thing is that due to the subjectivity of the steady-state response, we
don’t care too much about what the Laplace integral in (2.20a) converges to.
Most importantly, it does converge. And normally it will converge for any Re s
(with some additional care being taken in evaluation of (2.20a) if the integration
path Re s = const contains some poles). It’s just that as we horizontally shift
the integration path Re s = const, and this path is thereby traversing through
the poles of H(s)X(s), the integral (2.20a) will converge to some other function,
but it will converge nevertheless. In fact we even cannot say what the Laplace
transform’s region of convergence for (2.20a) is. We could say what the region
of convergence is for X(s), since we have the original signal x(t), but we cannot
2.15. TRANSIENT RESPONSE
37
say what is the region of convergence for H(s)X(s), since its original signal
would be ys(t) and we don’t have an exact deﬁnition of the latter.
Therefore we actually could choose which of the diﬀerent resulting signals
delivered by (2.20a) (for diﬀerent choices of the “region of convergence” of
H(s)X(s)) to take as the steady-state response. For one, we probably shouldn’t
go outside of the region of convergence of X(s), since otherwise we would have
a diﬀerent input signal and the result would be simply wrong. However, other
than that we have total freedom. Given that all poles (or actually, the only pole,
since so far H(s) is a 1-pole) of H(s) are located to the left of the imaginary axis
(which is the case for the stable ﬁlter), it probably makes most sense to choose
the range of Re s containing the imaginary axis as the region of convergence of
H(s)X(s), because H(s) evaluated on the imaginary axis gives the amplitude
and phase responses and thus the steady-state response deﬁnition will be in
agreement with amplitude and phase responses.
What shall we do, however, if Re p > 0 (where p is the pole of H(s)), that
is H(s) is unstable? First, let’s notice that as we change the integration path
in (2.20a) from Re s < p to Re s > p the integral (2.20a) changes exactly by the
residue of H(s)X(s)est at s = p (it directly follows from the residue theorem).
But this residue is simply
Res
s=p
(cid:0)H(s)X(s)est(cid:1) = Res
s=p
(cid:18) a
s − p
(cid:19)
· X(s)est
= aX(p)ept
(where a = −p)
Therefore the steady state response ys(t) deﬁned by the integral (2.20a) is chang-
ing by a term of the form aX(p)ept, which is then added to or subtracted from
the transient response to keep the sum y(t) unchanged. But the transient re-
sponse already consists of a similar term, just with a diﬀerent amplitude. Thus
the change from Re s < p to Re s > p simply changes the transient response’s
amplitude. Therefore, there is not much diﬀerence, whether in the unstable case
we evaluate (2.20a) for e.g. Re s = 0 or for some Re s > p. It might therefore
be simply more consistent to always evaluate it for Re s = 0, regardless of the
stability, but, as we just explained, this is not really a must.
Note that thereby, even though amplitude and phase responses make no
sense for unstable ﬁlters, the equation (2.20a) still applies, therefore the transfer
function H(s) itself makes total sense regardless of the ﬁlter stability.
Jordan 1-pole
For the purposes of theoretical analysis of systems of higher order it is sometimes
helpful to use 1-poles where the input signal is not multiplied by the cutoﬀ −p:
˙y = py + x
(2.21)
(Fig. 2.23). We also allow p to take complex values. Such 1-poles are the
building elements of the state-space diagonal forms and of the so-called Jordan
chains. For that reason we will refer to (2.21) as a Jordan 1-pole.
One could argue that there is not much diﬀerence between the 1-pole equa-
tions (2.14) and (2.21) and respectively between Fig. 2.2 and Fig. 2.23, since
one could always represent the Jordan 1-pole via the ordinary 1-pole lowpass
by dividing the input signal of the latter by the cutoﬀ. Also it would be no
problem to allow p to take complex values in (2.14). This approach however
38
CHAPTER 2. ANALOG 1-POLE FILTERS
•(cid:47)
y(t)
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82)(cid:47)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
p
Figure 2.23: Jordan 1-pole. Note that the integrator is not sup-
posed to internally contain the implicit cutoﬀ gain!
won’t work if p = 0. For that reason, in certain cases it is more conveninent to
use a Jordan 1-pole instead.
Changing from (2.14) to (2.21) eﬀectively takes away the −p coeﬃcient in
front of x from all formulas derived from (2.14). Particularly, (2.16) turns into
y(t) = y(0)ept +
(cid:90) t
0
ep(t−τ )x(τ ) dτ
(2.22)
and (2.17) turns into
y(t) = y(0)ept + eptX(s)
(cid:18)
=
y(0) −
1
s − p
X(s)
(cid:90) t
0
(cid:19)
e(s−p)τ dτ =
ept +
1
s − p
X(s)est
(2.23)
where have
and
ys(t) =
1
s − p
X(s)est = H(s)x(t)
H(s) =
1
s − p
From this point on we’ll continue the transient response analysis in terms of
Jordan 1-poles. The results can be always converted to ordinary 1-poles by
multiplying the input signal by −p.
Hitting the pole
Suppose the input signal of the ﬁlter is x(t) = X(p)ept (where X(p) is the com-
plex amplitude). In this case (2.23) cannot be applied, because the denominator
s − p turns to zero and we have to compute the result diﬀerently. From (2.22)
we obtain
y(t) = y(0)ept + X(p)
(cid:90) t
0
ep(t−τ )epτ dτ = y(0)ept + X(p)tept
(2.24)
Now there doesn’t really seem to be a steady-state component in (2.24). The
second term might look a bit like the steady-state response. Clearly it’s not
having the usual steady-state response form H(p)X(p)ept, but that would be
impossible since H(p) = ∞. Not only that, it’s not even proportional to the
input signal (or, more precisely, the proportionality coeﬃcient is equal to t,
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
2.15. TRANSIENT RESPONSE
39
thereby changing with time), thus not really looking like any kind of a steady
state. The ﬁrst term doesn’t work as a steady-state response either, since it
depends on the initial state of the system.
Since the idea of the steady-state response is, to an extent, subjective, it
means the output which we expect from the system independently of the initial
state, we could formally introduce
ys(t) = X(p)tept
as the steady-state response in this case, thereby further transforming (2.24) as
y(t) = y(0)ept + Xtept = (y(0) − ys(0))ept + ys(t) = yt(t) + ys(t)
The beneﬁt of this choice is that the transient response still consists of a single
ept partial. The other option is letting
ys(t) ≡ 0
which means that (2.24) entirely consists of the transient response.
In either case, the problem is that as s → p in (2.23), the steady-state
response deﬁned by ys(t) = H(s)X(s)est becomes inﬁnitely large and we need
to switch to a diﬀerent steady-state response deﬁnition. Note, that there is
no jump in the output signal y(t), nor does y(t) become inﬁnitely large. The
switching is occuring only in the way how we separate y(t) into steady-state and
transient parts.
We could further illustrate what is going on by a detailed evaluation of (2.23)
at s → p. The part which needs special attention is the integral of e(s−p)τ :
(cid:90) t
0
lim
s→p
e(s−p)τ dτ = lim
s→p
e(s−p)τ
s − p
(cid:12)
(cid:12)
(cid:12)
(cid:12)
t
τ =0
= lim
s→p
e(s−p)t − 1
s − p
= t
and thus y(t) = y(0)ept + X(p)tept, which matches our previous result.
In the particular case of p = 0 the equation (2.24) turns into
y(t) = y(0) + X(0)t
thus the marginally stable system to which Fig. 2.23 turns at p = 0 explodes if
s = 0, that is if x(t) is constant.26
Jordan chains
Fur the purposes of further analysis of transient responses of systems of higher
orders it will be instructive to analyse the transient response generated by serial
chains of identical Jordan 1-poles, referred to as Jordan chains (Fig. 2.24).
Given a complex exponential input signal x(t) = X(s)est, the output of the
ﬁrst 1-pole will have the form
y1(t) = ys1(t) + yt1(t) = H1(s)X(s)est + (y1(0) − H1(s)X(s))ept
where
H1(s) =
1
s − p
26It’s easy to see that this system is simply an integrator.
40
CHAPTER 2. ANALOG 1-POLE FILTERS
1
s−p
1
s−p
· · ·
1
s−p
Figure 2.24: Jordan chain
The output of the second 1-pole will be therefore
y2(t) = H 2
1 (s)X(s)est + (y1(0) − H1(s)X(s))tept + (y2(0) − H 2
1 (s)X(s))ept
where we have used (2.23) and (2.24).
Before we obtain the output of the further 1-poles we ﬁrst need to apply
(2.22) to x(t) = Xtnept yielding
y(t) = y(0)ept + X
tn+1
(n + 1)!
ept
Then
y3(t) = H 3
1 (s)X(s)est + (y1(0) − H1(s)X(s))
t2
2
ept+
+ (y2(0) − H 2
1 (s)X(s))tept + (y3(0) − H 3
1 (s)X(s))ept
and, continuing in the same fashion, we obtain for the n-th 1-pole:
yn(t) = H n
1 (s)X(s)est +
n−1
(cid:88)
(yn−ν(0) − H n−ν
1
ν=0
(s)X(s))
tν
ν!
ept
(2.25)
1 (s)X(s)est is the steady-state response whereas
Apparently the ﬁrst term H n
the remaining terms are the transient response. In principle, one could argue,
that treating the remaining terms as transient response can be questioned, since
we have some ambiguity in the deﬁnition of the steady-state response of the 1-
poles if their poles are hit by their input signals. However, while this argument
might be valid in respect to individual 1-poles, from the point of view of the
entire Jordan chain all terms tνept/ν! are arising out of the mismatch between
the chain’s internal state and the input signal, therefore we should stick to the
1 (s)X(s)est. This also matches the fact that
steady-state response deﬁnition H n
the transfer function of the entire Jordan chain is H N
1 (s) = 1/(s − p)N , where
N is the number of 1-poles in the chain.
2.16 Cutoff as time scaling
Almost all analysis of the ﬁlters which we have done so far applies only to
linear time-invariant ﬁlters.
In practice, however, ﬁlter parameters are often
being modulated. This means that the ﬁlters no longer have the time-invariant
property and our analysis does not really apply.
In general, the analysis of
time-varying ﬁlters is a pretty complicated problem. However, in the speciﬁc
(but pretty common) case of cutoﬀ modulation there is actually a way to apply
the results obtained for the time-invariant case.
Imagine a system of an arbitrary order (therefore, containing one or more
integrators). Suppose the cutoﬀ gain elements are always preceding the integra-
tors and suppose all integrators have the same cutoﬀ gain (that is, these gains
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
2.16. CUTOFF AS TIME SCALING
41
always have the same value, even when modulated). For each such integrator,
given its input signal (which we denote as x(t)), its output signal is deﬁned by
y(t) = y(t0) +
(cid:90) t
t0
ωcx(τ ) dτ
If cutoﬀs are synchronously varying with time, we could reﬂect this explicitly:
y(t) = y(t0) +
(cid:90) t
t0
ωc(τ )x(τ ) dτ
(2.26)
We would like to introduce a new time variable ˜τ deﬁned by
and respectively write
d˜τ = ωc(τ ) dτ
y(t) = y(t0) +
(cid:90) t
τ =t0
x(τ ) d˜τ (τ )
In order to be able to do that, we need to require that ωc(t) > 0 ∀t, or, more
precisely that there is some ω0 > 0 such that
ωc(t) ≥ ω0 > 0 ∀t
In an obvious analogy to the concept of uniform convergence, we could describe
this requirement as “ωc(t) being uniformly positive”.
Apparently the requirement on ωc(t) to be uniformly positive is quite a
reasonable restriction, we just need to put a lower limit on the cutoﬀ. Given
such ωc(t) we then can introduce the “warped” time ˜t, where from
we obtain
E.g. we could take
d˜t = ωc(t) dt
(cid:90)
˜t =
ωc(t)dt
˜t =
(cid:90) t
0
ωc(τ )dτ
(2.27)
Since ωc(t) is uniformly positive, the function ˜t(t) is monotonically increasing
and maps t ∈ (−∞, +∞) to ˜t ∈ (−∞, +∞). We can therefore reexpress the
signals x(t) and y(t) in terms of ˜t, obtaining some functions ˜x (cid:0) ˜t (cid:1) and ˜y (cid:0) ˜t (cid:1),
and ultimately
˜y (cid:0) ˜t (cid:1) = ˜y (cid:0) ˜t0
(cid:1) +
(cid:90) ˜t
˜t0
˜x(˜τ ) d˜τ
(2.28)
This means that the variation of ωc can be equivalently represented as warping
of the time axis, the cutoﬀ gains in the warped time scale having a constant
unity value.27
27Instead of unit cutoff we can have any other positive value, by simply linearly stretching
the time axis in addition to the warping ˜t(t).
42
CHAPTER 2. ANALOG 1-POLE FILTERS
Equivalent topologies
The fact that cutoﬀ modulation can be equivalently represented as warping of
the time scale has several implications of high importance. One implication has
to do with equivalence of systems with diﬀerent topologies.
The term topology in this context simply refers to the components used
in the system’s block diagram and the way they are connected to each other.
Often, the reason we would want to talk about the topology would be to put
there can be
it against the idea of the transfer function. More speciﬁcally:
systems with different topologies implementing the same transfer function. We
have already seen the example of that: there is an ordinary 1-pole multimode
and a transposed 1-pole multimode, which both can be used to implement one
and the same transfer function.
According to our previous discussion, systems having identical transfer func-
tions will behave identically (at least in the absence of the transient response
arising out of a non-zero initial state of the system). However, all of our anal-
ysis of system behavior, including the transient response, was done under the
assumption of time-invariance. This assumption is actually critical: for a time-
varying system the situation is more complicated and two systems may behave
diﬀerently even if they share the same transfer function.28 We had a brief ex-
ample of that in Section 2.7 where we compared diﬀerent positionings of the
cutoﬀ gain relative to the integrator.
However (2.28) means that if the cutoﬀ modulation is compliant to (2.26)
(pre-integrator cutoﬀ gain) and if the only time-varying aspect of the system
is the cutoﬀ modulation, the systems will behave identically. Indeed, we could
use one and the same time-warping (2.27) for both of the systems, thus, if they
are identically behaving in the original time-invariant case, so will they in the
time-warped case.
This question will be addressed once again from a slightly more detailed
point in Section 7.12.
Time-varying stability
A further implication of (2.28) is the fact that the stability of a system cannot
be destroyed by the cutoﬀ modulation. This is true for an arbitrary system,
given all cutoﬀ gains are preceding the integrators and are having equal values
all the time. Indeed, the warping of the time axis (2.27) can’t aﬀect the BIBO
property of the signals x(t) and y(t), thus stability is unaﬀected by the time
warping.29
28Of course, strictly speaking time-varying systems do not have a transfer function. But it
is intuitive to use the idea of a “time-varying transfer function”, understood as the transfer
function which is formally evaluated pretending the system’s parameters are fixed at each
time moment. E.g. if we have a 1-pole lowpass with a varying cutoff ωc(t), we would say that
its transfer function at each time moment is H(s) = ωc(t)/(ωc(t) + s). Of course, this is not
a true transfer function in the normal sense. Particularly, for an exponential input est the
filter’s output is not equal to y(t) = H(s, t)est.
29Note that this applies only to the idealized continuous-time systems. After conversion to
discrete time the same argument will not automatically hold and the stability of the resulting
discrete time system will need to be proven again. However, it is not unreasonable to expect,
given a discretization method which preserves time-invariant stability, that it will also at least
approximately preserve the time-varying stability.
SUMMARY
43
For the 1-pole ﬁlter, however, the time-varying stability can be checked in a
much simpler manner. As we should remember, the output signal equation for
a 1-pole lowpass, written in the diﬀerential form is
˙y = ωc(t)(x − y)
This means that, as long as ωc > 0, the value of y always “moves in the direction
towards the input signal”. In this case, clearly, the absolute value of y can’t
exceed the maximum of the absolute value of x.30 On the contrary, imagine
ωc < 0, x(t) = 0 and y(t) (cid:54)= 0 (let’s say x(t) was nonzero for a while and then
we switched it oﬀ, leaving y(t) at a nonzero value). The diﬀerential equation
turns into ˙y = −ωcy = |ωc| · y, which clearly produces an indeﬁnitely growing
y(t).
The 1-pole highpass ﬁlter’s output is simply x(t) − y(t) (where y(t) is the
lowpass signal), therefore the highpass ﬁlter is stable if and only if the lowpass
ﬁlter is stable.
We have seen that cutoﬀ is a very special ﬁlter parameter, such that its mod-
ulation can’t destroy the ﬁlter’s stability (provided some reasonable conditions
are met). There are also some trivial cases, when the modulated parameters
are not a part of a feedback loop, such as e.g. the mixing gain of a shelving
ﬁlter. Apparently, such parameters when being varied can’t destroy the ﬁlter’s
stability as well. With the ﬁlter types which we introduce later in this book
there will be other parameters within feedback loops which in principle can be
modulated. Unfortunately, for the modulation of such other parameters there
is no simple answer (although sometimes the stability can be proven by some
means). Respectively there is no easy general criterion for time-varying ﬁlter
stability as there is for the time-invariant case. Often, we simply hope that
the modulation of the ﬁlter parameters does not make the (otherwise stable)
ﬁlter unstable. This is not simply a theoretical statement, on the contrary, such
cases, where the modulation destabilizes a ﬁlter, do occur in practice.
SUMMARY
The analog 1-pole ﬁlter implementations are built around the idea of the mul-
timode 1-pole ﬁlter in Fig. 2.13. The transfer functions of the lowpass and
highpass 1-pole ﬁlters are
HLP(s) =
ωc
s + ωc
and
HHP(s) =
s
s + ωc
respectively. Other 1-pole ﬁlter types can be built by combining the lowpass
and the highpass signals.
30If ωc = 0 then y(t) doesn’t change. This is the marginally stable case. Particularly, even
if x(t) = 0, the output y(t) will stay at whatever value it is, rather than decaying towards the
zero.
44
CHAPTER 2. ANALOG 1-POLE FILTERS
Chapter 3
Time-discretization
Now that we have introduced the basic ideas of analog ﬁlter analysis, we will
develop an approach to convert analog ﬁlter models to the discrete time.
3.1 Discrete-time signals
The discussion of the basic concepts of discrete-time signal representation and
processing is outside the scope of this book. We are assuming that the reader
is familiar with the basic concepts of discrete-time signal processing, such as
sampling, sampling rate, sampling period, Nyquist frequency, analog-to-digital
and digital-to-analog signal conversion. However we are going to make some
remarks in this respect.
As many other texts do, we will use the square bracket notation to denote
discrete-time signals and round parentheses notation to denote continuous-time
signals: e.g. x[n] and x(t).
We will often assume a unit sampling rate fs = 1 (and, respectively, a unit
sampling period T = 1), which puts the Nyquist frequency at 1/2, or, in the
circular frequency terms, at π. Apparently, this can be achieved simply by a
corresponding choice of time units.
Theoretical DSP texts typically state that discrete-time signals have periodic
frequency spectra. This might be convenient for certain aspects of theoretical
analysis such as analog-to-digital and digital-to-analog signal conversion, but it’s
highly unintuitive otherwise. It would be more intuitive, whenever talking of a
discrete-time signal, to imagine an ideal DAC connected to this signal, and think
that the discrete-time signal represents the respective continuous-time signal
produced by such DAC. Especially, since by sampling this continuous-time sig-
nal we obtain the original discrete-time signal again. So the DAC and ADC con-
versions are exact inverses of each other (in this case). Now, the continuous-time
signal produced by such DAC doesn’t contain any partials above the Nyquist
frequency. Thus, its Fourier integral representation (assuming T = 1) is
x[n] =
(cid:90) π
−π
X(ω)ejωn dω
2π
45
46
CHAPTER 3. TIME-DISCRETIZATION
and its Laplace integral representation is
x[n] =
(cid:90) σ+jπ
σ−jπ
X(s)esn ds
2πj
Introducing notation z = es and noticing that
ds = d(log z) =
dz
z
we can rewrite the Laplace integral as
(cid:73)
x[n] =
X(z)zn dz
2πjz
(where X(z) is apparently a diﬀerent function than X(s)) where the integration
is done counterclockwise along a circle of radius eσ centered at the complex
plane’s origin:1
z = es = eσ+jω = eσ · ejω
(−π ≤ ω ≤ π)
(3.1)
We will refer the representation (3.1) as the z-integral.2 The function X(z) is
referred to as the z-transform of x[n].
In case of non-unit sampling period T (cid:54)= 1 the formulas are the same, except
that the frequency-related parameters get multiplied by T (or divided by fs), or
equivalently, the n index gets multiplied by T in continuous-time expressions:3
x[n] =
(cid:90) πfs
−πfs
X(ω)ejωT n dω
2π
x[n] =
(cid:90) σ+jπfs
σ−jπfs
X(s)esT n ds
2πj
z = esT
(cid:73)
x[n] =
X(z)zn dz
2πjz
(z = eσ+jωT , −πfs ≤ ω ≤ πfs)
The notation zn is commonly used for discrete-time complex exponential
signals. A continuous-time signal x(t) = est is written as x[n] = zn in discrete-
time, where z = esT . The Laplace-integral amplitude coeﬃcient X(s) in X(s)est
then may be replaced by a z-integral amplitude coeﬃcient X(z) such as in
X(z)zn.
1As with Laplace transform, sometimes there are no restrictions on the radius eσ of the
circle, sometimes there are.
2A more common term for (3.1) is the inverse z-transform, but we will prefer the z-integral
term for the same reason as with Fourier and Laplace integrals.
3Formally the σ parameter of the Laplace integral (and z-integral) should have been mul-
tiplied by T as well, but it doesn’t matter, since this parameter is chosen rather arbitrarily.
3.2. NAIVE INTEGRATION
47
3.2 Naive integration
The most “interesting” element of analog ﬁlter block diagrams is obviously the
integrator. The time-discretization for other elements is trivial, so we should
concentrate on building the discrete-time models of the analog integrator.
The continuous-time integrator equation is
y(t) = y(t0) +
(cid:90) t
t0
x(τ ) dτ
In discrete time we could approximate the integration by a summation of the
input samples. Assuming for simplicity T = 1, we could have implemented a
discrete-time integrator as
y[n] = y[n0 − 1] +
n
(cid:88)
ν=n0
x[ν]
We will refer to the above as the naive digital integrator.
A pseudocode routine for this integrator could simply consist of an accumu-
lating assignment:
// perform one sample tick of the integrator
integrator_output := integrator_output + integrator_input;
It takes the current state of the integrator stored in the integrator output vari-
able and adds the current sample’s value of the integrator input on top of that.
In case of a non-unit sampling period T (cid:54)= 1 we have to multiply the accu-
mulated input values by T :4
// perform one sample tick of the integrator
integrator_output := integrator_output + integrator_input*T;
3.3 Naive lowpass filter
We could further apply this “naive” approach to construct a discrete-time model
of the lowpass ﬁlter in Fig. 2.2. We will use the naive integrator as a basis for
this model.5
Let the x variable contain the current input sample of the ﬁlter. Consid-
ering that the output of the ﬁlter in Fig. 2.2 coincides with the output of the
integrator, let the y variable contain the integrator state and simultaneously
serve as the output sample. As we begin to process the next input sample, the
4Alternatively, we could, of course, scale the integrator’s output by T , but this is less
useful in practice, because the T factor will be usually combined with the cutoff gain factor
ωc preceding the integrator.
5Based on the fact that the naive integration introduced above is identical to Euler
backward-difference integration, there is an opinion that the naive approach (loosely defined
as “take whatever values we have now at the integrator inputs and apply a single naive inte-
gration step to those”) is identical to the Euler method. This is not 100% so. The readers are
encouraged to formally apply backward- and forward-difference Euler methods to ˙y = ωc(y−x)
to convince themselves that there are some differences. Particularly, the backward-difference
method is implicit (requires solving an equation), while the forward-difference method pro-
duces the “future” value of the output. For more complicated systems the differences could
be more drastic, although the author didn’t explicitly verify that.
48
CHAPTER 3. TIME-DISCRETIZATION
y variable will contain the previous output value. At the end of the processing
of the sample (by the ﬁlter model) the y variable will contain the new output
sample. In this setup, the input value for the integrator is apparently (x − y)ωc,
thus we simply have
// perform one sample tick of the lowpass filter
y := y + (x-y)*omega_c;
(mind that ωc must have been scaled to the time units corresponding to the
unit sample period!)
A naive discrete-time model of the multimode ﬁlter in Fig. 2.13 could have
been implemented as:
// perform one sample tick of the multimode filter
hp := x-lp;
lp := lp + hp*omega_c;
where the integrator state is stored in the lp variable.
The above naive implementations (and any other similar naive implemen-
tations, for that matter) work reasonably well as long as ωc (cid:28) 1, that is the
cutoﬀ must be much lower than the sampling rate. At larger ωc the behavior
of the ﬁlter becomes rather strange, ultimately the ﬁlter gets unstable. We will
now develop some theoretical means to analyse the behavior of the discrete-time
ﬁlter models, ﬁgure out what are the problems with the naive implementations,
and then introduce another discretization approach.
3.4 Block diagrams
Let’s express the naive discrete-time integrator in the form of a discrete-time
block diagram. The discrete-time block diagrams are constructed from the same
elements as continuous-time block diagrams, except that instead of integrators
they have unit delays. A unit delay simply delays the signal by one sample.
That is the output of a unit delay comes “one sample late” compared to the
input. Apparently, the implementation of a unit delay requires a variable, which
will be used to store the new incoming value and keep it there until the next
sample. Thus, a unit delay element has a state, while the other block diagram
elements are obviously stateless. This makes the unit delays in a way similar to
the integrators in the analog block diagrams, where the integrators are the only
elements with a state.
A unit delay element in a block diagram is denoted as:
z−1
The reason for the notation z−1 will be explained a little bit later. Using a unit
delay, we can create a block diagram for our naive integrator (Fig. 3.1). For an
arbitrary sampling period we obtain the structure in Fig. 3.2. For an integrator
with embedded cutoﬀ gain we can combine the ωc gain element with the T gain
element (Fig. 3.3). Notice that the integrator thereby becomes invariant to the
choice of the time units, since ωcT is invariant to this choice.
Now let’s construct the block diagram of the naive 1-pole lowpass ﬁlter.
Recalling the implementation routine:
(cid:47)
(cid:47)
(cid:47)
(cid:47)
3.4. BLOCK DIAGRAMS
49
x[n]
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1 (cid:111)
•(cid:47)
y[n]
Figure 3.1: Naive integrator for T = 1.
x[n]
T
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
y[n]
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1 (cid:111)
Figure 3.2: Naive integrator for arbitrary T .
x[n]
ωcT
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
y[n]
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1 (cid:111)
Figure 3.3: Naive integrator with embedded cutoﬀ.
// perform one sample tick of the lowpass filter
y := y + (x-y)*omega_c;
we obtain the diagram in Fig. 3.4. The z−1 element in the feedback from the
ﬁlter’s output to the leftmost summator is occurring due to the fact that we are
picking up the previous value of y in the routine when computing the diﬀerence
x − y.
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
x[n]
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1 (cid:111)
•(cid:47)
•(cid:47)
y[n]
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
z−1 (cid:111)
Figure 3.4: Naive 1-pole lowpass ﬁlter (the dashed line denotes the
integrator).
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
50
CHAPTER 3. TIME-DISCRETIZATION
This unit delay occurring in the discrete-time feedback is a common problem
in discrete-time implementations. This problem is solvable, however it doesn’t
make too much sense to solve it for the naive integrator-based models, as the
increased complexity doesn’t justify the improvement in sound. We will address
the problem of the zero-delay discrete-time feedback later, for now we’ll con-
centrate on the naive model in Fig. 3.4. This model can be simpliﬁed a bit, by
combining the two z−1 elements into one (Fig. 3.5), so that the block diagram
explicitly contains a single state variable (as does its pseudocode counterpart).
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
x[n]
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
y[n]
•(cid:47)
z−1
•(cid:15)
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
Figure 3.5: Naive 1-pole lowpass ﬁlter with just one z−1 element
(the dashed line denotes the integrator).
3.5 Transfer function
Let x[n] and y[n] be respectively the input and the output signals of a unit
delay:
x[n]
z−1
y[n]
For a complex exponential input x[n] = esn = zn we obtain
y[n] = es(n−1) = esne−s = znz−1 = z−1x[n]
That is
y[n] = z−1x[n]
That is, z−1 is the transfer function of the unit delay! It is common to express
discrete-time transfer functions as functions of z rather than functions of s. The
reason is that in this case the transfer functions are nonstrictly proper6 rational
functions, similarly to the continuous-time case, which is pretty convenient. So,
for a unit delay we could write H(z) = z−1.
Now we can obtain the transfer function of the naive integrator in Fig. 3.1.
Suppose7 x[n] = X(z)zn and y[n] = Y (z)zn, or shortly, x = X(z)zn and
6Under the assumption of causality, which holds if the system is built of unit delays.
7As in continuous-time case, we take for granted the fact that complex exponentials zn are
eigenfunctions of discrete-time linear time-invariant systems.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
3.5. TRANSFER FUNCTION
51
y = Y (z)zn. Then the output of the z−1 element is yz−1. The output of the
summator is then x + yz−1, thus
from where
and
y = x + yz−1
y(1 − z−1) = x
H(z) =
y
x
=
1
1 − z−1
This is the transfer function of the naive integrator (for T = 1).
It is relatively common to express discrete-time transfer functions as ratio-
nal functions of z−1 (like the one above) rather than rational functions of z.
However, for the purposes of the analysis it is also often convenient to have
them expressed as rational functions of z (particularly, for ﬁnding their poles
and zeros). We can therefore multiply the numerator and the denominator of
the above H(z) by z, obtaining:
H(z) =
z
z − 1
Since z = es, the frequency response is obtained as H(ejω). The amplitude
and phase responses are (cid:12)
For T (cid:54)= 1 we obtain
(cid:12)H(ejω)(cid:12)
(cid:12) and arg H(ejω) respectively.8
H(z) = T
z
z − 1
and, since z = esT , the frequency response is H(ejωT ).
Now let’s obtain the transfer function of the naive 1-pole lowpass ﬁlter in
Fig. 3.5, where, for the simplicity of notation, we assume T = 1. Assuming
complex exponentials x = X(z)zn and y = Y (z)zn we have x and yz−1 as
the inputs of the ﬁrst summator. Respectively the integrator’s input is ωc(x −
yz−1). And the integrator output is the sum of yz−1 and the integrator’s input.
Therefore
From where
y = yz−1 + ωc(x − yz−1)
(cid:0)1 − (1 − ωc)z−1(cid:1)y = ωcx
and
=
y
x
H(z) =
ωc
1 − (1 − ωc)z−1 =
The transfer function for T (cid:54)= 1 can be obtained by simply replacing ωc by ωcT .
The respective amplitude response is plotted in Fig. 3.6. Comparing it to
the amplitude response of the analog prototype we can observe serious deviation
closer to the Nyquist frequency. The phase response (Fig. 3.7) has similar
deviation problems.
ωcz
z − (1 − ωc)
In principle, the amplitude response deviation can be drastically reduced
by correcting the ﬁlter’s cutoﬀ setting. E.g. one could notice that the second
8Another way to look at this is to notice that in order for zn to be a complex sinusoid ejωn
we need to let z = ejω.
52
CHAPTER 3. TIME-DISCRETIZATION
|H(ejω)|, dB
0
-6
-12
-18
0.001π
0.01π
0.02π
0.1π
1 1.2
π
ω
Figure 3.6: Amplitude response of a naive 1-pole lowpass ﬁlter for a
number of diﬀerent cutoﬀs. Dashed curves represent the respective
analog ﬁlter responses for the same cutoﬀs.
arg H(ejω)
0
−π/4
−π/2
0.001π
0.01π
0.02π
0.1π
1 1.2
π
ω
Figure 3.7: Phase response of a naive 1-pole lowpass ﬁlter for a
number of diﬀerent cutoﬀs. Dashed curves represent the respective
analog ﬁlter responses for the same cutoﬀs.
of the amplitude responses in Fig. 3.6 is occurring a bit too far to the right,
compared to the analog response (which is what we’re aiming at). Therefore
we could achieve a better matching between the two responses by reducing the
cutoﬀ setting of the digital ﬁlter by a small amount. Depending on the formal
deﬁnition of the response matching, one could derive an analytical expression
for such cutoﬀ correction. There are two main problems with that, though.
One problem is that many other ﬁlters, e.g. a 2-pole resonating lowpass, have
more parameters, e.g. not only cutoﬀ but also the resonance, and we potentially
may need to correct all of them, which results in much more involved math.
This problem is not as critical though, and there are some methods utilizing
this approach.
The other problem, though, is the phase response. Looking at Fig. 3.7 it
3.6. TRAPEZOIDAL INTEGRATION
53
seems that no matter how we try to correct the ﬁlter’s cutoﬀ, the phase response
will be always zero at Nyquist, whereas we would desire something close to
−π/2. The eﬀects of the deviation of the ﬁlter’s phase response are mostly
quite subtle. Therefore it’s somewhat diﬃcult to judge how critical the phase
devations might be.9 However there’s one absolutely objective and major issue
associated with the phase deviations. Attempting to mix outputs of two ﬁlters
with some deviations in either or both of the amplitude and phase responses
may easily lead to unexpected and undesired results. For that reason in this
book we will concentrate on a diﬀerent method which is much more robust in
this respect.
Poles and zeros
Discrete-time block diagrams are diﬀering from continuous-time block diagrams
only by having z−1 elements instead of integrators. Recalling that the transfer
function of an integrator is s−1, we conclude that from the formal point of view
the diﬀerence is purely notational.
Now, the transfer functions of continuous-time block diagrams are non-
strictly proper rational functions of s. Respectively, the transfer functions of
discrete-time block diagrams are nonstrictly proper rational functions of z.
Thus, discrete-time transfer functions will have poles and zeros in a way sim-
ilar to continuous-time transfer functions. Similarly to continuous-time transfer
functions, the poles will deﬁne the stability of a linear time-invariant ﬁlter. Con-
sider that z = esT and recall the stability criterion Re s < 0 (where s = pn,
where pn are the poles). Apparently, Re s < 0 ⇐⇒ |z| < 1. We might there-
fore intuitively expect the discrete-time stability criterion to be |pn| < 1 where
pn are the discrete-time poles. This is indeed the case, a linear time-invariant
diﬀerence system10 is stable if and only if all its poles are located inside the
unit circle. We will give more detail about the mechanisms behind this in the
discussion of the discrete-time transient response in Sections 3.12 and 7.13.
3.6 Trapezoidal integration
Instead of naive integration, we could attempt using the trapezoidal integration
method (T = 1):
// perform one sample tick of the integrator
integrator_output := integrator_output +
(integrator_input + previous_integrator_input)/2;
previous_integrator_input := integrator_input;
Notice that now we need two state variables per integrator: integrator output
and previous integrator input. The block diagram of a trapezoidal integrator is
shown in Fig. 3.8. We’ll refer to this integrator as a direct form I trapezoidal
integrator. The reason for this term will be explained later.
9Many engineers seem to believe that the deviations in phase response are quite tolerable
acoustically. The author’s personal preference is to be on the safe side and not take the risks
which are difficult to estimate. At least some caution in this regard would be recommended.
10Difference systems can be defined as those, whose block diagrams consist of gains, sum-
mators and unit delays. More precisely those are causal difference systems. There are also
difference systems with a lookahead into the future, but we don’t consider them in this book.
54
CHAPTER 3. TIME-DISCRETIZATION
x[n]
•(cid:47)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
z−1 (cid:111)
•(cid:47)
y[n]
Figure 3.8: Direct form I trapezoidal integrator (T = 1).
We could also construct a trapezoidal integrator implementation with only
a single state variable. Consider the expression for the trapezoidal integrator’s
output:
y[n] = y[n0 − 1] +
n
(cid:88)
ν=n0
x[ν − 1] + x[ν]
2
(3.2)
Suppose y[n0−1] = 0 and x[n0−1]=0, corresponding to a zero initial state (recall
that both y[n0 − 1] and x[n0 − 1] are technically stored in the z−1 elements).
Then
y[n] =
n
(cid:88)
ν=n0
x[ν − 1] + x[ν]
2
=
1
2
(cid:32) n
(cid:88)
x[ν − 1] +
(cid:33)
x[ν]
=
n
(cid:88)
ν=n0
ν=n0
(cid:33)
x[ν]
=
1
2
n
(cid:88)
ν=n0
(cid:32) n−1
(cid:88)
n
(cid:88)
(cid:33)
x[ν]
=
x[ν] +
ν=n0
ν=n0
(cid:32) n
(cid:88)
x[ν − 1] +
1
2
ν=n0+1
u[n − 1] + u[n]
2
=
=
where
u[n] =
n
(cid:88)
ν=n0
x[ν]
Now notice that u[n] is the output of a naive integrator, whose input signal
is x[n]. At the same time y[n] is the average of the previous and the current
output values of the naive integrator. This can be implemented by the structure
in Fig. 3.9. Similar considerations apply for nonzero initial state. We’ll refer to
the integrator in Fig. 3.9 as a direct form II or canonical trapezoidal integrator.
The reason for this term will be explained later.
We can develop yet another form of the bilinear integrator with a single state
variable. Let’s rewrite (3.2) as
y[n] = y[n0 − 1] +
x[n0 − 1]
2
+
n−1
(cid:88)
ν=n0
x[ν] +
x[n]
2
and let
u[n − 1] = y[n] −
x[n]
2
= y[n0 − 1] +
x[n0 − 1]
2
+
n−1
(cid:88)
ν=n0
x[ν]
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
3.6. TRAPEZOIDAL INTEGRATION
55
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
y[n]
x[n]
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
z−1
•(cid:15)
Figure 3.9: Direct form II (canonical) trapezoidal integrator (T =
1).
Notice that
and
y[n] = u[n − 1] +
x[n]
2
u[n] = u[n − 1] + x[n] = y[n] +
x[n]
2
(3.3a)
(3.3b)
Expressing (3.3a) and (3.3b) in a graphical form, we obtain the structure in
Fig. 3.10. We’ll refer to the integrator in Fig. 3.10 as a transposed direct form
II or transposed canonical trapezoidal integrator. The reason for this term will
be explained later.
x[n]
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
+
•(cid:47)
y[n]
u[n−1]
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
u[n]
+
Figure 3.10: Transposed direct form II (transposed canonical)
trapezoidal integrator (T = 1).
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
The positioning of the 1/2 gain prior to the integrator in Fig. 3.10 is quite
convenient, because we can combine the 1/2 gain with the cutoﬀ gain into a
single gain element.
In case of an arbitrary sampling period we could also
include the T factor into the same gain element, thus obtaining the structure in
Fig. 3.11. A similar trick can be performed for the other two integrators, if we
move the 1/2 gain element to the input of the respective integrator. Since the
integrator is a linear time-invariant system, this doesn’t aﬀect the integrator’s
behavior in a slightest way.
Typically one would prefer the direct form II integrators to the direct form I
integrator, because the former have only one state variable. In this book we will
mostly use the transposed direct form II integrator, because this is resulting in
slightly simpler zero-delay feedback equations and also oﬀers a nice possibility
for the internal saturation in the integrator.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
56
CHAPTER 3. TIME-DISCRETIZATION
x[n]
ωcT /2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
+
•(cid:47)
y[n]
u[n−1]
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
u[n]
+
Figure 3.11: Transposed direct form II (transposed canonical)
trapezoidal integrator with “embedded” cutoﬀ gain.
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
The transfer functions of all three integrators are identical. Let’s obtain
e.g. the transfer function of the transposed canonical integrator (in Fig. 3.10).
Assuming signals of the exponential form zn, we can drop the index [n], under-
standing it implicity, while the index [n−1] will be replaced by the multiplication
by z−1. Then (3.3) turn into
x
2
y = uz−1 +
u = y +
x
2
Substituting the second equation into the ﬁrst one we have
(cid:16)
y +
y =
(cid:17)
x
2
z−1 +
x
2
yz = y +
y(z − 1) =
+
x
2
z
(z + 1)
x
2
x
2
and the transfer function of the trapezoidal integrator is thus
H(z) =
y
x
=
1
2
·
z + 1
z − 1
For an arbitrary T one has to multiply the result by T , to take the respective
gain element into account:
H(z) =
T
2
·
z + 1
z − 1
If also the cutoﬀ gain is included, we obtain
H(z) =
ωcT
2
·
z + 1
z − 1
One can obtain the same results for the other two integrators.
What is so special about this transfer function, that makes the trapezoidal
integrator so superior to the naive one, is to be discussed next.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
3.7. BILINEAR TRANSFORM
57
3.7 Bilinear transform
Suppose we take an arbitrary continuous-time block diagram, like the familiar
lowpass ﬁlter in Fig. 2.2 and replace all continuous-time integrators by discrete-
time trapezoidal integrators. On the transfer function level, this will correspond
to replacing all s−1 with T
z−1 . That is, technically we perform a substitution
2 · z+1
s−1 =
T
2
·
z + 1
z − 1
in the transfer function expression.
It would be more convenient to write this substitution explicitly as
s =
2
T
·
z − 1
z + 1
(3.4)
The substitution (3.4) is referred to as the bilinear transform, or shortly BLT.
For that reason we can also refer to trapezoidal integrators as BLT integrators.
Let’s ﬁgure out, how does the bilinear transform aﬀect the frequency response
of the ﬁlter, that is, what is the relationship between the original continuous-
time frequency response prior to the substitution and the resulting discrete-time
frequency response after the substitution.
Let Ha(s) be the original continuous-time transfer function. Then the re-
spective discrete-time transfer function is
(cid:18) 2
T
Hd(z) = Ha
(cid:19)
·
z − 1
z + 1
Respectively, the discrete-time frequency response is
(3.5)
Hd(ejωT ) = Ha
= Ha
(cid:18) 2
T
(cid:18) 2
T
·
ejωT − 1
ejωT + 1
(cid:19)
ωT
2
j tan
(cid:19)
= Ha
(cid:18) 2
T
·
ejωT /2 − e−jωT /2
ejωT /2 + e−jωT /2
(cid:19)
=
Notice that Ha(s) in the last expression is evaluated on the imaginary axis!!!
That is, the bilinear transform maps the imaginary axis in the s-plane to the
(cid:0) 2
(cid:1) is the analog frequency response
T j tan ωT
unit circle in the z-plane! Now, Ha
evaluated at 2
T tan ωT
2 . That is, the digital frequency response at ω is equal to
the analog frequency response at 2
T tan ωT
2 . This means that the analog fre-
quency response in the range 0 ≤ ω < +∞ is mapped into the digital frequency
range 0 ≤ ωT < π (0 ≤ ω < πfs), that is from zero to Nyquist!11 Denoting
the analog frequency as ωa and the digital frequency as ωd we can express the
argument mapping of the frequency response function as
2
or, in a more symmetrical way
ωa =
2
T
tan
ωdT
2
ωaT
2
= tan
ωdT
2
11A similar mapping obviously occurs for the negative frequencies.
(3.6)
(3.7)
58
CHAPTER 3. TIME-DISCRETIZATION
Notice that for frequencies much smaller that Nyquist frequency we have ωT (cid:28)
1 and respectively ωa ≈ ωd.
This is what is so unique about the bilinear transform. It simply warps the
frequency range [0, +∞) into the zero-to-Nyquist range, but otherwise doesn’t
change the frequency response at all! Considering in comparison a naive inte-
grator, we would have obtained:
s−1 =
z
z − 1
s =
z − 1
z
(cid:18) z − 1
z
Hd(z) = Ha
(3.8)
(cid:19)
Hd(ejω) = Ha
(cid:19)
(cid:18) ejω − 1
ejω
= Ha
(cid:0)1 − e−jω(cid:1)
which means that the digital frequency response is equal to the analog transfer
function evaluated on a circle of radius 1 centered at s = 1. This hardly deﬁnes
a clear relationship between the two frequency responses.
So, by simply replacing the analog integrators with digital trapezoidal in-
tegrators, we obtain a digital ﬁlter whose frequency response is essentially the
same as the one of the analog prototype, except for the frequency warping.
Particularly, the relationship between the amplitude and phase responses of the
ﬁlter is fully preserved, which is particularly highly important if the ﬁlter is to
be used as a building block in a larger ﬁlter. Very close to perfect!
Furthermore, the bilinear transform maps the left complex semiplane in the
s-domain into the inner region of the unit circle in the z-domain. Indeed, let’s
obtain the inverse bilinear transform formula. From (3.4) we have
from where
and
(z + 1)
sT
2
= z − 1
1 +
sT
2
(cid:18)
= z
1 −
(cid:19)
sT
2
z =
1 + sT
2
1 − sT
2
(3.9)
The equation (3.9) deﬁnes the inverse bilinear transform. Now, if Re s < 0,
then, obviously
(cid:12)
(cid:12)
1 +
(cid:12)
(cid:12)
sT
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
<
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1 −
sT
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
and |z| < 1. Thus, the left complex semiplane in the s-plane is mapped to the
inner region of the unit circle in the z-plane. In the same way one can show
that the right complex semiplane is mapped to the outer region of the unit
circle. And the imaginary axis is mapped to the unit circle itself. Comparing
the stability criterion of analog ﬁlters (the poles must be in the left complex
semiplane) to the one of digital ﬁlters (the poles must be inside the unit circle),
3.8. CUTOFF PREWARPING
59
we conclude that the bilinear transform exactly preserves the stability of the
ﬁlters!
In comparison, for a naive integrator replacement we would have the follow-
ing. Inverting the (3.8) substitution we obtain
and
sz = z − 1
z(1 − s) = 1
z =
1
1 − s
Assuming Re s < 0 and considering that in this case
(cid:12)
(cid:12)
(cid:12)
(cid:12)
z −
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
2
=
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
1 − s
−
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
2
=
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1 − 1
2 + s
2
1 − s
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
2
·
1 + s
1 − s
(cid:12)
(cid:12)
(cid:12)
(cid:12)
<
1
2
we conclude that the left semiplane is mapped into a circle of radius 0.5 cen-
tered at z = 0.5. So the naive integrator overpreserves the stability, which is
not nice, since we would rather have digital ﬁlters behaving as closely to their
analog prototypes as possible. Considering that this comes in a package with a
poor frequency response transformation, we should rather stick with trapezoidal
integrators.
So, let’s replace e.g. the integrator in the familiar lowpass ﬁlter structure in
Fig. 2.2 with a trapezoidal integrator. Performing the integrator replacement,
we obtain the structure in Fig. 3.12.12 We will refer to the trapezoidal integrator
replacement method as the topology-preserving transform (TPT) method. This
term will be explained and properly introduced later. For now, before we simply
attempt to implement the structure in Fig. 3.12 in code, we should become aware
of a few further issues.
3.8 Cutoff prewarping
Suppose we are using the lowpass ﬁlter structure in Fig. 3.12 and we wish to have
its cutoﬀ at ωc. If we however simply put this ωc parameter into the respective
integrator gain element ωcT /2, the frequency response itself and, speciﬁcally, its
value at the cutoﬀ will be diﬀerent from the expected one. Fig. 3.13 illustrates.
The −3dB level is speciﬁcally highlighted in Fig. 3.13, since this is the amplitude
response value of the 1-pole lowpass ﬁlter at the cutoﬀ, thereby aiding the visual
identiﬁcation of the cutoﬀ point on the response curves.13
Apparently, the diﬀerence between analog and digital response is occuring
due to the warping of the frequency axis (3.6). We would like to estimate the
12Note that thereby, should we become interested in the amplitude and phase responses of
Fig. 3.12, we don’t have to derive the discrete-time transfer function of Fig. 3.12. Instead we
can simply take the amplitude and phase responses of the analog 1-pole (which are simpler
to compute) and apply the mapping (3.7). This is the reason that we almost exclusively deal
with analog transfer functions in this book, we simply don’t need digital ones most of the
time.
13Apparently, the picture in Fig. 3.13 will be the same at any other sampling rate, except
that the frequency axis values will need to be relabelled proportionally to the sampling rate
change. E.g. at 88.2kHz the labels would be 4, 8, 16, 22.05, 32 and 44.1kHz respectively. We
could have labelled the axis in terms of normalized ω instead, but giving the absolute values
is more illustrative. Particularly, the audible frequency range is easier to see.
60
CHAPTER 3. TIME-DISCRETIZATION
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
x[n]
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT /2
•(cid:47)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
+
•(cid:47)
•(cid:47)
y[n]
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 3.12: 1-pole TPT lowpass ﬁlter (the dashed line denotes the
trapezoidal integrator).
|H(ejω)|, dB
0
-3
-6
-9
2
4
8
11.025
16
22.05
f, kHz
Figure 3.13: Amplitude response of an unprewarped bilinear-
transformed 1-pole lowpass ﬁlter for a number of diﬀerent cutoﬀs.
Dashed curves represent the respective analog ﬁlter responses for
the same cutoﬀs. Sampling rate 44.1kHz.
frequency error introduced by the warping. To simplify the further discussion
let’s rewrite (3.6) as a mapping function µ(ω):
ωa = µ(ωd) =
2
T
tan
ωdT
2
(3.10)
Now, given some desired analog response, we could take some point ωa on
this response and ask ourselves, where is the same point located on the digital
response. According to (3.10), it is located at µ−1(ωa) (where µ−1 is the function
inverse of µ). Thus the ratio of the actual and desired frequencies is µ−1(ωa)/ωa,
or, in the octave scale:
∆P = log2
µ−1(ωa)
ωa
The solid curve in Fig. 3.14 illustrates (note that Fig. 3.14 labels the ∆P axis
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
3.8. CUTOFF PREWARPING
61
in semitones).
∆P, st
0
−4
−8
−12
2
4
8
11.025
16
22.05
fa, kHz
Figure 3.14: Bilinear transform’s detuning of analog frequencies
plotted against analog frequency (solid curve) or digital frequency
(dashed curve). Sampling rate 44.1kHz.
We could also express the detuning of analog frequencies in terms of the
corresponding digital frequency. Given the digital frequency response, we take
some point ωd and ask ourselves, where is the same point located on the analog
response. According to (3.10), it is located at µ(ωd) and thus the frequency
ratio is ωd/µ(ωd), respectively
∆P = log2
ωd
µ(ωd)
The dashed curve in Fig. 3.14 illustrates. Note that we are not talking about how
much the speciﬁed digital frequency will be detuned (because digital frequencies
are not getting detuned, they are already where they are), it’s still about how
much the corresponding analog frequency will be detuned.
Thus, given an analog ﬁlter with a frequency response Ha(jω), its digital
counterpart will have its frequencies detuned as shown in Fig. 3.14. Particularly,
the cutoﬀ point, instead of being at the speciﬁed frequency ω = ωc, will be at
ωd = µ−1(ωc)
(3.11)
In principle, one could argue that the frequency response change in Fig. 3.13
is not that drastic and could be tolerated, especially since the deviation occurs
mostly in the high frequency range, which is not the most audible part of the
frequency spectrum. This might have been the case with the 1-pole lowpass
ﬁlter, however for other ﬁlters with more complicated amplitude responses it
won’t be as acceptable. Fig. 3.15 illustrates the frequency error for a 2-pole
resonating lowpass ﬁlter. The resonance peaks (which occur close to the ﬁlter’s
cutoﬀ) are very audible and so would be their detuning, which according to
Fig. 3.14 is in the range of semitones. Particularly, at 16kHz the dashed curve
in Fig. 3.14 shows a detuning of ca. 1 octave, meaning that we would have a
resonance at this point when it should have been occurring at ca. 32kHz.
62
CHAPTER 3. TIME-DISCRETIZATION
|H(ejω)|, dB
+12
+6
0
-6
2
4
8
11.025
16
22.05
f, kHz
Figure 3.15: Amplitude response of an unprewarped bilinear-
transformed resonating 2-pole lowpass ﬁlter for a number of dif-
ferent cutoﬀs. Dashed curves represent the respective analog ﬁlter
responses for the same cutoﬀs. Sampling rate 44.1kHz.
Prewarping at cutoff
As a general rule (to which there are exceptions), we would like the cutoﬀ point
of the ﬁlter to be positioned exactly at the speciﬁed cutoﬀ frequency ωc. In this
regard we could notice that if we used a diﬀerent cutoﬀ value
˜ωc = µ(ωc)
(3.12)
then (3.11) would give
ωd = µ−1(˜ωc) = µ−1(µ(ωc)) = ωc
and the cutoﬀ point would be exactly where we wanted it to be. Fig. 3.16 illus-
trates. The cutoﬀ correction (3.12) is a standard technique used in combination
with the bilinear transform. It is referred to as cutoff prewarping.
Technically, cutoﬀ prewarping means that we use ˜ωc instead of ωc in the
gains of the ﬁlter’s integrators. However, the integrator gains are not exactly
ωc but rather ωcT /2. From (3.12) and (3.10) we have
ωcT
2
Thus, we can directly apply (3.13) to compute the prewarped gains ˜ωcT /2. Note
that (3.13) is essentially identical to (3.7).
ωcT
2
˜ωcT
2
(3.13)
= tan
2
T
T
2
tan
=
·
The cutoﬀ prewarping redistributes the frequency error shown in Fig. 3.14.
In the absence of prewarping the error was zero at ω = 0 and monotonically
growing as ω increases. With the cutoﬀ prewarping the error is zero at ω = ωc
instead and grows further away from this point.
Indeed, let H(jω) be unit-cutoﬀ analog response of the ﬁlter in question.
And let’s pick up an analog frequency ωa and ﬁnd the respective detuning. The
correct frequency response at ωa is
Ha(ωa) = H(jωa/ωc)
(3.14a)
3.8. CUTOFF PREWARPING
63
|H(ejω)|, dB
0
-3
-6
-9
2
4
8
11.025
16
22.05
f kHz
Figure 3.16: Amplitude response of a prewarped bilinear-
transformed 1-pole lowpass ﬁlter for a number of diﬀerent cutoﬀs.
Dashed curves represent the respective analog ﬁlter responses for
the same cutoﬀs. Sampling rate 44.1kHz.
On the other hand, given the prewarped cutoﬀ ˜ωc = µ(ωc), the digital frequency
response at some frequency ωd is
Hd(ejωd ) = H(jµ(ωd)/˜ωc) = H(jµ(ωd)/µ(ωc))
(3.14b)
We want to ﬁnd such ωd that the arguments of H(jω) in (3.14a) and (3.14b)
are identical:
µ(ωd)
µ(ωc)
=
ωa
ωc
(3.15)
From where
ωd = µ−1
(cid:19)
(cid:18)
ωa
µ(ωc)
ωc
The solid curves in Fig. 3.17 illustrate the respective detuning ωa/ωd (in semi-
tones) of analog frequencies.
Alternatively from (3.15) we could express ωa as a function of ωd:
ωa =
ωc
µ(ωc)
µ(ωd)
thus expressing the analog frequency detuning ωa/ωd as a function of ωd. The
dashed curves in Fig. 3.17 illustrate.
Apparently, the maximum error to the left of ω = ωc is attained at ω = 0.
Letting ωa → 0 and, equivalently, ωd → 0 we have µ(ωd) ∼ ωd and (3.15) turns
into
ωd
µ(ωc)
∼
ωa
ωc
or
ωd
ωa
∼
µ(ωc)
ωc
and thus the detuning at ω = 0 is
(cid:12)
(cid:12)
(cid:12)
(cid:12)ω=0
∆P
= log2
µ(ωc)
ωc
(3.16)
64
∆P, st
+12
+8
+4
0
−4
−8
−12
CHAPTER 3. TIME-DISCRETIZATION
2
4
8
11.025
16
22.05
fa, kHz
Figure 3.17: Prewarped bilinear transform’s detuning of analog fre-
quencies plotted against analog frequency (solid curves) or digital
frequency (dashed curves). Diﬀerent curves correspond to pre-
warping at diﬀerent cutoﬀ frequencies. Sampling rate 44.1kHz.
Other prewarping points
We have just developed the prewarping technique from the condition that the
cutoﬀ point must be preserved by the mapping (3.6). However instead we could
have required any other point ωp to be preserved.
Given the cutoﬀ ωc, the analog response at ωp is
Ha(ωp) = H(jωp/ωc)
(3.17a)
On the other hand, given the prewarped cutoﬀ ˜ωc (we want to prewarp at
a diﬀerent point now, therefore we don’t know yet, what is the relationship
between ωc and ˜ωc) the digital frequency response at ωp is
Hd(ejωp ) = H(jµ(ωp)/˜ωc)
(3.17b)
We want to ﬁnd such ˜ωc that the arguments of H(jω) in (3.17a) and (3.17b)
are identical:
and
ωp
ωc
=
µ(ωp)
˜ωc
˜ωc =
µ(ωp)
ωp
ωc
(3.18)
3.8. CUTOFF PREWARPING
65
Equation (3.18) is the generalized prewarping formula, where ωp is the frequency
response point of zero detuning. We will refer to ωp as the prewarping point.
According to (3.18) prewarping at ωp simply means that the cutoﬀ should
be multiplied by µ(ωp)/ωp. At ωp = ωc this multiplication reduces to (3.12).
In order to ﬁnd the detuning at other frequencies, notice that equations
(3.14) turn into
Ha(ωa) = H(jωa/ωc)
Hd(ejωd ) = H(jµ(ωd)/˜ωc) = H
(cid:19)
(cid:18)
j
ωpµ(ωd)
ωcµ(ωp)
from where, equating the arguments of H(jω):
we have
ωpµ(ωd)
ωcµ(ωp)
=
ωa
ωc
µ(ωd)
µ(ωp)
=
ωa
ωp
(3.19)
Equation (3.19) is identical to (3.15) except that it has ωp in place of ωc. Thus we
could reuse the results of the previous analysis of the analog frequency detuning.
In particular Fig. 3.17 fully applies, diﬀerent curves corresponding to diﬀerent
prewarping points. At the same time, (3.16) simply turns to
∆P
(cid:12)
(cid:12)
(cid:12)
(cid:12)ω=0
= log2
µ(ωp)
ωp
(3.20)
Bounded cutoff prewarping
Even though cutoﬀ prewarping is an absolutely standard technique and is often
used without any second thought, the need for a diﬀerent choice of the prewarp-
ing point is actually not as exotic as it might seem. Consider e.g. the amplitude
response of a 1-pole highpass ﬁlter prewarped by (3.12), shown in Fig. 3.18.
One can notice a huge discrepancy between analog and digital amplitude re-
sponses occurring well into the audible frequency range [0, 16kHz]. The error is
getting particularly bad at cutoﬀs above 16kHz. In comparison, the responses
of unprewarped ﬁlters in Fig. 3.19 even look kind of better, especially if only the
audible frequency range is considered. This would be even more so, if higher
sampling rates are involved, where the audible range error in Fig. 3.19 would
become smaller, while the same error in Fig. 3.18 can still get as large, given a
suﬃciently high cutoﬀ value.
Apparently, the large error within the audible range in Fig. 3.18 is due to
the detuning error illustrated in Fig. 3.17. This error wasn’t as obvious in the
case of the 1-pole lowpass ﬁlter, since this ﬁlter’s amplitude response is almost
constant to the left of the cutoﬀ point. On the other hand, highpass ﬁlter’s
amplitude response is changing noticeably in the same area, which makes the
detuning error is made much more promiment.
Does this suggest that we shouldn’t use cutoﬀ prewarping with highpass ﬁl-
ters? In principle this is engineer’s decision. However consider the unprewarped
resonating 2-pole highpass ﬁlter’s amplitude response in Fig. 3.20. As with the
66
CHAPTER 3. TIME-DISCRETIZATION
|H(ejω)|, dB
2
4
8
11.025
16
22.05
f kHz
Figure 3.18: Amplitude response of a prewarped bilinear-
transformed 1-pole highpass ﬁlter for a number of diﬀerent cutoﬀs.
Dashed curves represent the respective analog ﬁlter responses for
the same cutoﬀs. Sampling rate 44.1kHz.
|H(ejω)|, dB
0
-3
-6
-9
0
-3
-6
-9
2
4
8
11.025
16
22.05
f, kHz
Figure 3.19: Amplitude response of an unprewarped bilinear-
transformed 1-pole highpass ﬁlter for a number of diﬀerent cutoﬀs.
Dashed curves represent the respective analog ﬁlter responses for
the same cutoﬀs. Sampling rate 44.1kHz.
responating lowpass, the resonance peak detuning is quite prominent here. Also
the diﬀerence in the response value is magniﬁed around the resonance point. All
in all, we’d rather prewarp the cutoﬀ (Fig. 3.21) and tolerate the detuning to
the left of ωc.
However notice, that as the cutoﬀ peak is getting out of the audible range,
we stop caring, where exactly it is positioned, since it can’t be heard anyway.
So, why should we then tolerage the error in the audible range which continues
to increase even faster? Instead, at this moment we could ﬁx the prewarping
3.8. CUTOFF PREWARPING
67
|H(ejω)|, dB
+12
+6
0
-6
2
4
8
11.025
16
22.05
f, kHz
Figure 3.20: Amplitude response of an unprewarped bilinear-
transformed resonating 2-pole highpass ﬁlter for a number of dif-
ferent cutoﬀs. Dashed curves represent the respective analog ﬁlter
responses for the same cutoﬀs. Sampling rate 44.1kHz.
|H(ejω)|, dB
+12
+6
0
-6
2
4
8
11.025
16
22.05
f, kHz
Figure 3.21: Amplitude response of a prewarped bilinear-
transformed resonating 2-pole highpass ﬁlter for a number of dif-
ferent cutoﬀs. Dashed curves represent the respective analog ﬁlter
responses for the same cutoﬀs. Sampling rate 44.1kHz.
point to the upper boundary of the audible range:
ωp =
(cid:40)
ωc
ωmax
if ωc ≤ ωmax
if ωc ≥ ωmax
or simply
ωp = min {ωc, ωmax}
(3.21)
where ωmax is some point around 16kHz. At least then the detuning in the
audible range won’t grow any further than it is at ωc = ωmax (Fig. 3.22). The
picture gets even better at higher sampling rates (Fig. 3.23).
68
CHAPTER 3. TIME-DISCRETIZATION
|H(ejω)|, dB
+12
+6
0
-6
2
4
8
11.025
16
22.05
f, kHz
Figure 3.22: Eﬀect of cutoﬀ prewarping bounded at 16kHz. Sam-
pling rate 44.1kHz.
|H(ejω)|, dB
+12
+6
0
-6
-12
-18
1
2
16
22.05
32
44.1
f, kHz
Figure 3.23: Eﬀect of cutoﬀ prewarping bounded at 16kHz. Sam-
pling rate 88.1kHz.
Substituting (3.21) into (3.18) we obtain
˜ωc =
µ(min {ωc, ωmax})
min {ωc, ωmax}
ωc =



µ(ωc)
µ(ωmax)
ωmax
if ωc ≤ ωmax
ωc
if ωc ≥ ωmax
(3.22)
The mapping deﬁned by (3.22) is shown in Fig. 3.24. Note that thereby we
become able to specify the cutoﬀs beyond Nyquist, and actually to specify arbi-
trarily large cutoﬀs, since the new mapping curve is crossing the former vertical
asymptote at ωc = π/2.
3.8. CUTOFF PREWARPING
69
˜ωc
µ(ωmax)
0
ωmax
π/2
ωc
Figure 3.24: Bounded cutoﬀ prewarping. The thick dashed line
shows the unbounded prewarping continuation. The thin oblique
dashed line is the continuation of the straight part of the prewarp-
ing curve. The black dot marks the breakpoint of the prewarping
curve.
The breakpoint at ωc = ωmax in Fig. 3.24 can be somewhat unexpected. In
order to understand the mechanism behind its appearance suppose ωc is varying
with time. Using (3.18) we compute the time derivative of ˜ωc:
˙˜ωc = ωc
d
dt
µ(ωp)
ωp
+
µ(ωp)
ωp
˙ωc
As ωc becomes larger than ωmax the ﬁrst term suddenly disappears and there is a
jump in ˙˜ωc. In other words, the variation of the prewarping point makes its own
contribution to ˙˜ωc. As soon as the variation stops, the respective contribution
abruptly disappears.
The frequency detunings occurring in case of (3.22) can be found directly
from Fig. 3.17, keeping in mind that (3.22) is simply another expression of
(3.21).
Continuous-speed prewarping
The breakpoint occurring in Fig. 3.24 might be undesirable if the cutoﬀ is being
modulated, since there can be a sudden change of the perceived modulation
speed as the cutoﬀ traverses through the prewarping breakpoint. For that reason
it might be desirable to smooth the breakpoint in one way or the other. The
simplest approach would be to continue the curve as a tangent line after the
CHAPTER 3. TIME-DISCRETIZATION
˜ωc =
(cid:40)µ(ωc)
if ωc ≤ ωmax
µ(ωmax) + (ωc − ωmax)µ(cid:48)(ωmax)
if ωc ≥ ωmax
(3.23)
70
breakpoint:
where
µ(cid:48)(ω) =
d
dω
(cid:18) 2
T
tan
(cid:19)
=
ωT
2
1
cos2 ωT
2
= 1 + tan2 ωT
2
= 1 +
(cid:19)2
µ(ω)
(cid:18) T
2
is the derivative of µ(ω). Note, however, that this will no longer keep ωmax as
the prewarping point and the situation would be something in between (3.12)
and (3.22).
At ωc → ∞ from (3.23) we have
˜ωc = µ(ωmax) + (ωc − ωmax)µ(cid:48)(ωmax) ∼ µ(cid:48)(ωmax)ωc
(3.24)
Comparing the right-hand side of (3.24) to (3.18) we obtain the equation for
the eﬀective prewarping point at inﬁnity:
µ(ωp)
ωp
= µ(cid:48)(ωmax)
(3.25)
In principle ωp can be found from (3.25), however we are not so much interested
in how far oﬀ will be the prewarping point, as in the estimation of the associated
increase in detuning. By (3.20)
∆P
(cid:12)
(cid:12)
(cid:12)
(cid:12)ω=0
= log2
µ(ωp)
ωp
= log2 µ(cid:48)(ωmax)
(at ωc = ∞)
which for ωmax = 16kHz at 44.1kHz sampling rate gives ca. 2.5 octaves, while
In comparison, at
at 88.2kHz sampling rate it gives only about 0.5 octave.
ωc = ωmax (and respectively ωp = ωmax) we would have a smaller value
∆P
(cid:12)
(cid:12)
(cid:12)
(cid:12)ω=0
= log2
µ(ωp)
ωp
= log2
µ(ωmax)
ωmax
which for ωmax = 16kHz at 44.1kHz sampling rate gives ca. 1 octave, while at
88.2kHz sampling rate it gives only about 2 semitones.
We have therefore found the eﬀect of (3.23) on the detuning occurring at
ω = 0 for ωc → ∞. It would also be nice to estimate the same eﬀect at ω = ωmax.
By (3.19)
which by (3.25) becomes
Letting ωd = ωmax we have
ωa =
µ(ωmax)
µ(cid:48)(ωmax)
=
µ(ωd)
ωa
=
µ(ωp)
ωp
µ(ωd)
ωa
= µ(cid:48)(ωmax)
2
T
tan
ωmaxT
2
(cid:18) ωmaxT
2
cos−2
(cid:19) =
1
T
sin (ωmaxT )
3.8. CUTOFF PREWARPING
71
and the detuning itself is the logarithm of the ratio
ωa
ωd
=
ωa
ωmax
=
sin (ωmaxT )
ωmaxT
= sinc (ωmaxT )
For ωmax = 16kHz at 44.1kHz sampling rate this gives ca. −1.5 octaves, while
at 88.1kHz it gives ca. −4 semitones.
Since (as illustrated by Fig. 3.17) the detuning is a monotonic function of
ω, at ωc = ∞ we are having the audible range detuning error in the range of
ca. [−1.5, 2.5] octaves at 44.1kHz sampling rate and in the range of ca. [−4, 6]
semitones at 88.2kHz sampling rate. Therefore it is more or less balanced out,
although being a bit larger at higher frequencies.
Apparently, more elaborate ways of smoothing the breakpoint in Fig. 3.24
may be designed, but we won’t cover them here as the options are almost inﬁnite.
Prewarping of systems of filters
According to (3.18), prewarping is nothing more than a multiplication of the
cutoﬀ gains of the integrators by µ(ωp)/ωp, where ωp is the prewarping point.
Suppose we are having a system consisting of several ﬁlters connected to-
gether. When prewarping ﬁlters in such system, it would be a good idea to
choose a common prewarping point for all ﬁlters. In this case we multiply their
cutoﬀs by one and the same coeﬃcient. Thereby their amplitude and phase
responses are shifted by one and the same amount (in the logarithmic frequency
axis), and they all retain the frequency response relationships which existed
between them prior to prewarping. Eﬀectively this is the same as changing the
“common cutoﬀ” of the ﬁlter system, and the frequency response of the entire
system is simply shifted horizontally by the same amount, fully retaining its
shape.
On the other hand by prewarping them independently we shift the frequency
response of each ﬁlter diﬀerently from the others and the amplitude and phase
relationships between those are thereby destroyed. Therefore the amplitude and
phase response shapes of the entire system of ﬁlters are not preserved.
As an illustration consider a parallel connection of a 1-pole lowpass and a
1-pole highpass ﬁlter, the lowpass cutoﬀ being ωc/2, the highpass cutoﬀ being
2ωc where ωc is the formal cutoﬀ of the system:
•(cid:47)
LP
HP
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
The transfer function of such system (written in the unit-cutoﬀ form) is
H(s) =
1
1 + 2s
+
s/2
1 + s/2
The result of prewarping the lowpass and the highpass separately at their respec-
tive cutoﬀs ωc/2 and 2ω is shown in Fig. 3.25. Actually for the ωc = 11.025kHz
and ωc = 16kHz the highpass cutoﬀ 2ωc needed to be clipped prior to prewaring,
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
72
CHAPTER 3. TIME-DISCRETIZATION
since it is equal or exceeds Nyquist and cannot be directly prewarped by (3.12).
Compare to Fig. 3.26 where the prewarping of both ﬁlters has been done at the
common point ωc.
|H(ejω)|, dB
2
4
8
11.025
16
22.05
f, kHz
Figure 3.25: Separate prewarping of system components (for a
number of diﬀerent cutoﬀs). Dashed curves represent the respec-
tive analog ﬁlter responses for the same cutoﬀs. Sampling rate
44.1kHz.
|H(ejω)|, dB
0
-6
-12
-18
0
-6
-12
-18
2
4
8
11.025
16
22.05
f, kHz
Figure 3.26: Common-point prewarping of system components (for
a number of diﬀerent cutoﬀs). Dashed curves represent the respec-
tive analog ﬁlter responses for the same cutoﬀs. Sampling rate
44.1kHz.
Other prewarping techniques
With 1-pole lowpass and highpass ﬁlters the only available control parameter is
the ﬁlter cutoﬀ. Thus the only option which we had for compensating the bilin-
ear transform’s frequency detuning was correcting the cutoﬀ value. Other ﬁlters
3.9. ZERO-DELAY FEEDBACK
73
may have more parameters available. Usually their parameters (e.g. resonance)
will have a strong “vertical” eﬀect on the amplitude response and thus are not
very suitable for compensating the frequency detuning. However in some cases
there will be further options of horizonally altering the ﬁlter’s amplitude (and
phase) responses without causing noticeable changes in the vertical direction.
In such cases we will have further options for more detailed compensation of the
frequency detuning.
Note, however, that these compensations, being not expressible as cutoﬀ
multiplication, may destroy the frequency response of a system of ﬁlters, unless
there is some other way to make them have identical eﬀect on all of the ﬁlters
in the system. We will discuss some examples of this later in the book.
3.9 Zero-delay feedback
There is a further problem with the trapezoidal integrator replacement in the
TPT method. Replacing the integrators with trapezoidal ones introduces delay-
less feedback loops (that is, feedback loops not containing any delay elements)
into the structure. E.g. consider the structure in Fig. 3.12. Carefully examining
this structure, we ﬁnd that it has a feedback loop which doesn’t contain any
unit delay elements. This loop goes from the leftmost summator through the
gain, through the upper path of the integrator to the ﬁlter’s output and back
through the large feedback path to the leftmost summator.
Why is this delayless loop a problem? Let’s consider for example the naive
lowpass ﬁlter structure in Fig. 3.5. Suppose we don’t have the respective pro-
gram code representation and wish to obtain it from the block diagram. We
could do it in the following way. Consider Fig. 3.27, which is the same as Fig. 3.5,
except that it labels all signal points. At the beginning of the computation of
a new sample the signals A and B are already known. A = x[n] is the current
input sample and B is taken from the internal state memory of the z−1 element.
Therefore we can compute C = A − B. Then we can compute D = (ωcT )C
and ﬁnally E = D + B. The value of E is then stored into the internal memory
of the z−1 element (for the next sample computation) and is also sent to the
output as the new y[n] value. Easy, right?
A
x[n]
C
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
D
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
E
•(cid:47)
y[n]
z−1
B
•(cid:15)
Figure 3.27: Naive 1-pole lowpass ﬁlter and the respective signal
computation order.
Now the same approach doesn’t work for the structure in Fig. 3.12. Because
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
74
CHAPTER 3. TIME-DISCRETIZATION
there is a delayless loop, we can’t ﬁnd a starting point for the computation
within that loop.
The classical way of solving this problem is exactly the same as what we had
in the naive approach: introduce a z−1 into the delayless feedback, turning it
into a feedback containing a unit delay (Fig. 3.28). Now there are no delayless
feedback paths and we can arrange the computation order in a way similar to
Fig. 3.27. This however destroys the resulting frequency response, because the
transfer function is now diﬀerent. In fact the obtained result is not signiﬁcantly
better (if better at all) than the one from the naive approach. There are some
serious artifacts in the frequency response closer to the Nyquist frequency, if the
ﬁlter cutoﬀ is suﬃciently high.
•(cid:47)
•(cid:47)
y[n]
x[n]
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT /2
•(cid:47)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1 (cid:111)
Figure 3.28: Digital 1-pole lowpass ﬁlter with a trapezoidal inte-
grator and an extra delay in the feedback.
Therefore we shouldn’t introduce any modiﬁcations into the structure and
solve the zero-delay feedback problem instead. The term “zero-delay feedback”
originates from the fact that we avoid introducing a one-sample delay into the
feedback (like in Fig. 3.28) and instead keep the feedback delay equal to zero.
So, let’s solve the zero-delay feedback problem for the structure in Fig. 3.12.
Notice that this structure simply consists of a negative feedback loop around
a trapezoidal integrator, where the trapezoidal integrator structure is exactly
the one from Fig. 3.11. We will now introduce the concept of the instantaneous
response of this integrator structure.
So, consider the integrator structure in Fig. 3.11. Since there are no delayless
loops in the integrator, it’s not diﬃcult to obtain the following expression for
y[n]:
y[n] =
ωcT
2
x[n] + u[n − 1]
(3.26)
Notice that, at the time x[n] arrives at the integrator’s input, all values in
the right-hand side of (3.26) are known (no unknown variables). Introducing
notation
we have
g =
ωcT
2
s[n] = u[n − 1]
y[n] = gx[n] + s[n]
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
3.9. ZERO-DELAY FEEDBACK
75
or, dropping the discrete time argument notation for simplicity,
y = gx + s
That is, at any given time moment n, the output of the integrator y is a linear
function of its input x, where the values of the parameters of this linear function
are known. The g parameter doesn’t depend on the internal state of the integra-
tor, while the s parameter does depend on the internal state of the integrator.
We will refer to the linear function f (x) = gx + s as the instantaneous response
of the integrator at the respective implied time moment n. The coeﬃcient g can
be referred to as the instantaneous response gain or simply instantaneous gain.
The term s can be referred to as the instantaneous response offset or simply
instantaneous offset.
Let’s now redraw the ﬁlter structure in Fig. 3.12 as in Fig. 3.29. We have
changed the notation from x to ξ in the gx + s expression to avoid the confusion
with the input signal x[n] of the entire ﬁlter.
x[n]
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
gξ + s
•(cid:47)
y[n]
Figure 3.29: 1-pole TPT lowpass ﬁlter with the integrator in the
instantaneous response form.
Now we can easily write and solve the zero-delay feedback equation. Indeed,
suppose we already know the ﬁlter output y[n]. Then the output signal of the
feedback summator is x[n] − y[n] and the output of the integrator is respectively
g(x[n] − y[n]) + s. Thus
y[n] = g(x[n] − y[n]) + s
or, dropping the time argument notation for simplicity,
y = g(x − y) + s
(3.27)
The equation (3.27) is the zero-delay feedback equation for the ﬁlter in Fig. 3.29
(or, for that matter, in Fig. 3.12). Solving this equation, we obtain
and respectively
y(1 + g) = gx + s
y =
gx + s
1 + g
(3.28)
Having found y (that is, having predicted the output y[n]), we can then proceed
with computing the other signals in the structure in Fig. 3.12, beginning with
the output of the leftmost summator.14
14Notice that the choice of the signal point for the prediction is rather arbitrary. We could
have chosen any other point within the delayless feedback loop.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
76
CHAPTER 3. TIME-DISCRETIZATION
It’s worth mentioning that (3.28) can be used to obtain the instantaneous
response of the entire ﬁlter from Fig. 3.12. Indeed, rewriting (3.28) as
and introducing notations
y =
g
1 + g
x +
s
1 + g
G =
S =
g
1 + g
s
1 + g
we have
y = Gx + S
(3.29)
So, the instantaneous response of the entire lowpass ﬁlter in Fig. 3.12 is again a
linear function of the input. We could use the expression (3.29) e.g. to solve the
zero-delay feedback problem for some larger feedback loop containing a 1-pole
lowpass ﬁlter.
3.10 Implementations
1-pole lowpass
We are now going to convert the structure in Fig. 3.12 into a piece of code. Let’s
introduce helper variables into Fig. 3.12 as shown in Fig. 3.30, where we have
used the already known to us fact that, given the integrator’s instantaneous
response gx + s, the value of s equals the output of the z−1 element.
x[n]
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT /2
v
•(cid:47)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
•(cid:47)
y[n]
+
s
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
u
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 3.30: 1-pole TPT lowpass ﬁlter with helper variables.
We already know y from (3.29). Since g = ωcT /2, we have
v = g(x − y) = g (x − Gx − S) = g
x −
(cid:18)
g
1 + g
x −
(cid:19)
=
s
1 + g
= g
(cid:18) 1
1 + g
x −
(cid:19)
s
1 + g
= g
x − s
1 + g
(3.30)
Now (3.30) gives a direct expression for v in terms of known signals, not using
y. In order to avoid unnecessary computations, we can simply reobtain y using
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
3.10.
IMPLEMENTATIONS
the obvious from Fig. 3.30 fact that y is just a sum of v and s:
y = v + s
77
(3.31)
We also need the z−1 input (which we need to store in the z−1’s memory) which
is also obtained from Fig. 3.30 in an obvious way:
u = y + v
(3.32)
The equations (3.30), (3.31) and (3.32) can be directly expressed in program
code:
// perform one sample tick of the lowpass filter
// G = g/(1+g)
// the variable ’s’ contains the state of z^-1 block
v := (x-s)*G;
y := v + s;
s := y + v;
or instead expressed in a block diagram form (Fig. 3.31). Notice that the block
diagram doesn’t contain any delayless loops anymore.
g/(1 + g)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
x[n]
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
y[n]
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:79)
z−1
+
Figure 3.31: 1-pole TPT lowpass ﬁlter with resolved zero-delay
feedback.
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
1-pole multimode
The highpass signal can be obtained from the structure in Fig. 3.31 in a trivial
manner, since yHP = x−yLP, thereby turning Fig. 3.31 into a multimode 1-pole.
1-pole highpass
If we need only the highpass signal, we could do it in a smaller number of
operations than in the multimode 1-pole. By noticing that
yHP = x − yLP = x − (v + s) = (x − s) − v = (x − s) −
g
1 + g
(x − s) =
1
1 + g
(x − s)
and that
u = yLP + v = s + 2v = s +
2g
1 + g
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
78
CHAPTER 3. TIME-DISCRETIZATION
we can implement the highpass ﬁlter as follows:
// perform one sample tick of the highpass filter
// Ghp = 1/(1+g)
// the variable ’s’ contains the state of z^-1 block
xs := x - s;
y := xs*Ghp;
s := s + y*2g;
however this way we have traded an addition/subtraction pair for one multipli-
cation, plus instead of one cutoﬀ-dependent parameter G we need to store and
access Ghp and 2g. Therefore this is not necessarily a performance improve-
ment.
1-pole allpass
The allpass signal can be obtained from the multimode 1-pole in a trivial man-
ner, recalling that yAP = yLP −yHP. However, if we need only the allpass signal,
we could save a couple of operations. Noticing that
yAP = yLP − yHP = v + s − (x − (v + s)) = (s + 2v) − (x − s)
and that s + 2v = u is the new state of the z−1 block, we obtain
// perform one sample tick of the allpass filter
// 2Glp=2g/(1+g)
// the variable ’s’ contains the state of z^-1 block
xs := x - s;
s := s + xs*2Glp;
y := s - xs;
3.11 Direct forms
Consider again the equation (3.5), which describes the application of the bilin-
ear transform to convert an analog transfer function to a digital one. There is a
classical method of digital ﬁlter design which is based directly on this transfor-
mation, without using any integrator replacement techniques. In the author’s
experience, for music DSP needs this method typically has a largely inferior
quality, compared to the TPT. Nevertheless we will describe it here for com-
pleteness and for a couple of other reasons. Firstly, it would be nice to try to
analyse and understand the reasons for the problems of this method. Secondly,
this method could be useful once in a while. Particularly, its deﬁciencies mostly
disappear in the time-invariant (unmodulated or suﬃciently slowly modulated)
case.
Having obtained a digital transfer function from (3.5), we could observe,
that, since the original analog transfer function was a rational function of s, the
resulting digital transfer function will necessarily be a rational function of z.
E.g. using the familiar 1-pole lowpass transfer function
Ha(s) =
ωc
s + ωc
3.11. DIRECT FORMS
79
we obtain
Hd(z) = Ha
(cid:18) 2
T
·
z − 1
z + 1
(cid:19)
=
ωc
T · z−1
z+1 + ωc
2
=
=
ωcT
2 (z + 1)
(z − 1) + ωcT
2 (z + 1)
=
ωcT
2 (z + 1)
(cid:1) z − (cid:0)1 − ωcT
(cid:1)
2
(cid:0)1 + ωcT
2
Now, there are standard discrete-time structures allowing an implementation
It is easier to use
of any given nonstrictly proper rational transfer function.
these structures, if the transfer function is expressed as a rational function of
z−1 rather than the one of z. In our particular example, we can multiply the
numerator and the denominator by z−1, obtaining
Hd(z) =
ωcT
2 (1 + z−1)
(cid:1) − (cid:0)1 − ωcT
2
(cid:0)1 + ωcT
2
(cid:1) z−1
The further requirement is to have the constant term in the denominator equal
to 1, which can be achieved by dividing everything by 1 + ωcT /2:
Hd(z) =
(1 + z−1)
ωc T
2
1+ ωc T
2
1 − 1− ωc T
2
1+ ωc T
2
z−1
(3.33)
Now suppose we have an arbitrary rational nonstrictly proper transfer function
of z, expressed via z−1 and having the constant term in the denominator equal
to 1:
N
(cid:88)
bnz−n
H(z) =
n=0
N
(cid:88)
1 −
n=1
anz−n
(3.34)
This transfer function can be implemented by the structure in Fig. 3.32 or by
the structure in Fig. 3.33. One can verify (by computing the transfer functions
of the respective structures) that they indeed implement the transfer function
(3.34). There are also transposed versions of these structures, which the readers
should be able to construct on their own.
Let’s use the direct form II to implement (3.33). Apparently, we have
N = 1
b0 = b1 =
ωcT
2
1 + ωcT
2
a1 =
1 − ωcT
2
1 + ωcT
2
and the direct form implementation itself is the one in Fig. 3.34 (we have merged
the b0 and b1 coeﬃcients into a single gain element).
In the time-invariant (unmodulated) case the performance of the direct form
ﬁlter in Fig. 3.34 should be identical to the TPT ﬁlter in Fig. 3.12, since both
80
CHAPTER 3. TIME-DISCRETIZATION
x(t)
•(cid:47)
z−1
•(cid:47)
z−1
•(cid:47)
. . .
z−1
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
b0
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:15)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:15)
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
b1
+
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
a1
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
b2
+
. . .
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
a2
z−1
•(cid:47)
z−1
•(cid:47)
. . .
z−1
Figure 3.32: Direct form I (DF1).
+
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
a1
+
. . .
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
a2
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
bN
+
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
aN
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
aN
z−1
•(cid:47)
z−1
•(cid:47)
. . .
z−1
•(cid:47)
y(t)
x(t)
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
b0
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
b1
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
b2
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
bN
y(t)
+
+
+
. . .
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 3.33: Direct form II (DF2), a.k.a. canonical form.
b0
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
y[n]
x[n]
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)(cid:49)(cid:49)(cid:49)(cid:79)
a1
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
z−1
•(cid:15)
Figure 3.34: Direct form II 1-pole lowpass ﬁlter.
implement the same bilinear-transformed analog transfer function (2.5). When
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:15)
(cid:111)
(cid:111)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
3.11. DIRECT FORMS
81
the cutoﬀ is modulated, however, the performance will be diﬀerent.
We have already discussed in Sections 2.7 and 2.16 that diﬀerent topologies
may have diﬀerent time-varying behavior even if they share the same transfer
function. Apparently, the diﬀerence in behavior between Fig. 3.34 and Fig. 3.12
is another example of that. Comparing the implementations in Figs. 3.34 and
3.12, we notice that the structure in Fig. 3.34 contains a gain element at the
output, the value of this gain being approximately proportional to the cutoﬀ (at
low cutoﬀs). This will particularly produce unsmoothed jumps in the output in
response to jumps in the cutoﬀ value. In the structure in Fig. 3.12, on the other
hand, the cutoﬀ jumps will be smoothed by the integrator. Thus, the diﬀerence
between the two structures is similar to the just discussed eﬀect of the cutoﬀ
gain placement with the integrator.
We should conclude that, other things being equal, the structure in Fig. 3.34
is inferior to the one in Fig. 3.12 (or Fig. 3.31). In this respect consider that
Fig. 3.12 is trying to explicitly emulate the analog integration behavior, preserv-
ing the topology of the original analog structure, while Fig. 3.34 is concerned
solely with implementing a correct transfer function. Since Fig. 3.34 implements
a classical approach to the bilinear transform application for digital ﬁlter design
(which ignores the ﬁlter topology) we’ll refer to the trapezoidal integration re-
placement technique as the topology-preserving bilinear transform (or, shortly,
TPBLT). Or, even shorter, we can refer to this technique as simply the topology-
preserving transform (TPT), implicitly assuming that the bilinear transform is
being used.15
In principle, sometimes there are possibilities to “manually ﬁx” the struc-
tures such as in Fig. 3.34. E.g. the time-varying performance of the latter is
drastically improved by moving the b0 gain to the input. The problem however
is that this kind of ﬁxing quickly gets more complicated (if being possible at
all) with larger ﬁlter structures. On the other hand, the TPT method explicitly
aims at emulating the time-varying behavior of the analog prototype structure,
which aspect is completely ignored by the classical transform approach. Be-
sides, if the structure contains nonlinearities, preserving the topology becomes
absolutely critical, because otherwise these nonlinearites can not be placed in
the digital model.16 Also, the direct forms suﬀer from precision loss issues, the
problem growing bigger with the order of the system. For that reason in practice
the direct forms of orders higher than 2 are rarely used,17 but even 2nd-order
direct forms could already noticeably suﬀer from precision losses.
15Apparently, naive filter design techniques also preserve the topology, but they do a rather
poor job on the transfer functions. Classical bilinear transform approach does a good job on
the transfer function, but doesn’t preserved the topology. The topology-preserving transform
achieves both goals simultaneously.
16This is related to the fact that transfer functions can be defined only for linear time-
invariant systems. Nonlinear cases are obviously not linear, thus some critical information
can be lost, if the conversion is done solely based on the transfer functions.
17A higher-order transfer function is typically decomposed into a product of transfer func-
tions of 1st- and 2nd-order rational functions (with real coefficients!). Then it can be imple-
mented by a serial connection of the respective 1st- and 2nd-order direct form filters.
82
CHAPTER 3. TIME-DISCRETIZATION
3.12 Transient response
Looking at the 1-pole lowpass ﬁlter’s discrete-time transfer function (3.33) and
noticing that ωc = −p where p is the analog pole, we could rewrite (3.33) as
H(z) =
(1 + z−1)
−pT
2
1− pT
2
1 − 1+ pT
2
1− pT
2
z−1
Comparing this to (3.9) we notice that
1 + pT
2
1 − pT
2
= ˜p
where ˜p is the result of the application of the inverse bilinear transform formula
(3.9) to p. Further noticing that
(cid:32)
−pT
2
1 − pT
2
=
1
2
·
1 − pT
2
1 − pT
2
−
1 + pT
2
1 − pT
2
(cid:33)
=
1 − ˜p
2
(3.35)
we rewrite H(z) as
H(z) =
1 − ˜p
2
·
1 + z−1
1 − ˜pz−1 =
1 − ˜p
2
·
z + 1
z − ˜p
Thus ˜p is the pole of H(z), as we should have expected.
On the other hand, applying trapezoidal integration to the 1-pole lowpass
diﬀerential equation in the pole form (2.14), we have
y[n] − y[n − 1] = pT ·
(cid:18) y[n] + y[n − 1]
2
−
x[n] + x[n − 1]
2
(cid:19)
from where
(cid:18)
1 −
(cid:19)
pT
2
(cid:18)
y[n] =
1 +
(cid:19)
pT
2
y[n − 1] −
pT
2
· (x[n] + x[n − 1])
y[n] =
1 + pT
2
1 − pT
2
y[n − 1] +
= ˜py[n − 1] + (1 − ˜p)
−pT
2
1 − pT
2
x[n] + x[n − 1]
2
· (x[n] + x[n − 1]) =
where we have used (3.35).
Now consider a complex exponential x[n] = X(z)zn. For such x[n] we have
y[n] = ˜py[n − 1] + (1 − ˜p)
1 + z−1
2
X(z)zn = ˜py[n − 1] + ˜qzn
(3.36)
where we introduced notation
˜q = (1 − ˜p)
1 + z−1
2
X(z)
3.13.
INSTANTANEOUSLY UNSTABLE FEEDBACK
83
for conciseness. Recursively substituting (3.36) into itself at progessively de-
creasing values of n we obtain
y[n] = ˜py[n − 1] + ˜qzn =
= ˜p (cid:0)˜py[n − 2] + ˜qzn−1(cid:1) + ˜qzn =
= ˜p2y[n − 2] + (cid:0)˜pz−1 + 1(cid:1) ˜qzn =
= ˜p2 (cid:0)˜py[n − 3] + ˜qzn−2(cid:1) + (cid:0)˜pz−1 + 1(cid:1) ˜qzn =
= ˜p3y[n − 3] +
(cid:16)(cid:0)˜pz−1(cid:1)2
+ ˜pz−1 + 1
(cid:17)
˜qzn =
. . .
= ˜pny[0] +
= ˜pny[0] +
(cid:16)(cid:0)˜pz−1(cid:1)n−1
(cid:0)˜pz−1(cid:1)n
− 1
˜pz−1 − 1
+ (cid:0)˜pz−1(cid:1)n−2
+ . . . + ˜pz−1 + 1
(cid:17)
˜qzn =
˜qzn = ˜pny[0] +
˜pn − zn
˜p − z
˜qz =
(1 − ˜p)
z + 1
2
= ˜pny[0] +
zn − ˜pn
z − ˜p
= ˜pny[0] + (zn − ˜pn)H(z)X(z) =
= H(z)X(z)zn + (y[0] − H(z)X(z)) · ˜pn =
= ys[n] + (y[0] − ys[0]) · ˜pn = ys[n] + yt[n]
X(z) =
where
ys[n] = H(z)X(z)zn
yt[n] = (y[0] − ys[0]) · pn
are the steady-state and transient responses respectively.
Thus, the discrete-time 1-pole transient response is a decaying exponent ˜pn,
provided the discrete-time system is stable and |˜p| < 1. If |˜p| > 1 the transient
response grows inﬁnitely.
3.13
Instantaneously unstable feedback
Writing the solution (3.28) for the zero-delay feedback equation (3.27) we in fact
have slightly jumped the gun. Why? Let’s consider once again the structure in
Fig. 3.29 and suppose g gets negative and starts growing in magnitude further
in the negative direction.18 When g becomes equal to −1, the denominator of
(3.28) turns into zero. Something bad must be happening at this moment.
Instantaneous smoother
In order to understand the meaning of this situation, let’s consider the delayless
feedback path as if it was an analog feedback. An analog signal value can’t
change instantaneously.
It can change very quickly, but not instantaneously,
it’s always a continuous function of time. We could imagine there is a smoother
18Of course, such lowpass filter formally has a negative cutoff value. It is also unstable.
However unstable circuits are very important as the linear basis for the analysis and imple-
mentation of e.g. nonlinear self-oscillating filters. Therefore we wish to be able to handle
unstable circuits as well.
84
CHAPTER 3. TIME-DISCRETIZATION
unit somewhere in the feedback path (Fig. 3.35). This smoother unit has a
very very fast response time. We introduce the notation ¯y for the output of the
smoother.
x[n]
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
gξ + s
•(cid:47)
y[n]
¯y
ˆσ
Figure 3.35: Digital 1-pole lowpass ﬁlter with a trapezoidal inte-
grator in the instantaneous response form and a smoother unit ˆσ
in the delayless feedback path.
So, suppose we wish to compute a new output sample y[n] for the new input
sample x[n]. At the time x[n] “arrives” at the ﬁlter’s input, the smoother still
holds the old output value y[n − 1]. Let’s freeze the discrete time at this point
(which formally means we simply are not going to update the internal state
of the z−1 element). At the same time we will let the continuous time t run,
formally starting at t = 0 at the discrete time moment n.
In this time-frozen setup we can choose arbitrary units for the continuous
time t. The smoother equation can be written as
sgn ˙¯y(t) = sgn(cid:0)y(t) − ¯y(t)(cid:1)
That is, we don’t specify the details of the smoothing behavior, however the
smoother output always changes in the direction from ¯y towards y at some (not
necessarily constant) speed.19 Particularly, we can simply deﬁne a constant
speed smoother:
˙¯y = sgn(y − ¯y)
or we could use a 1-pole lowpass ﬁlter as a smoother:
˙¯y = y − ¯y
The initial value of the smoother is apparently ¯y(0) = y[n − 1].
Now consider that
sgn ˙¯y(t) = sgn(cid:0)y(t) − ¯y(t)(cid:1) = sgn(cid:0)g(x[n] − ¯y(t)) + s − ¯y(t)(cid:1) =
= sgn(cid:0)(gx[n] + s) − (1 + g)¯y(t)(cid:1) = sgn(cid:0)a − (1 + g)¯y(t)(cid:1)
where a = gx[n] + s is constant in respect to t. First, assume 1 + g > 0. Further,
suppose a − (1 + g)¯y(0) > 0. Then ˙¯y(0) > 0 and then the value of the expression
a−(1+g)¯y(t) will start decreasing until it turns to zero at some t, at which point
the smoothing process converges. On the other hand, if a−(1+g)¯y(0) < 0, then
˙¯y(0) < 0 and the value of the expression a − (1 + g)¯y(t) will start increasing until
it turns to zero at some t, at which point the smoothing process converges. If
a − (1 + g)¯y(0) = 0 then the smoothing is already in a stable equilibrium state.
19We also assume that the smoothing speed is sufficiently large to ensure that the smoothing
process will converge at all cases where it potentially can converge (this statement should
become clearer as we discuss more details).
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
3.13.
INSTANTANEOUSLY UNSTABLE FEEDBACK
85
So, in case 1 + g > 0 the instantaneous feedback smoothing process always
converges. Now assume 1 + g ≤ 0. Further, suppose a − (1 + g)¯y(0) > 0. Then
˙¯y(0) > 0 and then the value of the expression a − (1 + g)¯y(t) will start further
increasing (or stay constant if 1 + g = 0). Thus, ¯y(t) will grow indeﬁnitely.
Respectively, if a − (1 + g)¯y(0) < 0, then ¯y(t) will decrease indeﬁnitely. This
indeﬁnite growth/decrease will occur within the frozen discrete time. Therefore
we can say that ¯y grows inﬁnitely in an instant. We can refer to this as to an
instantaneously unstable zero-delay feedback loop.
The idea of the smoother introduced in Fig. 3.35 can be used as a general
means for analysing zero-delay feedback structures for instantaneous instability.
We will refer to this technique as instantaneous smoother.
1-pole lowpass as an instantaneous smoother
The analysis of the instantaneous stability can also be done using the analog
ﬁlter stability analysis means. Let the smoother be an analog 1-pole lowpass
s+1 )20 and notice that in
ﬁlter with a unit cutoﬀ (whose transfer function is
that case the structure in Fig. 3.35 can be redrawn as in Fig. 3.36. This ﬁlter
has two formal inputs x[n] and s and one output y[n].
1
x[n]
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
s
g
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
1
s+1
•
y[n]
Figure 3.36: An instantaneous representation of a digital 1-pole
lowpass ﬁlter with a trapezoidal integrator and an analog lowpass
smoother.
We can now e.g. obtain a transfer function from the x[n] input to the y[n]
output. Ignoring the s input signal (assuming it to be zero), for a continuous-
time complex exponential input signal arriving at the x[n] input, which we
denote as x[n](t), we have a respective continuous-time complex exponential
signal at the y[n] output, which we denote as y[n](t):
from where
that is
(cid:18)
y[n](t) = g
x[n](t) −
(cid:19)
y[n](t)
1
s + 1
y[n](t) =
g
1 + g 1
s+1
x[n](t)
H(s) =
g
1 + g 1
s+1
= g
s + 1
s + (1 + g)
20Apparently, the variable s used in the transfer function 1
s+1 is a different s than the one
used in the instantaneous response expression for the integrator. The author apologizes for
the slight confusion.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
86
CHAPTER 3. TIME-DISCRETIZATION
This transfer function has a pole at s = −(1 + g). Therefore, the structure is
stable if 1 + g > 0 and not stable otherwise.
The same transfer function analysis could have been done between the s
input and the y[n] output, in which case we would have obtained
H(s) =
s + 1
s + (1 + g)
The poles of this transfer function however, are exactly the same, so it doesn’t
matter.21
Generalized zero-delay feedback loop
The zero-delay feedback instantaneous response structure in Fig. 3.29 can be
considered as a particular case of a general one, drawn in Fig. 3.37, where the
input signal x[n] has been incorporated into the s term of the instantaneous
response gξ + s and the negative feedback has been incorporated into the factor
g. Indeed, Fig. 3.37 can be obtained from Fig. 3.29 via
G = −g
S = s + gx
Gξ + S
•(cid:47)
(cid:47) y[n]
Figure 3.37: General zero-delay feedback structure in the instan-
taneous response form.
The zero-delay feedback equation solution written for Fig. 3.37 is obviously
y =
S
1 − G
(3.37)
From the previous discussion it should be clear that the structure becomes
instantaneously unstable for G ≥ 1, that is when the total instantaneous gain
of the feedback loop is 1 or more.
The solution form (3.37) therefore provides a generic means to check an
arbitrary zero-delay feedback loop for instantaneous instability. E.g. rewriting
(3.28) (which we had written for Fig. 3.29) in the form (3.37) we obtain
y =
gx + s
1 − (−g)
where −g is the total instantaneous gain of the feedback loop (including the
feedback inversion), and thus the structure is instantaneously unstable at −g ≥ 1
(or, equivalently, g ≤ −1).
21This is a common rule: the poles of a system with multiple inputs and/or multiple outputs
are always the same regardless of the particular input-output pair for which the transfer
function is being considered (exceptions in singular cases, arising out of pole/zero cancellation
are possible, though).
(cid:47)
(cid:47)
(cid:47)
(cid:47)
3.13.
INSTANTANEOUSLY UNSTABLE FEEDBACK
87
It might be tempting to simply say that the instantaneously unstable zero-
delay feedback occurs whenever the denominator of the zero-delay feedback
equation’s solution becomes zero or negative. However, this actually depends
on how did we arrive at the solution expression. E.g. if we multiply both the
numerator and the denominator of (3.28) by −1, the instantaneously unstable
case will occur for zero or positive denominator values. Therefore, we need to
make sure that our solution is written in the form (3.37) (where we need to verify
that G is the total instantaneous gain of the feedback loop) and only then can
we say that zero or negative denominator values correspond to instantaneously
unstable feedback.
Limits of bilinear transform
We have seen that for 1-poles the continuous- and discrete-time transient re-
sponses are
yt(t) = (y(0) − ys(0)) · ept
yt[n] = (y[0] − ys[0]) · ˜pn
where the the discrete-time pole ˜p is obtained from continuous-time pole p via
inverse bilinear transform (3.9).
In order to compare the transient responses we could compare the growth
of y over one sampling period T :
yt(t) = yt(t − T ) · epT
yt[n] = yt[n − 1] · ˜pn
The comparison of epT vs. ˜p is done in Fig. 3.38. One can notice that as
p → 2/T − 0 the value of ˜p grows too quickly (compared to epT ), approaching
inﬁnity. This means that discrete-time transient response is growing inﬁnitely
fast at p = 2/T , or, respectively as pT /2 = 1. At p > 2/T the value of ˜p
is getting completely diﬀerent from epT , particulary the sign of y[n] begins to
alternate between successive samples.
Now recall that in the 1-pole zero-delay feedback equation (3.28) we had
g = ωcT /2 = −pT /2. Thus, as g = −pT /2 → −1 + 0 the discrete-time transient
response is becoming inﬁnitely fast. Close to this point and further beyond
it, trapezoidal integration doesn’t deliver a reasonable approximation to the
continuous-time case anymore.
If we attempt to interpret the same in terms of bilinear transform, then we
already know (Fig. 3.38) that the inverse bilinear transform (3.9) is becoming
inﬁnitely large at s = 2/T , that is the inverse bilinear transform formula has a
pole at s = 2/T . This means that, if we are having a continuous-time system
with a pole at s ≈ −2/T (which in case of the 1-pole lowpass corresponds to
g = ωcT /2 ≈ −1), then after the bilinear transform the system will have a pole
at z ≈ ∞, and the transformation result doesn’t work really well.
Avoiding instantaneously unstable feedback
Alright, so we have found out that zero-delay feedback structures are instan-
taneously unstable when the total instantaneous gain of the feedback loop is
greater than or equal to 1, but what can we do about it? Firstly, the problem
88
CHAPTER 3. TIME-DISCRETIZATION
2/T
p
1
-1
0
Figure 3.38: e−pT (solid) vs. ˜p = (1 + pT /2)/(1 − pT /2) (thick
dashed) as functions of p. The two thin dashed lines are asymptotes
of (1 + pT /2)/(1 − pT /2).
typically doesn’t occur. Mostly, in (3.37) we have G < 0, e.g. in the 1-pole
lowpass case we have G = −g < 0 for positive cutoﬀ values. Even if G is or
can become positive, the situation G ≥ 1 occurs at really excessive parameter
settings. Therefore one can consider, whether these extreme parameter settings
are so necessary to support, and possibly simply clip the ﬁlter parameters in
such a way that the instantaneous instability doesn’t occur.
Secondly, let’s notice that g = ωcT /2. Therefore another solution could be to
increase the sampling rate, which reduces the sampling period T and respectively
the value of g (from an alternative point of view, it shifts the inverse bilinear
transform’s pole 2/T further away from the origin).
Unstable bilinear transform
There is yet another idea, which is not widely used, but we are going to discuss
it anyway.22 So, the instantaneous instability is occurring at the moment when
one of the analog ﬁlter’s poles hits the pole of the inverse bilinear transform (3.9),
which is located at s = 2/T . On the other hand, recall that the bilinear trans-
form is mapping the imaginary axis to the unit circle, thus kind-of preserving
the frequency response. If the system is not stable, then the frequency response
doesn’t make sense. Formally, the reason for this is that the inverse Laplace
22This idea has occurred to the author during the writing of the first revision of this book.
The author didn’t try it in practice yet, neither is he aware of other attempts.
Sufficient theoretical analysis is not possible here due to the fact that practical applications
of instantaneously unstable (or any unstable, for that matter) filters occur typically for non-
linear filters, and there are not many theoretical analysis means for the latter. Hopefully there
are no mistakes in the theoretical transformations, but even if there are mistakes, at least the
idea itself could maybe work.
3.13.
INSTANTANEOUSLY UNSTABLE FEEDBACK
89
transform of transfer functions only converges for σ > max {Re pn} where pn
are the poles of the transfer function, and respectively, if max {Re pn} ≥ 0, it
doesn’t converge on the imaginary axis (σ = 0). However, instead of the imag-
inary axis Re s = σ = 0, let’s choose some other axis Re s = σ > max {Re pn}
and use it instead of the imaginary axis to compute the “frequency response”.
We also need to ﬁnd a discrete-time counterpart for Re s = σ. Considering
that Re s deﬁnes the magnitude growth speed of the exponentials est we could
choose a z-plane circle, on which the magnitude growth speed of zn is the same
as for eσt. Apparently, this circle is |z| = eσT . So, we need to map Re s = σ
to |z| = eσT . Considering the bilinear transform equation (3.4), we divide z by
eσT to make sure ze−σT has a unit magnitude and shift the s-plane result by σ:
s = σ +
2
T
·
ze−σT − 1
ze−σT + 1
(3.38)
We can refer to (3.38) as the unstable bilinear transform, where the word “un-
stable” refers not to the instability of the transform itself, but rather to the
fact that it is designed to be applied to unstable ﬁlters.23 Notice that at σ = 0
the unstable bilinear transform turns into an ordinary bilinear transform. The
inverse transform is obtained by
from where
and
(s − σ)T
2
(ze−σT + 1) = ze−σT − 1
ze−σT
(cid:18)
1 −
(cid:19)
(s − σ)T
2
= 1 +
(s − σ)T
2
z = eσT 1 + (s−σ)T
1 − (s−σ)T
2
2
(3.39)
Apparently the inverse unstable bilinear transform (3.39) has a pole at s = σ+ 2
T .
In order to avoid hitting that pole by the poles of the ﬁlter’s transfer function
(or maybe even generally avoid the real parts of the poles to go past that value)
we could e.g. simply let
σ = max {0, Re pn}
or we could position σ midways:
(cid:26)
σ = max
0, Re pn −
(cid:27)
1
T
In order to construct an integrator deﬁned by (3.38) we ﬁrst need to obtain
the expression for 1/s from (3.38):
1
s
=
1
σ + 2
T · ze−σT −1
ze−σT +1
= T
ze−σT + 1
σT (ze−σT + 1) + 2(ze−σT − 1)
=
= T
ze−σT + 1
(σT + 2)e−σT z + (σT − 2)
= T
1 + eσT z−1
(σT + 2) − (2 − σT )eσT z−1 =
23Apparently, the unstable bilinear transform defines the same relationship between Im s
and arg z as the ordinary bilinear transform. Therefore prewarping can be done in the same
way as for the ordinary bilinear transform.
90
CHAPTER 3. TIME-DISCRETIZATION
=
T
2 + σT
·
1 + eσT z−1
1 − 2−σT
2+σT eσT z−1
That is
1
s
=
T
2 + σT
·
1 + eσT z−1
1 − 2−σT
2+σT eσT z−1
(3.40)
A discrete-time structure implementing (3.40) could be e.g. the one in Fig. 3.39.
Yet another approach could be to convert the right-hand side of (3.40) to the
analog domain by the inverse bilinear transform, construct an analog implemen-
tation of the resulting transfer function and apply the trapezoidal integrator
replacement to convert back to the digital domain. It is questionable, whether
this produces better (or even diﬀerent) results than Fig. 3.39.
T
2+σT
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
x[n]
•(cid:47)
y[n]
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
2−σT
2+σT
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
(cid:13)(cid:13)(cid:13)(cid:49)(cid:49)(cid:49)(cid:79)
+
eσT
Figure 3.39: Transposed direct form II-style “unstable” trapezoidal
integrator.
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
3.14 Other replacement techniques
The trapezoidal integrator replacement technique can be seen as a particular
case of a more general set of replacement techniques. Suppose we have two
ﬁlters, whose frequency response functions are F1(ω) and F2(ω) respectively.
The ﬁlters do not need to have the same nature, particularly one can be an
analog ﬁlter while the other can be a digital one. Suppose further, there is a
frequency axis mapping function ω(cid:48) = µ(ω) such that
F2(ω) = F1(µ(ω))
Typically µ(ω) should map the entire domain of F2(ω) onto the entire domain
of F1(ω) (however the exceptions are possible).
To make the subsequent discussion more intuitive, we will assume that µ(ω)
is monotone, although this is absolutely not a must.24 In this case we could say
24Strictly speaking, we don’t even care whether µ(ω) is single-valued. We could have instead
required that
for some µ1(ω) and µ2(ω).
F2(µ2(ω)) = F1(µ1(ω))
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
3.14. OTHER REPLACEMENT TECHNIQUES
91
that F2(ω) is obtained from F1(ω) by a frequency axis warping. Particularly,
this is exactly what happens in the bilinear transform case (the mapping µ(ω)
is then deﬁned by the equation (3.6)). One cool thing about the frequency axis
warping is that it preserves the relationship between the amplitude and phase.
Suppose that we have a structure built around ﬁlters of frequency response
F1(ω), and the rest of the structure doesn’t contain any memory elements (such
as integrators or unit delays). Then the frequency response F (ω) of this struc-
ture will be a function of F1(ω):
F (ω) = Φ(F1(ω))
where the speciﬁcs of the function Φ(w) will be deﬁned by the details of the
container structure. E.g. if the building-block ﬁlters are analog integrators, then
F1(ω) = 1/jω. For the ﬁlter in Fig. 2.2 we then have
Φ(w) =
w
w + 1
Indeed, substituting F1(ω) into Φ(w) we obtain
F (ω) = Φ(F1(ω)) = Φ(1/jω) =
1/jω
1 + 1/jω
=
1
1 + jω
which is the already familiar to us frequency response of the analog lowpass
ﬁlter.
Now, we can view the trapezoidal integrator replacement as a substitution
of F2 instead of F1, where µ(ω) is obtained from (3.6):
ωa = µ(ωd) =
2
T
tan
ωdT
2
The frequency response of the resulting ﬁlter is obviously equal to Φ(F2(ω)),
where F2(ω) is the frequency response of the trapezoidal integrators (used in
place of analog ones). But since F2(ω) = F1(µ(ω)).
Φ(F2(ω)) = Φ(F1(µ(ω)))
which means that the frequency response Φ(F2(·)) of the structure with trape-
zoidal integrators is obtained from the frequency response Φ(F1(·)) of the struc-
ture with analog integrators simply by warping the frequency axis. If the warp-
ing is not too strong, the frequency responses will be very close to each other.
This is exactly what is happening in the trapezoidal integrator replacement and
generally in the bilinear transform.
Differentiator-based filters
We could have used some other two ﬁlters, with their respective frequency re-
sponses F1 and F2. E.g. we could consider continuous-time systems built around
diﬀerentiators rather than integrators.25 The transfer function of a diﬀerentia-
tor is apparently simply H(s) = s, so we could use (3.4) to build a discrete-time
25The real-world analog electronic circuits are “built around” integrators rather than dif-
ferentiators. However, formally one still can “invert” the causality direction in the equations
and pretend that ˙x(t) is defined by x(t), and not vice versa.
92
CHAPTER 3. TIME-DISCRETIZATION
“trapezoidal diﬀerentiator”. Particularly, if we use the direct form II approach,
it could look similarly to the integrator in Fig. 3.9. When embedding the cutoﬀ
control into a diﬀerentiator (in the form of a 1/ωc gain), it’s probably better
to position it after the diﬀerentiator, to avoid the unnecessary “de-smoothing”
of the control modulation by the diﬀerentiator. Replacing the analog diﬀeren-
tiators in a structure by such digital trapezoidal diﬀerentiators we eﬀectively
perform a diﬀerentiator-based TPT.
E.g. if we replace the integrator in the highpass ﬁlter in Fig. 2.9 by a dif-
ferentiator, we essentially perform a 1/s ← s substitution, thus we should have
obtained a (diﬀerentiator-based) lowpass ﬁlter. Remarkably, if we perform a
diﬀerentiator-based TPT on such ﬁlter, the obtained digital structure is fully
equivalent to the previously obtained integrator-based TPT 1-pole lowpass ﬁl-
ter.
Allpass substitution
One particularly interesting case occurs when F1 and F2 deﬁne two diﬀerent
allpass frequency responses. That is |F1(ω)| ≡ 1 and |F2(ω)| ≡ 1. In this case
the mapping µ(ω) is always possible. Especially since the allpass responses (de-
ﬁned by rational transfer functions of analog and digital systems) always cover
the entire phase range from −π to π.26 In intuitive terms it means: for a ﬁlter
built of identical allpass elements, we can always replace those allpass elements
with an arbitrary other type of allpass elements (provided all other elements are
memoryless, that is there are only gains and summators). We will refer to this
process as allpass substitution. Whereas in the trapezoidal integrator replace-
ment we have replaced analog integrators by digital trapezoidal integrators, in
the allpass substitution we replace allpass ﬁlters of one type by allpass ﬁlters of
another type.
We can even replace digital allpass ﬁlters with analog ones and vice versa.
E.g., noticing that z−1 elements are allpass ﬁlters, we could replace them with
analog allpass ﬁlters. One particularly interesting case arises out of the inverse
bilinear transform (3.9). From (3.9) we obtain
z−1 =
1 − sT
2
1 + sT
2
(3.41)
The right-hand side of (3.41) obviously deﬁnes a stable 1-pole allpass ﬁlter,
whose cutoﬀ is 2/T . We could take a digital ﬁlter and replace all z−1 elements
with an analog allpass ﬁlter structure implementing (3.41). By doing this we
would have performed a topology-preserving inverse bilinear transform.
We could then apply the cutoﬀ parametrization to these underlying analog
allpass elements:
so that we obtain
sT
2
←
s
ωc
z−1 =
1 − s/ωc
1 + s/ωc
26Actually, for −∞ < ω < +∞, they cover this range exactly N times, where N is the order
of the filter.
SUMMARY
93
The expression s/ωc can be also rewritten as sT /2α, where α is the cutoﬀ scaling
factor:
z−1 =
1 − sT /2α
1 + sT /2α
(3.42)
Finally, we can apply the trapezoidal integrator replacement to the cutoﬀ-scaled
analog ﬁlter, converting it back to the digital domain. By doing so, we have
applied the cutoﬀ scaling in the digital domain! On the transfer function level
this is equivalent to applying the bilinear transform to (3.42), resulting in
z−1 =
1 − sT /2α
1 + sT /2α
←
1 − z−1
α(z+1)
1 + z−1
α(z+1)
=
=
α(z + 1) − (z − 1)
α(z + 1) + (z − 1)
=
(α − 1)z + (α + 1)
(α + 1)z + (α − 1)
That is, we have obtained a discrete-time allpass substitution
z−1 ←
(α − 1)z + (α + 1)
(α + 1)z + (α − 1)
which applies cutoﬀ scaling in the digital domain.27 The allpass ﬁlter
H(z) =
(α − 1)z + (α + 1)
(α + 1)z + (α − 1)
should have been obtained, as described, by the trapezoidal integrator replace-
ment in an analog implementation of (3.42), alternatively we could use a direct
form implementation. Notice that this ﬁlter has a pole at z = (α − 1)/(α + 1).
Since |α − 1| < |α + 1| ∀α > 0, the pole is always located inside the unit circle,
and the ﬁlter is always stable.
SUMMARY
We have considered three essentially diﬀerent approaches to applying time-
discretization to analog ﬁlter models: naive, TPT (by trapezoidal integrator
replacement), and the classical bilinear transform (using direct forms). The
TPT approach combines the best features of the naive implementation and the
classical bilinear transform.
27Differently from the analog domain, the digital cutoff scaling doesn’t exactly shift the
response along the frequency axis in a logarithmic scale, as some frequency axis warping is
involved. The resulting frequency response change however is pretty well approximated as
shiting in the lower frequency range.
94
CHAPTER 3. TIME-DISCRETIZATION
Chapter 4
State variable filter
After having discussed 1-pole ﬁlters, we are going to instroduce a 2-pole ﬁlter.
With 2-pole ﬁlters there is more freedom in choosing the ﬁlter topology than
with 1-poles, where any implementation of the latter would essentially be based
on a feedback loop around an integrator. A 2-pole topology of fundamental
importance and high usability is a classical analog model, commonly referred to
as state-variable filter (SVF). It can also serve as a basis for building arbitrary
2-pole ﬁlters by means of modal mixture.
4.1 Analog model
The block diagram of the state-variable ﬁlter is shown in Fig. 4.1. The three
outputs are the highpass, bandpass and lowpass signals. As usual, one can apply
transposition to obtain a ﬁlter with highpass, bandpass and lowpass inputs
(Fig. 4.2).
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
yHP(t)
yBP(t)
•(cid:47)
(cid:82)(cid:47)
•(cid:47)
(cid:82)(cid:47)
•(cid:47)
yLP(t)
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
2R
+
Figure 4.1: 2-pole multimode state-variable ﬁlter.
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
The diﬀerential equations implied by Fig. 4.1 are
yHP = x − 2RyBP − yLP
˙yBP = ωc · yHP
˙yLP = ωc · yBP
95
(4.1)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
96
CHAPTER 4. STATE VARIABLE FILTER
y(t)
•
xHP(t)
xBP(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
−1
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:82)
+
(cid:82)
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
2R
•
xLP(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 4.2: Transposed 2-pole multimode state-variable ﬁlter.
Rewriting them in terms of the lowpass signal y = yLP and combining them
together we obtain
2
¨y
ωc
+ 2R
˙y
ωc
+ y = x
or
¨y + 2Rωc ˙y + ω2
c y = ω2
c x
(4.2)
(4.3)
In a similar fashion one can easily obtain the transfer functions for the output
signals in Fig. 4.1. Assuming unit cutoﬀ and complex exponential signals, we
have
yHP = x − 2RyBP − yLP
yBP =
yLP =
1
s
1
s
yHP
yBP
yHP = x − 2R ·
1
s
yHP −
1
s2 yHP
(cid:18)
1 +
(cid:19)
2R
s
+
1
s2
yHP = x
HHP(s) =
yHP
x
=
1
2R
s
+
1
s2
1 +
=
s2
s2 + 2Rs + 1
from where
from where
and
Thus
HHP(s) =
HBP(s) =
HLP(s) =
s2
s2 + 2Rs + 1
s
s2 + 2Rs + 1
1
s2 + 2Rs + 1
=
=
=
s2
s2 + 2Rωcs + ω2
c
ωcs
s2 + 2Rωcs + ω2
c
ω2
c
s2 + 2Rωcs + ω2
c
(ωc = 1)
(ωc = 1)
(ωc = 1)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
4.1. ANALOG MODEL
97
Notice that yLP(t)+2RyBP(t)+yHP(t) = x(t), that is, the input signal is split
into lowpass, bandpass and highpass components. The same can be expressed
in the transfer function form:
HLP(s) + 2RHBP(s) + HHP(s) = 1
(4.4)
Amplitude responses
The amplitude responses of the state-variable ﬁlter are plotted in Figs. 4.3, 4.4
and 4.5. The pass-, stop- and transition bands of the low- and high-pass ﬁlters
are deﬁned in the same manner as for the 1-poles, where the transition band now
can contain a peak in the amplitude response. For the bandpass the passband is
located in the middle (around the cutoﬀ), and there is a stop- and a transition
band on each side of the cutoﬀ. The slope rolloﬀ speed is obviously −12dB/oct
for the low- and high-pass, and −6dB/oct for the bandpass.
|H(jω)|, dB
+12
+6
0
-6
-12
-18
R = 0.1
R = 1
ωc/8
ωc
8ωc
ω
Figure 4.3: Amplitude response of a 2-pole lowpass ﬁlter.
One could observe that the highpass response is a mirrored version of the
lowpass response, while the bandpass response is symmetric by itself. The sym-
metry between the lowpass and the highpass amplitude responses has a clear
algebraic explanation: applying the LP to HP substitution to a 2-pole lowpass
produces a 2-pole highpass and vice versa. The symmetry of the bandpass am-
plitude response has the same explanation: applying the LP to HP substitution
to the 2-pole bandpass converts it into itself.
Since
|s2 + 2Rs + 1|
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)s=j
= | − 1 + 2Rj + 1| = 2R
the amplitude response at the cutoﬀ is 1/2R for all three ﬁlter types. Except
for the bandpass, the cutoﬀ point ω = 1 is not exactly the peak location but it’s
CHAPTER 4. STATE VARIABLE FILTER
R = 0.1
R = 1
98
|H(jω)|, dB
+12
+6
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 4.4: Amplitude response of a 2-pole highpass ﬁlter.
|H(jω)|, dB
+12
+6
0
-6
-12
-18
d
n
a
b
p
o
t
S
n
o
i
t
i
s
n
a
r
T
d
n
a
b
R = 0.1
n
o
i
t
i
s
n
a
r
T
d
n
a
b
R = 1
d
n
a
b
p
o
t
S
Passband
ωc/8
ωc
8ωc
ω
Figure 4.5: Amplitude response of a 2-pole bandpass ﬁlter.
pretty close (the smaller the value of R, the closer is the true peak to ω = 1).
4.1. ANALOG MODEL
Phase responses
The phase response of the lowpass is
99
arg HLP(jω) = arg
1
1 + 2Rjω − ω2 = − arg(1 + 2Rjω − ω2) =
= − arctan
2Rω
1 − ω2 = − arccot
1 − ω2
2Rω
= − arccot
ω−1 − ω
2R
(4.5)
where we had to switch from arctan to arccot, since the principal value of arctan
gives wrong results for ω > 1. Fig. 4.6 illustrates.
arg H(jω)
0
−π/2
−π
R = 0.2
R = 1
R = 5
ωc/8
ωc
8ωc
ω
Figure 4.6: Phase response of a 2-pole lowpass ﬁlter. Bandpass
and highpass responses are the same, except that they are shifted
by +90◦ and 180◦ respectively.
We could notice the 2-pole phase response has the same kind of symmetry
around the cutoﬀ point in the logarithmic frequency scale as the 1-pole ﬁlters.
This property can be explained from (4.5) by noticing that the substitution
ω ← 1/ω changes the sign of the argument of arccot and by using the property
of arccot
arccot x + arccot(−x) = π
We also could notice that the steepness of the phase response is aﬀected by the
parameter R. Explicitly writing the phase response in a logarithmic frequency
scale we have
arg HLP(jex) = − arccot
e−jx − ejx
2R
= − arccot
− sinh x
R
(4.6)
thus R simply scales the argument of arccot which results in stretching or shrink-
ing of the phase response.
The bandpass phase response is a +90◦-shifted lowpass response:
arg HBP(jω) = arg
jω
1 + 2Rjω − ω2 =
π
2
+ arg HLP(s)
100
CHAPTER 4. STATE VARIABLE FILTER
The bandpass phase response is a 180◦-shifted lowpass response:
arg HHP(jω) = arg
(jω)2
1 + 2Rjω − ω2 = π + arg HLP(s)
The phase response at the cutoﬀ is −90◦ for the lowpass:
arg HLP(j) = arg
1
1 + 2Rj − 1
= arg
1
2Rj
= −
π
2
respectively giving 0◦ for the bandpass and +90◦ for the highpass.
It can be also observed in Fig. 4.6 that the lowpass phase response is close
to zero in the passband, the same as for the 1-pole lowpass. As we shuld
have expected, the same also holds for the highpass’s passband. Somewhat
remarkably, as we just established by evaluating the bandpass phase response
at the cutoﬀ, the same property also holds for the bandpass’s passpand, although
at small values of R the phase will be close to zero only in a small neighborhood
of the cutoﬀ.
4.2 Resonance
With a 1-pole lowpass or highpass ﬁlter, the only parameter to control was the
ﬁlter cutoﬀ, shifting the amplitude response to the left or to the right in the
logarithmic frequency scale. With 2-pole ﬁlters there is an additional parameter
R, which, as the reader could have noticed from Figs. 4.3, 4.4 and 4.5 controls
the height of the amplitude response peak occuring closely to ω = ωc. A narrow
peak in the amplitude response is usually referred to as resonance. Thus, we
can say that the R parameter controls the amount of resonance in the ﬁlter.
On the other hand, from the same ﬁgures we can notice that the resonance
increases (the peak becomes higher and more narrow) as R decreases. It is easy
to verify that at R = 0 the resonance peak becomes inﬁnitely high. A little bit
later we will also establish the fact that the state variable ﬁlter is stable if and
only if R > 0. Thus, the parameter R actually has the function of decreasing
or damping the resonance. For that reason we refer to the R parameter as
the damping.1 By controlling the damping parameter we eﬀectively control the
ﬁlter’s resonance.2
Damping and selfoscillation
At R = 0 and x(t) ≡ 0 the equation (4.3) turns into
¨y = −ω2
c y
1A more correct term, used in theory of harmonic oscillations, is damping ratio, where the
commonly used notation for the same parameter is ζ.
2The “resonance” control for the SVF filter can be introduced in a number of different
ways. One common approach is to use the parameter Q = 1/2R, however this doesn’t allow
to go easily into the selfoscillation range in the nonlinear versions of this filter, also the
math is generally more elegant in terms of R than in terms of Q. Another option is using
r = 1−R, which differs from the resonance control parameter k of SKF/TSK filters (discussed
in Section 5.8) just by a factor of 2, the selfoscillation occuring at r = 1. Other, more
sophisticated mappings, can be used for a “more natural feel” of the resonance control.
4.2. RESONANCE
101
which is eﬀectively a spring-mass equation
or
m¨y = −ky
¨y = −
k
m
y
where respectively ωc = (cid:112)k/m. Starting from a non-zero initial state such
system will oscillate around the origin inﬁnitely long. Thus, in the absence of
the damping signal path (Fig. 4.7), the ﬁlter will be constantly selfoscillating.3
Notably, the selfoscillation is appearing at the setting R = 0 where the resonance
peak is getting inﬁnitely high. This is a general property of resonating ﬁlters
and has to do with the relationship between the ﬁlter poles and the ﬁlter’s
transient response, both covered later in this chapter and additionally and in a
more general form in Chapter 7.
yHP(t)
yBP(t)
•(cid:47)
(cid:82)(cid:47)
•(cid:47)
(cid:82)(cid:47)
•(cid:47)
yLP(t)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 4.7: 2-pole multimode state-variable ﬁlter without the
damping path (selfoscillating).
The introduction of the damping signal
¨y = −ω2
c y − 2R ˙y
reduces the amount of resonance in the ﬁlter, which in terms of a spring-mass
system works as a 1st-order energy dissipation term:
m¨y = −ky − 2c ˙y
This should give a better idea of why the R parameter is referred to as damping.
By further adding an external force to the spring-mass system one eﬀectively
adds the input signal.4
3The selfoscillating state at R = 0 is a marginally stable state. As mentioned earlier, due
to the noise present in the system (such as numerical errors in a digital implementation),
In
we shouldn’t expect to be able to exactly hold a system in a marginally stable state.
order to have reliable selfoscillation one usually needs to introduce nonlinear elements into
the system. E.g. by introducing the saturating behavior one would be able to lower R below
0, thereby increasing the resonance even further, without making the filter explode. So,
while selfoscillation formally appears at R = 0, it is becoming reliable at R < 0, given that
nonlinearities prevent the filter from exploding.
4Thereby the differential equation becomes formally equivalent to an SVF, but there still
is an essential difference. The state of a spring-mass system consists of a position y(t) and a
velocity ˙y(t). Changes to the system parameters will therefore directly change the kinetic and
potential energies, which can result in a sudden increase or reduction of the amplitude of the
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
102
CHAPTER 4. STATE VARIABLE FILTER
Resonance peak
We can ﬁnd the exact position and the height of the resonance peak by looking
for the local maximum of the (squared) amplitude response. E.g. for the lowpass
amplitude response:
|HLP(jω)|2 =
1
|(jω)2 + 2Rjω + 1|2 =
1
(1 − ω2)2 + 4R2ω2
Instead of looking for the maximum of |HLP(jω)|2 we can look for the minumum
of the reciprocal function:
|HLP(jω)|−2 = ω4 + 2(2R2 − 1)ω2 + 1
Clearly, |HLP(jω)|−2 is a quadratic polynomial in ω2 with the minimum at
ω2 = 1 − 2R2. The resonance peak position is thus
ωpeak =
(cid:112)
1 − 2R2
√
2 (we are considering only positive values of R) there is no
where for R ≥ 1/
minimum at ω2 > 0 and respectively no resonance peak. Note that the peak
thereby starts at ω = 0 at R = 1/
2 and, as R decreases to zero, moves towards
ω = 1.
√
The resonance peak height is simply the value of the amplitude response
evaluated at ωpeak:
|HLP(jωpeak)|2 =
1
(1 − (1 − 2R2))2 + 4R2(1 − 2R2)
=
1
4R4 + 4R2 − 8R4 =
=
1
4R2 − 4R4 =
1
4R2(1 − R2)
(R < 1/
√
2)
and
|HLP(jωpeak)| =
√
√
1
1 − R2
2R
√
(R < 1/
2)
Thus at R = 1/
2 the peak height is formally |HLP(jωpeak)| = 1, correspond-
ing to the amplitude response not having the peak yet. At R → 0 we have
|HLP(jωpeak)| ∼ 1/2R. The above expression also allows us to ﬁnd the value of
R given a desired peak height A. Starting from
A =
√
1
1 − R2
2R
(4.7)
we have
2R
(cid:112)
1 − R2 = A−1
A−2
4
R2(1 − R2) =
R4 − R2 +
A−2
4
= 0
swinging. In comparison, in the SVF the system state consists of the “lowpass” integrator’s
state y(t) and “bandpass” integrator’s state, which according to (4.1) is ˙y(t)/ωc.
In this
case changes to the filter parameters will affect the filter’s output in a more gradual way.
Particulary, according to (2.28), changes to the cutoff will not affect the output amplitude at
all.
4.3. POLES
103
√
1 ±
R2 =
1 − A−2
2
√
Taking into account the allowed range of R (which is 0 < R < 1/
2), we obtain
(cid:115)
√
1 −
R =
1 − A−2
2
(A ≥ 1)
(4.8)
Recalling that the amplitude response of the 2-pole highpass is simply a
symmetrically ﬂipped amplitude response of the 2-pole lowpass, we realize that
the same considerations apply to the 2-pole highpass, except that the expres-
sion for ωpeak needs to be reciprocated. For the bandpass ﬁlter the amplitude
response peak is always exactly at the cutoﬀ.
Butterworth filter
√
2 at which the resonance peak starts to appear has
The threshold value R = 1/
another interesting property. At this setting the (logarithmic frequency scale)
amplitude responses of the 2-pole lowpass and highpass are shrunk horizontally
two times around the cutoﬀ point, as compared to those of 1-poles (the phase
response is transformed in a more complicated way, which is of little interest to
us here). This is a particular case of a Butterworth filter. Butterworth ﬁlters
will be discussed in a generalized form in Chapter 8, but we can also show this
shrinking property explicitly here. Indeed, for R = 1/
2 we have
√
√
(cid:12)
(cid:12)s2 +
(cid:12)
2 · s + 1
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)s=jω
=
(cid:12)
(cid:12)1 − ω2 + j
(cid:12)
√
2 · ω
2
(cid:12)
(cid:12)
(cid:12)
= (1 − ω2)2 + 2ω2 =
= 1 + ω4 = (cid:12)
(cid:12)1 + jω2(cid:12)
(cid:12)
2
= |1 + s|2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)s=jω2
Now, the substitution ω ← ω2 corresponds to the two times shrinking in the
log ω ← 2 log ω. Thus, for the lowpass 2-pole we
logarithmic frequency scale:
have
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
√
2 · s + 1
s2 +
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)s=jω
=
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
1 + s
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)s=jω2
and for the highpass ﬁlter we have
√
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
s2 +
s2
2 · s + 1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)s=jω
The readers can refer to Fig. 8.13 for the illustration of the shrinking eﬀect.
Since for R < 1/
2 the amplitude response obtains a resonance peak, the
Butterworth 2-pole ﬁlter is the one with the “sharpest” possible cutoﬀ among
all non-resonating 2-poles.
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)s=jω2
s
1 + s
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
√
=
4.3 Poles
Solving s2 + 2Rs + 1 = 0 we obtain the poles of the ﬁlter at
p1,2 = −R ±
(cid:112)
R2 − 1 =
(cid:40)
−R ±
−R ± j
√
R2 − 1
√
1 − R2
if |R| ≥ 1
if −1 ≤ R ≤ 1
104
CHAPTER 4. STATE VARIABLE FILTER
Thus, the poles are located in the left semiplane if and only if R > 0. As with
1-poles, the location of the poles in the left semiplane is suﬃcient and necessary
for the ﬁlter to be stable.5
For |R| ≤ 1 the poles are located on the unit circle
(Re p)2 + (Im p)2 = (−R)2 + (
(cid:112)
1 − R2)2 = 1
This also implies that R is equal to the cosine of the angle between the negative
real axis and the direction to the pole (Fig. 4.8).
Im s
j
α
−α
−1
0
1
Re s
−j
cos α = R
Figure 4.8: Poles of a resonating 2-pole ﬁlter (ωc = 1).
As R is getting close to zero, the poles are getting close to the imaginary
axis. By deﬁnition of a pole, the transfer function is inﬁnitely large at the
poles, which means it is also having large values on the imaginary axis close to
the poles. This corresponds to the resonance peak appearing in the amplitude
response. At R = 0 the poles are located right on the imaginary axis and the
ﬁlter selfoscillates.
At |R| ≥ 1 the poles are real and mutually reciprocal:6
(cid:112)
(−R −
R2 − 1) · (−R +
(cid:112)
R2 − 1) = 1
(Fig. 4.9). The ﬁlter thus “falls apart” into a serial combination of two 1-pole
ﬁlters:
HLP(s) =
1
s2 + 2Rs + 1
=
1
s − p1
·
1
s − p2
5Later we will discuss the transient response of the SVF and the respective effects of the
poles position on the stability.
6Actually, the poles are mutually reciprocal at any R (since their product should be equal
to the constant term of the denominator). For complex poles the reciprocal property manifests
itself as conjugate symmetry of the poles, since the poles are lying on the unit circle and the
reciprocation does not change their absolute magnitude.
4.3. POLES
105
HBP(s) =
HHP(s) =
s
s2 + 2Rs + 1
s2
s2 + 2Rs + 1
=
=
s
s − p1
s
s − p1
·
·
1
s − p2
s
s − p2
where p1p2 = 1.7 These 1-pole ﬁlters become visible in the amplitude responses
at suﬃciently large R as two diﬀerent “cutoﬀ points” (Fig. 4.10).
Im s
j
p1
p2
−1
0
1
Re s
−j
p1p2 = 1
Figure 4.9: Poles of a non-resonating 2-pole ﬁlter (ωc = 1).
|H(jω)|, dB
0
-12
-24
-36
-48
)
1
−
2
R
√
−
R
(
c
ω
=
ω
)
1
−
2
R
√
+
R
(
c
ω
=
ω
ωc/8
ωc
8ωc
ω
Figure 4.10: Amplitude response of a non-resonating 2-pole low-
pass ﬁlter.
7Of course the same decomposition is formally possible for complex poles, but a 1-pole
filter with a complex pole cannot be implemented as a real system.
106
CHAPTER 4. STATE VARIABLE FILTER
Resonance redefined
√
The pole positions can give us another way of deﬁning the point where we
consider the resonance to appear. Previously we have found that the resonance
2. However, the amplitude response peak is only one
peak appears at R < 1/
manifestation of the resonance eﬀect. Another aspect of resonance is that, as we
shall see later, the transient response of the ﬁlter contains sinusoidal oscillation,
which occurs whenever the poles are complex. Therefore, using the presence of
transient oscillations as the alternative deﬁnition of the resonance, we can say
that the resonance occurs when R < 1.
Similarly to R = 1/
2, the threshold setting R = 1 has a special property.
At this setting both poles are located at s = −1 and the transfer function of the
2-pole lowpass becomes equal to the transfer function of two serially connected
1-pole lowpasses:
√
1
s2 + 2s + 1
=
(cid:18) 1
(cid:19)2
s + 1
while the transfer function of the 2-pole highpass becomes equal to the transfer
function of two serially connected 1-pole highpasses:
s2
s2 + 2s + 1
=
(cid:18) s
(cid:19)2
s + 1
This means that at this value of R the (decibel-scale) amplitude responses of
the 2-pole lowpass and highpass are stretched vertically two times compared to
those of the 1-pole lowpass and highpass (Fig. 4.11), and the same holds for the
phase responses (Fig. 4.12).
|H(jω)|, dB
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 4.11: Amplitude response of the 2-pole lowpass ﬁlter at
R = 1 (solid line) compared to the amplitude response of the 1-
pole lowpass ﬁlter (dashed line).
Non-unit cutoff
If ωc (cid:54)= 1 then the transfer function denominator becomes s2 + 2Rωcs + ω2
c (or
(s/ωc)2 + 2Rs/ωc + 1, if no simpliﬁcations are performed on the entire transfer
4.3. POLES
arg H(jω)
0
−π/2
−π
107
ωc/8
ωc
8ωc
ω
Figure 4.12: Phase response of the 2-pole lowpass ﬁlter at R =
1 (solid line) compared to the amplitude response of the 1-pole
lowpass ﬁlter (dashed line).
function) and the formula for the poles becomes
√
R2 − 1)
√
1 − R2)
ωc · (−R ±
ωc · (−R ± j
p1,2 = ωc · (−R ±
R2 − 1) =
(cid:112)
(cid:40)
if |R| ≥ 1
if −1 ≤ R ≤ 1
(4.9)
The formula (4.9) can be obtained either by directly solving the quadratic equa-
tion or by noticing that the cutoﬀ substitution s ← s/ωc scales the poles ac-
cording to p ← pωc. Complex poles are therefore located on the circle of radius
ωc (Fig. 4.13), while real poles have a geometric mean equal to ωc (Fig. 4.14).
Im s
jωc
j
0
−j
−jωc
α
−α
−1
−ωc
cos α = R
1
ωc
Re s
Figure 4.13: Poles of a resonating 2-pole ﬁlter (ωc (cid:54)= 1).
108
CHAPTER 4. STATE VARIABLE FILTER
Im s
j
p1
p2
−ωc
−1
p1p2 = ω2
c
jωc
1
0
−j
−jωc
ωc
Re s
Figure 4.14: Poles of a non-resonating 2-pole ﬁlter (ωc (cid:54)= 1).
Transfer function in terms of poles
Writing the lowpass transfer funtion in terms of poles we have for ωc = 1
HLP(s) =
1
s2 + 2Rs + 1
=
1
s − p1
·
1
s − p2
=
1
s2 − (p1 + p2)s + 1
and for an arbitrary ωc respectively
HLP(s) =
ω2
c
s2 + 2Rωcs + ω2
c
=
p1p2
s2 − (p1 + p2)s + p1p2
Respectively
from where
−(p1 + p2) = 2Rωc
p1p2 = ω2
c
√
ωc =
R = −
p1p2
p1 + p2
2ωc
= −
(p1 + p2)/2
p1p2
√
(4.10a)
(4.10b)
(4.11a)
(4.11b)
In terms of ω1 = −p1 and ω2 = −p2 the same turns into
ω1 + ω2 = 2Rωc
ω1ω2 = ω2
c
and
ωc =
√
ω1ω2
(4.12a)
4.4. DIGITAL MODEL
R =
ω1 + ω2
2ωc
=
(ω1 + ω2)/2
ω1ω2
√
109
(4.12b)
Notice that thereby ωc is a geometric mean of the 1-pole cutoﬀs, and R is a
ratio of their arithmetic and geometric means. Equations (4.12) can be used to
represent a series of two 1-poles with given cutoﬀs by an SVF.8
Pole cutoff and damping
A pair of complex poles of an SVF must be a conjugate pair, therefore we have
|p1| = |p2|, Re p1 = Re p2 and Im p1 = − Im p2. The equations (4.10) in this
case turn into
−2 Re pn = 2Rωc
|pn|2 = ω2
c
(n = 1, 2)
These relationships motivate the introduction of the notion of the “associated
cutoﬀ and damping” of an arbitrary pair of conjugate poles p and p∗, where we
would have
−2 Re p = 2Rωc
|p|2 = ω2
c
(n = 1, 2)
and
ωc = |p|
R =
− Re p
|p|
(n = 1, 2)
(4.13)
(Fig. 4.13 can serve as an illustration).
This idea is particularly convenient, if we imply that a particular high-order
transfer function is to be implemented as a cascade of 2-poles (further discussed
in Section 8.2), in which case (4.13) gives us ready formulas for the computation
of the cutoﬀ and damping of the respective 2-pole. Also, unless the high-order
transfer function is having coinciding complex poles, the separation of complex
poles into pairs of conjugate poles is unambiguous.
The same can be done for real poles, if desired, where we can use (4.11)
instead of (4.13) but this would work only under the restriction that both poles
are having the same sign (Fig. 4.14 can serve as an illustration).9 Also the
grouping of such poles into pairs can be done in diﬀerent ways.
Sometimes the same terminology is also convenient for zeros. Even though
formally it is not correct, since zeros are not directly associated with a cutoﬀ or
damping, it is sometimes handy to treat a pair of zeros as roots of a polynomial
s2 + 2Rωcs + ω2
c .
4.4 Digital model
Skipping the naive implementation, which the readers should be perfectly capa-
ble of creating and analyzing themselves by now, we proceed with the discussion
of the TPT model.
8Apparently, (4.12) defines only the denominator of the SVF’s transfer function. The
numerator would need to be computed separately.
9Apparently (4.11) can be used all the time, regarless of whether the poles are complex
or real. It’s just that in case of complex poles we have simpler and more intuitive formulas
(4.13).
110
CHAPTER 4. STATE VARIABLE FILTER
Assuming gξ +sn instantaneous responses for the two trapezoidal integrators
one can redraw Fig. 4.1 to obtain the discrete-time model in Fig. 4.15.
x[n]
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
yHP[n]
yBP[n]
•(cid:47)
gξ + s1
•(cid:47)
gξ + s2
•(cid:47)
yLP[n]
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
2R
+
Figure 4.15: TPT 2-pole multimode state-variable ﬁlter in the in-
stantaneous response form.
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Picking yHP as the zero-delay feedback equation’s unknown10 we obtain from
Fig. 4.15:
from where
from where
yHP = x − 2R(gyHP + s1) − g(gyHP + s1) − s2
(cid:0)1 + 2Rg + g2(cid:1) yHP = x − 2Rs1 − gs1 − s2
yHP =
x − (2R + g)s1 − s2
1 + 2Rg + g2
(4.14)
Apparently (4.14) has the form (3.37), where the total instantaneous gain of
the zero-delay feedback loop in Fig. 4.15 is G = −(2Rg + g2) and thus the
instantaneously unstable case occurs when the denominator of (4.14) is negative.
However, as long as g > 0 and R > −1, the denominator of (4.14) is always
positive:
1 + 2Rg + g2 > 1 + 2 · (−1) · g + g2 = (1 − g)2 ≥ 0
thus under these conditions the ﬁlter in not becoming instantaneously unstable.
Using yHP we can proceed deﬁning the remaining signals in the structure,
in the same way as we did for the 1-pole in Section 3.9. Assuming that we are
using trasposed direct form II integrators (Fig. 3.11), sn are the states of the z−1
elements in the respective integrators and g = ωcT /2 (prewarped). Therefore
by precomputing the values 1/(1 + 2Rg + g2) and 2R + g in advance, the formula
(4.14) can be computed in 2 subtractions and 2 multiplications. What remains
is the processing of both integrators. A transposed direct form II integrator
can be computed in 1 multiplication and 2 additons. Thus, the entire SVF
processing routine needs 4 multiplications and 6 additions/subtractions:
// perform one sample tick of the SVF
HP := (x-g1*s1-s2)*d; // g1=2R+g, d=1/(1+2Rg+g^2)
10The state-variable filter has two feedback paths sharing a common path segment. In order
to obtain a single feedback equation rather than an equation system we should pick a signal
on this common path as the unknown variable.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
4.5. NORMALIZED BANDPASS FILTER
111
v1 := g*HP; BP := v1+s1; s1 := BP+v1; // first integrator
v2 := g*BP; LP := v2+s2; s2 := LP+v2; // second integrator
If we are not interested in the highpass signal, we could obtain a more optimal
implementation by solving for yBP instead:
yBP = g(x − 2RyBP − gyBP − s2) + s1
(1 + 2Rg + g2)yBP = g(x − s2) + s1
yBP =
g(x − s2) + s1
1 + 2Rg + g2
This gives us:
// perform one sample tick of the SVF BP/LP
BP := (g*(x-s2)+s1)*d; // d=1/(1+2Rg+g^2)
v1 := BP-s1; s1 := BP+v1; // first integrator
v2 := g*BP; LP := v2+s2; s2 := LP+v2; // second integrator
This implementation has 3 multiplications and 6 additions/subtractions.
If we need only the BP signal, then we could further transform the expres-
sions used to update the integrators:
// perform one sample tick of the SVF BP
BP := (g*(x-s2)+s1)*d; // d=1/(1+2Rg+g^2)
BP2 := BP+BP; s1 := BP2-s1; // first integrator
v22 := g*BP2; s2 := s2+v22; // second integrator
That’s 3 multiplications and 5 additions/subtractions.
4.5 Normalized bandpass filter
By multiplying the bandpass ﬁlter’s output by 2R:
HBP1(s) = 2RHBP(s) =
2Rs
s2 + 2Rs + 1
(4.15)
we obtain a bandpass ﬁlter which has a unit gain (and zero phase response) at
the cutoﬀ:
HBP1(j) =
2Rj
j2 + 2Rj + 1
= 1
For that reason this version of the 2-pole bandpass ﬁlter is referred to as a
unit-gain or normalized bandpass. Fig. 4.16 illustrates the amplitude response.
The normalized bandpass has a better deﬁned passband than the ordinary
bandpass, since here we can deﬁne the frequency range where |HBP1(jω)| ≈ 1 as
the passband. Notably, in Fig. 4.16 one observes that the width of the passband
grows with R. At the same time from Fig. 4.6 one can notice that the width
of the band where the bandpass phase response is close to zero also grows with
R. Thus, the phase response of the normalized bandpass ﬁlter is close to zero
in the entire passband of the ﬁlter, regardless of R.11
11This can be confirmed in a more rigorous manner by the fact (which we establish in
Section 4.6) that the frequency response of the 2-pole normalized bandpass filter can be
obtained from the frequency response of the 1-pole lowpass filter by a frequency axis mapping.
CHAPTER 4. STATE VARIABLE FILTER
R = 5
R = 0.1
R = 1
112
|H(jω)|, dB
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 4.16: Amplitude response of a 2-pole unit gain bandpass
ﬁlter.
Rewriting (4.4) in terms of the normalized bandpass we get
HLP(s) + HBP1(s) + HHP(s) = 1
x(t) = yLP(t) + yBP1(t) + yHP(t)
that is
Topology
Notice that the unit gain bandpass signal can be directly picked up at the output
of the 2R gain element as shown in Fig. 4.17.
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
yHP(t)
yBP(t)
•(cid:47)
(cid:82)(cid:47)
•(cid:47)
(cid:82)(cid:47)
•
yLP(t)
2R
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
•(cid:15)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
yBP1(t)
Figure 4.17: State-variable ﬁlter with a normalized bandpass out-
put.
If the damping parameter is to be modulated at high rate, rather than
multiplying the bandpass output by 2R, it might be better to multiply the
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
4.5. NORMALIZED BANDPASS FILTER
113
ﬁlter’s input by 2R:
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
2R
HBP(s)
The reasoning is pretty much the same as for positioning the cutoﬀ gains before
the integrators or for preferring the transposed (multi-input) ﬁlters for modal
mixing: we let the integrator smooth the jumps or quick changes in the signal.
This will be given for granted if we use the transposed version of Fig. 4.17.
Instead of using the transposed version, we could inject the input signal into
the Fig. 4.17 ﬁlter structure as shown in Fig. 4.18. However, by multplying
the input rather than the output by 2R we have not only changed the “BP”
output signal to normalized bandpass, we have also changed the amplitudes of
the LP and HP outputs. Notably, Fig. 4.18 is essentially the transposed version
of Fig. 4.17, except for the relative placement of the second integrator and an
invertor.
(cid:82)(cid:47)
x(t)
yBP1(t)
(cid:82)(cid:47)
•(cid:47)
−(cid:15)
+
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
2R
+
−
Figure 4.18: Normalized bandpass state-variable ﬁlter with pre-
ﬁlter 2R gain.
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Prewarping
The standard application of the bilinear transform prewarping technique implies
that we want the cutoﬀ point to be positioned exactly at ωc on the digital
frequency axis. However with the normalized bandpass ﬁlter the positioning
of the left and right transition band slopes is more important than the exact
positioning of the cutoﬀ. At the same time, the damping parameter doesn’t
seem to have much (or any) vertical eﬀect on the amplitude response, mainly
controlling the distance between the slopes. Thus we have two degrees of control
freedom (the cutoﬀ and the damping) which we could attempt to use to position
the two slopes as exactly as possible. Instead of developing the corresponding
math just for the normalized bandpass ﬁlter, though, we are going to do this in
a more general manner in Section 4.6.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
114
CHAPTER 4. STATE VARIABLE FILTER
4.6 LP to BP/BS substitutions
The 2-pole unit gain bandpass response can be obtained from the lowpass re-
sponse 1/(1 + s) by the so-called LP to BP (lowpass to bandpass) substitution:
s ←
1
R
·
s + s−1
2
(4.16)
We will also occasionally refer to the LP to BP substitution as the LP to BP
transformation, making no particular disctinction between both terms.
Since s and 1/s are used symmetrically within the right-hand side of (4.16),
it immediately follows that the result of the substitution is invariant relative
to the LP to HP substitution s ← 1/s. Therefore the result of the LP to BP
substitution has an amplitude response which is symmetric in the logarithmic
frequency scale.
Using s = jω, we obtain
or
jω ←
1
R
·
jω + 1/jω
2
ω ←
1
R
·
ω − ω−1
2
Denoting the new ω as ω(cid:48) we write
ω =
1
R
·
ω(cid:48) − ω(cid:48)−1
2
(4.17)
Instead of trying to understand the mapping of ω to ω(cid:48) it is easier to understand
the inverse mapping from ω(cid:48) to ω, as explicitly speciﬁed by (4.17). Furthermore,
it is more illustrative to express ω(cid:48) in the logarithmic scale:
ω =
1
R
·
eln ω(cid:48)
− e− ln ω(cid:48)
2
=
1
R
sinh ln ω(cid:48)
if ω > 0
ω = −
1
R
·
eln |ω(cid:48)| − e− ln |ω(cid:48)|
2
= −
1
R
sinh ln |ω(cid:48)|
if ω < 0
Thus
ω =
1
R
sinh (sgn ω(cid:48) · ln |ω(cid:48)|)
(4.18)
Since ln |ω(cid:48)| takes up the entire real range of values in each of the cases ω > 0
and ω < 0 and respectively, so does sinh(sgn ω(cid:48) · ln |ω(cid:48)|),
ω(cid:48) ∈ (0, +∞) ⇐⇒ ω ∈ (−∞, +∞)
ω(cid:48) ∈ (−∞, 0) ⇐⇒ ω ∈ (−∞, +∞)
This means that the entire range ω ∈ (−∞, +∞) is mapped once onto the
positive frequencies ω(cid:48) and once onto the negative frequencies ω(cid:48). Furthermore,
the mapping and its inverse are strictly increasing on each of the two segments
ω > 0 and ω < 0, since dω/dω(cid:48) > 0. The unit frequencies ω(cid:48) = ±1 are mapped
from ω = 0.
Since we are often dealing with unit-cutoﬀ transfer functions (ωc = 1), it’s
c the unit cutoﬀ is mapped. Recalling
interesting to see to which frequencies ω(cid:48)
4.6. LP TO BP/BS SUBSTITUTIONS
115
that the entire bipolar range of ω is mapped to the positive range of ω(cid:48), we need
to include the negative cutoﬀ point (ωc = −1) into our transformation. On the
other hand, we are interested only in positive ω(cid:48)
c, since the negative-frequency
range of the amplitude response is symmetric to the positive-frequency range
anyway. Under these reservations, from (4.18) we have:
1
R
sinh ln ω(cid:48)
c = ±1
from where ln ω(cid:48)
c = ± sinh−1 R, or, changing the logarithm base:
log2 ω(cid:48)
c = ±
sinh−1 R
ln 2
Note that the above immediately implies that the two points ω(cid:48)
mutually reciprocal positions.
c are located at
The distance in octaves between the two ω(cid:48)
c points can be deﬁned as the
bandwidth of the transformation:
∆ =
2
ln 2
sinh−1 R
(4.19)
Since the points ω(cid:48)
from ω = 1.
c are mutually reciprocal, they are located at ±∆/2 octaves
Inverting (4.19) we can obtain the damping, given the bandwidth ∆:
R = sinh
∆ · ln 2
2
=
2Δ/2 − 2−Δ/2
2
(4.20)
Frequency axis warping and parameter prewarping
An important consequence of the fact that the LP to BP substitution can be
seen as a mapping of the ω axis is that the only eﬀect of the variation of the R
parameter is the warping of the frequency axis. This means that (like in the bi-
linear transform) the amplitude and phase responses are warped identically and
the relationship between amplitude and phase responses is therefore preserved
across the entire range of ω.
If LP to BP substitution is involved, the resulting frequency response has
two points of interest which are the images ω(cid:48)
1 of the original point at ω1 = 1,
which often is the cutoﬀ point of the original frequency response.12 Given a
digital implementation of such LP to BP substitution’s result, we can prewarp
the R parameter of the substitution in such a way that the distance between the
ω(cid:48)
1 points in the digital frequency response is identical to the distance between
those in analog frequency response.
Indeed, given the original value of R, we can use (4.19) to compute the
distance ∆ between the ω(cid:48)
1 points. We know that the points are positioned
at ±∆/2 octaves from ω = 1, or, if the substitution result has its own cutoﬀ
parameter, from ωc. That is
1 = ωc · 2±Δ/2
ω(cid:48)
12We are using ω1 and ω(cid:48)
1 instead of previously used ω(cid:48)
c and ωc notation for the respective
point, since we are going to need ωc to denote the substitution result’s cutoff.
116
CHAPTER 4. STATE VARIABLE FILTER
So, these are the frequencies at which the unit frequency’s image points would
be normally located on an analog ﬁlter’s response and where we want them to
be located on the digital ﬁlter’s response. If ω(cid:48)
1 are the points on the digital
frequency response, then by (3.10) the corresponding analog points should be
located at
1 = µ(ω(cid:48)
˜ω(cid:48)
1) = µ
(cid:16)
ωc · 2±Δ/2(cid:17)
At unit cutoﬀ ˜ωc the points ˜ω(cid:48)
1 would have been mutually reciprocal.
cutoﬀ is not unity, then it must be equal to the geometric mean of ˜ω(cid:48)
1:
If the
(cid:113)
µ (cid:0)ωc · 2Δ/2(cid:1) · µ (cid:0)ωc · 2−Δ/2(cid:1)
˜ωc =
while the bandwidth is simply the logarithm of the ratio of ˜ω(cid:48)
1:
˜∆ = log2
µ (cid:0)ωc · 2Δ/2(cid:1)
µ (cid:0)ωc · 2−Δ/2(cid:1)
Given ˜∆, we obtain ˜R from (4.20).
So, we have obtained the prewarped parameters ˜ωc and ˜R, which can be used
to control a bilinear transform-based digital implementation of an LP to BP
substitution’s result, thereby ensuring the correct positioning of the ω(cid:48)
1 points.
Particularly, treating the normalized bandpass ﬁlter as the result of LP to BP
substitution’s application to a 1-pole lowpass 1/(1 + s), we could prewarp the
bandpass ﬁlter’s parameters to have exact positioning of the −3dB points on
the left and right slopes (since these are the images of the 1-pole lowpass’s unit
cutoﬀ point).
In principle, any other two points could have been chosen as prewarping
points, where the math is much easier if these two points are located symmet-
rically relatively to the cutoﬀ in the logarithm frequency scale. We will not go
into further detail of this, as the basic ideas of deriving the respective equations
are exactly the same.
Poles and stability
The transformation of the poles and zeros by the LP to BP transformation can
be obtained from
s =
1
R
·
s(cid:48) + s(cid:48)−1
2
(4.21)
resulting in
s(cid:48) = Rs ±
(cid:112)
R2s2 − 1
Regarding the stability preservation consider that the sum (s(cid:48) + 1/s(cid:48)) in
(4.21) is located in the same complex semiplane (left or right) as s(cid:48). Therefore,
as long as R > 0, the original value s is located in the same semiplane as its
images s(cid:48). which implies that the stability is preserved. On the other hand,
negative values of R “ﬂip” the stability.
Topological LP to BP substitution
As for performing the LP to BP substitution in a block diagram, diﬀerently from
the LP to HP substitution, here we don’t need diﬀerentiators. The substitution
4.7. FURTHER FILTER TYPES
117
can be performed by replacing all (unit-cutoﬀ) integrators in the system with
the structure in Fig. 4.19, thereby substituting 2Rs/(s2 + 1) for 1/s, which is
algebraically equivalent to (4.16).13
2R
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82)(cid:47)
•(cid:47)
(cid:82) (cid:111)
Figure 4.19: “LP to BP” integrator.
LP to BS substitution
The LP to BS (lowpass to bandstop) substitution 14 is obtained as a series of
LP to HP substitution followed by an LP to BP substitution. Indeed, applying
the LP to BP substitution to a 1-pole highpass, we obtain the 2-pole notch
(“bandstop”) ﬁlter. Therefore, applying a series of LP to HP and LP to BP
substitutions to a 1-pole lowpass we also obtain the 2-pole notch ﬁlter.
Combining the LP to HP and LP to BP substitutions expressions in the
mentioned order gives an algebraic expression for the LP to BS substitution:
1
s
←
1
R
·
s + s−1
2
(4.22)
The bandwidth considerations of the LP to BS substitution are pretty much
equivalent to those of LP to BP substitution and can be obtained by considering
the LP to BS substitution as an LP to BP substitution applied to a result of
the LP to HP substitution.
The block-diagram form of the LP to BS substitution can be obtained by
directly implementing the right-hand expression in (4.22) as a replacement for
the integrators. This however requires a diﬀerentiator for the implementation
of the s term of the sum.
4.7 Further filter types
By mixing the lowpass, bandpass and highpass outputs one can obtain further
ﬁlter types. We are now going to discuss some of them.
Often it will be convenient to also include the input signal and the normalized
bandpass signal into the set of the mixing sources. Apparently this doesn’t
bring any new possibilities in terms of the obtained transfer functions, since the
input signal can be obtained as a linear combination of LP, BP and HP signals.
However the mixing coeﬃcients might look simpler in certain cases. One can
13For a differentiator, a similar substitution structure (containing an integrator and a dif-
ferentiator) is trivially obtained from the right-hand side of (4.16).
14Notice that BS here stands for “bandstop” and not for “band-shelving”. The alternative
name for the substitution could have been “LP to notch”, but “LP to bandstop” seems to be
commonly used, so we’ll stick to that one.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
118
CHAPTER 4. STATE VARIABLE FILTER
also go further and consider using diﬀerent topologies implementing a given 2-
pole transfer function. Such topologies could diﬀer not only in which speciﬁc
signals are mixed, but also whether certain mixing coeﬃcients are used at the
input or at the output, whether transposed or non-transposed SVF is being
used, etc. We won’t go here into addressing this kind of detail, however the
discussion of the topological aspects of the normalized bandpass in Section 4.5
could serve as an example.
Band-shelving filter
By adding/subtracting the unit gain bandpass signal to/from the input signal
one obtains the band-shelving ﬁlter (Fig. 4.20):
HBS(s) = 1 + K · HBP1(s) = 1 + 2RKHBP(s) = 1 +
2RKs
s2 + 2Rs + 1
As with 1-pole shelving we can also specify the shelving boost in decibel:
GdB = 20 log10(K + 1)
|H(jω)|, dB
+12
+6
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 4.20: Amplitude response of a 2-pole band-shelving ﬁlter
for R = 1 and varying K.
The immediately noticeable problem in Fig. 4.20 is that the bandwidth of
the ﬁlter varies with the shelving boost K. A way to address this issue will be
described in Chapter 10.
Low- and high-shelving filters
Attempting to obtain 2-pole low- and high-shelving ﬁlters in a straightforward
fashion:
HLS(s) = 1 + K · HLP(s)
HHS(s) = 1 + K · HHP(s)
4.7. FURTHER FILTER TYPES
119
we notice that the amplitude responses of such ﬁlters have a strange dip (for
K > 0) or peak (for K < 0) even at a non-resonating setting of R = 1 (Fig. 4.21).
This peak/dip is due to a steeper phase response curve of the 2-pole lowpass
and highpass ﬁlters compared to 1-poles. A way to build 2-pole low- and high-
shelving ﬁlters, which do not have this problem, is described in Chapter 10.
|H(jω)|, dB
+6
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 4.21: Amplitude response of a naive 2-pole low-shelving
ﬁlter for R = 1 and varying K.
Notch filter
At K = −1 the band-shelving ﬁlter turns into a notch (or bandstop) ﬁlter
(Fig. 4.22):
HN(s) = 1 − HBP1(s) = 1 − 2RHBP(s) =
s2 + 1
s2 + 2Rs + 1
Allpass filter
At K = −2 the band-shelving ﬁlter turns into an allpass ﬁlter (Fig. 4.23):
HAP(s) = 1 − 2HBP1(s) = 1 − 4RHBP(s) =
s2 − 2Rs + 1
s2 + 2Rs + 1
(4.23)
It is not diﬃcult to show that for purely imaginary s the absolute magin-
tudes of the transfer function’s numerator and denominator are equal and thus
|HAP(jω)| = 1.
We could also notice that the phase respose of the 2-pole allpass is simply
the doubled 2-pole lowpass phase response:
arg HAP(jω) = arg
1 − 2Rjω − ω2
1 + 2Rjω − ω2 =
= arg(1 − 2Rjω − ω2) − arg(1 + 2Rjω − ω2) =
120
|H(jω)|
1
0.5
0
CHAPTER 4. STATE VARIABLE FILTER
R = 0.2
R = 5
R = 1
ωc/8
ωc
8ωc
ω
Figure 4.22: Amplitude response of a 2-pole notch ﬁlter. The
amplitude scale is linear.
arg H(jω)
0
−π
−2π
R = 0.2
R = 1
R = 5
ωc/8
ωc
8ωc
ω
Figure 4.23: Phase response of a 2-pole allpass ﬁlter.
= −2 arg(1 + 2Rjω − ω2) = 2 arg HLP(jω)
(4.24)
Thus the allpass phase response has the same symmetry around the cutoﬀ point
and the damping parameter has a similar eﬀect on the phase response slope.
At R ≥ 1 the 2-pole allpass can be decomposed into the product of 1-pole
allpasses:
HAP(s) =
s − ω1
s + ω1
·
s − ω2
s + ω2
=
ω1 − s
ω1 + s
·
ω2 − s
ω2 + s
where ωn = −pn. At R = 1 we have ω1 = ω2 = 1 and the ﬁlter turns into the
squared 1-pole allpass:
HAP(s) =
(cid:19)2
(cid:18) s − 1
s + 1
(cid:19)2
=
(cid:18) 1 − s
1 + s
4.7. FURTHER FILTER TYPES
121
Peaking filter
By subtracting the highpass signal from the lowpass signal (or also vice versa)
we obtain the peaking ﬁlter (Fig. 4.24):
HPK(s) = HLP(s) − HHP(s) =
1 − s2
s2 + 2Rs + 1
|H(jω)|, dB
+12
+6
0
-6
-12
-18
R = 0.2
R = 5
R = 1
ωc/8
ωc
8ωc
ω
Figure 4.24: Amplitude response of a 2-pole peaking ﬁlter.
The peaking ﬁlter is a special kind of bandshelving ﬁlter. However, as one
can see from Fig. 4.24, the bandwidth of the ﬁlter varies drastically with R,
which often may be undesired. A “properly built” bandshelving ﬁlter allows to
avoid this problem. This topic is further discussed in Chapter 10.
Arbitrary 2-pole transfer functions
It’s easy to see that the state-variable ﬁlter can be used to implement any 2nd-
order stable diﬀerential ﬁlter. Indeed, consider the generic 2nd-order transfer
function
H(s) =
where we assume a0 > 0.15 Then
b2s2 + b1s + b0
s2 + a1s + a0
H(s) =
b2s2 + b1s + b0
√
a0s +
a1
√
=
b2s2 + b1s + b0
s2 + 2Rωcs + ω2
c
=
√
2
a0
s2 + 2
2
a0
15If a0 = 0, this means that either one or both of the poles of H(s) are at s = 0. If a0 < 0
this means that we are having two real poles of opposite signs. Both situations correspond to
pretty exotic unstable cases.
122
CHAPTER 4. STATE VARIABLE FILTER
= b2
s2
s2 + 2Rωcs + ω2
c
+
b1
ωc
= b2HHP(s) +
b1
ωc
HBP(s) +
·
ωcs
s2 + 2Rωcs + ω2
c
b0
ω2
c
HLP(s)
+
b0
ω2
c
·
ω2
c
s2 + 2Rωcs + ω2
c
=
where we introduced ωc =
√
a0 and R = a1/ωc.
4.8 Transient response
In the transient response analysis of the state-variable ﬁlter we will concentrate
on the lowpass output. The bandpass and highpass can be obtained from the
lowpass using (4.1):
yBP = ˙yLP/ωc
yHP = ˙yBP/ωc = ¨yLP/ω2
c
(4.25a)
(4.25b)
Using (4.10) we rewrite (4.3) in terms of poles, obtaining
¨y − (p1 + p2) ˙y + p1p2 = p1p2x
(4.26)
where y = yLP. Let16
(4.27a)
(4.27b)
u1 = ˙y − p2y
u2 = ˙y − p1y
˙u1 = ¨y − p2 ˙y
˙u2 = ¨y − p1 ˙y
Therefore
and
( ˙u1 + ˙u2) − (p1u1 + p2u2) = (2¨y − (p1 + p2) ˙y) − ((p1 + p2) ˙y − 2p1p2y) =
= 2¨y − 2(p1 + p)2) ˙y + 2p1p2y
(4.28)
Noticing that the last expression is simply the doubled left-hand side of (4.26)
we obtain an equivalent form of (4.26):
( ˙u1 + ˙u2) − (p1u1 + p2u2) = 2p1p2x
(4.29)
Splitting the latter in two halves we have:
˙u1 − p1u1 = p1p2x
˙u2 − p2u2 = p1p2x
(4.30a)
(4.30b)
Adding both equations (4.30) back together, we obtain (4.29), which is equiva-
lent to (4.26). This means that if u1 and u2 are solutions of (4.30) then using
(4.27) we can ﬁnd y from u1 and u2, which will be the solution of (4.26).
16The substitution (4.27) can be obtained, knowing in advance the transient response form
y = C1ep1t + C2ep2t and expressing epnt via y and ˙y. Alternatively, it can be found by
diagonalizing the state-space form.
4.8. TRANSIENT RESPONSE
123
Now, each of the equations (4.30) is a Jordan 1-pole with input signal p1p2x.
Applying (2.22) we obtain
un(t) = un(0)epnt + p1p2
(cid:90) t
0
epn(t−τ )x(τ ) dτ
(n = 1, 2)
or, for x(t) = X(s)est, we have from (2.23):
un(t) = Hn(s)x(t) + (cid:0)un(0) − Hn(s)x(0)(cid:1)epnt = usn(t) + utn(t)
(4.31)
where
Hn(s) =
p1p2
s − pn
and where usn(t) and utn(t) denote the steady-state and transient response
parts of un(t) respectively. Expressing y via un from (4.27) we have
y =
u1 − u2
p1 − p2
(4.32)
For the steady-state response we therefore obtain from (4.31):
ys(t) =
us1 − us2
p1 − p2
=
H1(s) − H2(s)
p1 − p2
x(t) = H(s)x(t)
where
H(s) =
=
=
p1p2
s − p1
−
p1p2
s − p2
H1(s) − H2(s)
p1 − p2
=
·
p1p2
p1 − p2
p1 − p2
(s − p2) − (s − p1)
s2 − (p1 + p2)s + p1p2s
p1p2
s2 − (p1 + p2)s + p1p2s
=
=
=
ω2
c
s2 + 2Rωc + ω2
c
(4.33)
is the familiar 2-pole lowpass transfer function. The steady-state response ys(t)
is therefore having the same form H(s)x(t) for a complex exponential x(t) =
X(s)est as in case of the 1-pole ﬁlter. For signals of general form we respectively
obtain the same formula (2.20a) as for 1-poles.
For the transient response we have
yt(t) =
=
=
=
ut1 − ut2
p1 − p2
˙y(0) − p2y(0) − H1(s)x(0)
p1 − p2
˙y(0) − p2(y(0) − G1(s)x(0))
p1 − p2
· ep1t −
˙y(0) − p1y(0) − H2(s)x(0)
p1 − p2
· ep2t =
· ep1t −
˙y(0) − p1(y(0) − G2(s)x(0))
p1 − p2
· ep2t
(4.34)
where we introduce the ordinary (except that pn may be complex) 1-pole lowpass
transfer functions
Gn(s) =
−pn
s − pn
124
CHAPTER 4. STATE VARIABLE FILTER
Provided Re p1,2 < 0 we are having a sum of two exponentially decaying
terms. Since y(0) = ys(0) + yt(0), the initial value of this sum is yt(0) =
y(0) − ys(0), the same as in the 1-pole case, so we’re having an exponentially
decaying discrepancy between the output signal and the steady-state response.
However the decaying is now being “distributed” between two exponents ep1t
and ep2t. Also notice that while in the 1-pole case the decaying was only aﬀected
by the initial state y(0), in the 2-pole case ˙y(0) is also a part of the initial state
and therefore also aﬀects the decaying shape. Apparently, y(0) is the state of
the second (“lowpass”) integrator of the SVF, while, according to (4.25a), ˙y(0)
is essentially the state of the ﬁrst (“bandpass”) integrator.
At Re p1,2 > 0 the transient response grows inﬁnitely and the ﬁlter explodes.
Steady-state response
In regards to the choice of the steady-state response, there is a similar ambiguity
arising out of evaluating the inverse Laplace transform of H(s)X(s) to the left
or to the right of the poles of H(s). We won’t speciﬁcally go into the analysis
of this situation for the real poles occurring in the case |R| > 1. Complex poles
occurring in the case |R| < 1 deserve some specical attention.
Apparently Re p1 = Re p2 in this case, and we wish to know how much does
the inverse Laplace transform change when we switch the integration path from
Re s < Re pn to Re s > Re pn. By the residue theorem this change will be equal
to the sum of the residues of H(s)X(s)est at s = p1 and s = p2 respectively,
which is
Res
s=p1
H(s)X(s)est + Res
s=p2
H(s)X(s)est =
p1p2
p1 − p2
(cid:0)X(p1)ep1t − X(p2)ep2t(cid:1)
(4.35)
(where we have used (4.33)). That is we are again obtaining the terms which
already exist in the transient response and the integration path choice only
aﬀects the amplitudes of the transient response partials, as long as we are staying
within the region of convergence of X(s).
The case of coinciding poles requires a separate analysis which can be done
as a limiting case R → ±1. The respective discussion is occurring later in this
section. Even though we don’t speciﬁcally address the question of evaluation of
the inverse Laplace transform in the steady-state response there, it should be
clear what the principles would be.
Continuity
Since the input signal of an SVF passes through two integrators on the way to
the lowpass output, the lowpass signal should not only always be continuous but
should also always have a continuous 1st derivative. Therefore the appearance
of ˙y(0) besides y(0) in the transient response expression must have somehow
taken care of that. Let’s verify that this is indeed the case.
Evaluating (4.34) at t = 0 using we obtain
yt(0) =
˙y(0) − p2y(0) − H1(s)x(0)
p1 − p2
H1(s) − H2(s)
p1 − p2
= y(0) −
−
˙y(0) − p1y(0) − H2(s)x(0)
p1 − p2
=
x(0) = y(0) − H(s)x(0) = y(0) − ys(0)
4.8. TRANSIENT RESPONSE
125
where we have used (4.33). Evaluating the derivative of (4.34) at t = 0 we
obtain
˙yt(0) = p1
˙y(0) − p2y(0) − H1(s)x(0)
p1 − p2
− p2
˙y(0) − p1y(0) − H2(s)x(0)
p1 − p2
=
= ˙y(0) +
= ˙y(0) +
−p1H1(s) + p2H2(s)
p1 − p2
−p2
s − p2
−p1
s − p1
−
p1 − p2
· p1p2x(0) =
· p1p2x(0) =
= ˙y(0) +
(−p1)(s − p2) − (−p2)(s − p1)
p1 − p2
·
p1p2
(s − p1)(s − p2)
x(0) =
= ˙y(0) −
p1p2 · s
s2 − (p1 + p2)s + p1p2
x(0) = ˙y(0) −
ω2s
s2 + 2Rωc + ω2
c
x(0) =
= ˙y(0) −
ω2
s2 + 2Rωc + ω2
c
· sX(s)est(cid:12)
(cid:12)
(cid:12)t=0
= ˙y(0) − ˙ys(0)
which conﬁrms our expectations.
Complex vs. real poles
If p1,2 are complex we have
epnt = et Re pn · (cos(t Im pn) + j sin(t Im pn))
The mutual conjugate property of poles will ensure that the two terms of (4.34)
are mutually conjugate as well, therefore the addition result is purely real and
has the form
yt(t) = a · et Re p1 · cos (| Im p1| · t + ϕ) =
= a · et Re p2 · cos (| Im p2| · t + ϕ) =
(cid:16)
(cid:112)
= a · e−Rωct · cos
1 − R2 · t + ϕ
ωc
(cid:17)
(4.36)
The transient response therefore is a sinusoidal oscillation of frequency | Im pn|
decaying (or exploding) as et Re pn . Fig. 4.25 illustrates.
For purely real poles the transient response contains just two real exponents
of the form epnt, thereby having no oscillations. However, it can still contain
one “swing” at certain combinations of the amplitudes of the transient partials
ep1t and ep2t (Fig. 4.26).
Strong resonance case
The decay speed of the transient response oscillation (4.36) gets slower as R
decreases, which leads to an increased perceived duration of the transient in
the output signal. Therefore at high resonance settings a transient in the input
signal will produce audible ringing at resonance frequency, even if the steady-
state signal doesn’t contain it.
126
CHAPTER 4. STATE VARIABLE FILTER
yt(t)
y(0)−ys(0)
0
Figure 4.25: Transient response of a resonating 2-pole lowpass ﬁlter
(dashed line depicts the unstable case).
yt(t)
y(0)−ys(0)
0
t
t
Figure 4.26: Transient response of a non-resonating 1-pole lowpass
ﬁlter (for the case of a single zero-crossing).
A pretty characteristic and easy to analyse case occurs if we suddenly switch
oﬀ the ﬁlter’s input signal. At this moment the steady-state response instanta-
neously turns to zero and (4.34) turns into
yt(t) =
=
=
˙y(0) − p2y(0)
p1 − p2
˙y(0) − p∗
1y(0)
2j Im p1
˙y(0) − p∗
1y(0)
2j Im p1
· ep1t −
· ep1t −
· ep1t +
˙y(0) − p1y(0)
p1 − p2
˙y(0) − p1y(0)
2j Im p1
˙y(0) − p1y(0)
2j∗ Im p1
· ep2t =
· ep∗
1 t =
· ep∗
1 t =
4.8. TRANSIENT RESPONSE
127
= 2 Re
(cid:18) ˙y(0) − p∗
2j Im p1
1y(0)
(cid:19)
ep1t
= Re
(cid:18) ˙y(0) − p∗
j Im p1
1y(0)
(cid:19)
ep1t
Therefore
y(t) = ys(t) + yt(t) = 0 + yt(t) = Re
(cid:18) ˙y(0) − p∗
j Im p1
1y(0)
(cid:19)
ep1t
Unless both y(0) = 0 and ˙y(0) = 0, the signal y(t) will have a non-zero ampli-
1 − R2
tude and according to (4.36) we are having a sinusoid of frequency ωc
decaying as e−Rωct.
√
The opposite situation of a signal being turned on is a kind of a dual case
of turning a signal oﬀ. Indeed, let x0(t) be some inﬁnitlely long (that is t ∈
(−∞, ∞)) steady input signal and let y0(t) be the respective output signal.
Assuming that the ﬁlter is stable and that the initial time moment was at
t = −∞, by any ﬁnite time moment t the transient response component of y0(t)
has decayed to zero, and y0(t) consists solely of the steady-state response. Let
x1(t) =
(cid:40)
x0(t)
0
if t < 0
if t ≥ 0
be another inﬁnitely long signal decribing the case of the signal x0(t) being
turned oﬀ and let y1(t) be the respective output signal. The signal x1(t) contains
a transient at t = 0, thus y1(t) contains a non-zero transient response component
for t ≥ 0. The case of x0(t) being turned on is respectively described by
x2(t) = x0(t) − x1(t) =
(cid:40)
0
x0(t)
if t < 0
if t ≥ 0
and we let y2(t) denote the corresponding output signal. Since the system is
linear, the output signals are related in the same way as the input signals:
y2(t) = y0(t) − y1(t)
However y0(t) doesn’t contain any transient response, therefore the only tran-
sient response present in y2(t) is coming from y1(t), simply having the opposite
sign.
The eﬀect of the transient response is particularly remarkable if the input
1 − R2 as the transient response.
signal is a sinusoid of the same frequency ωc
First considering the case of turning such sinusoid oﬀ we take
√
(cid:112)
1 − R2 · t + ϕin)
x0(t) = ain cos(ωc
(cid:40)
x1(t) =
x0(t)
0
if t < 0
if t ≥ 0
We must have the same sinusoid at the output:
y0(t) = aout cos(ωc
(cid:40)
y1(t) =
aout cos(ωc
ate−Rωct · cos(ωc
√
(cid:112)
1 − R2 · t + ϕout)
√
1 − R2 · t + ϕout)
1 − R2 · t + ϕt)
if t < 0
if t ≥ 0
128
CHAPTER 4. STATE VARIABLE FILTER
where the transient response’s amplitude and phase at and ϕt may diﬀer from
the steady-state response’s aout and ϕout due to the additional factor e−Rωct
appearing in the signal. However from the requirement of continuity of y1(t)
and ˙y1(t) at t = 0 we may conclude that at → aout and ϕt → ϕout for R → 0.
Now let’s consider the case of turning the signal on. We let x2(t) = x0(t) −
x1(t). Since we already know that y2(t) = 0 for t < 0, we are interested only in
y2(t) for t ≥ 0 where we have
y2(t) = y0(t) − y1(t) =
= aout cos(ωc
(cid:112)
1 − R2 · t + ϕout) − ate−Rωct · cos(ωc
(cid:112)
1 − R2 · t + ϕt)
Since at R ≈ 0 we have at ≈ aout and ϕt ≈ ϕout, we may in this case rewrite
the above as
y2(t) ≈ (1 − e−Rωct) · aout cos(ωc
(cid:112)
1 − R2 · t + ϕout)
(R ≈ 0)
That is the sinusoid in the output signal is exponentially fading in as 1−e−Rωct.
Eﬀectively the transient response is suppressing the steady state signal in the
beginning and then slowly lets it fade in (Fig. 4.27).
y(t)
0
t
√
Figure 4.27: Initial suppression of the steady-state signal at ω =
ωc
1 − R2 by the transient response.
Selfoscillation
At R = 0 the transient response oscillates at a constant amplitude, the frequency
of the oscillation being ωc and coinciding with the inﬁnitely high peak of the
amplitude response. Thus, if in the absence of the input signal the system
is somehow in a non-zero state, it will stay in this state forever, producing a
sinusoid of frequency ωc. Such state of oscillating without an input signal is
referred to as selfoscillation.
At R < 0 the transient response turns into an inﬁnitely growing signal,
while the oscillation frequency becomes lower than ωc according to (4.36). In
nonlinear ﬁlters at −1 < R < 0 the growing amplitude of the oscillating transient
4.8. TRANSIENT RESPONSE
129
response will be limited by the saturation, which thereby prevents the ﬁlter
from exploding. In either case, apparently it is the transient response which is
responsible for the selfoscillation of the ﬁlter.
We can therefore refer to −1 < R ≤ 0 as the selfoscillation range of the
ﬁlter. The boundary R = 0 at which the selfoscillation appears may be referred
to as selfoscillation point.17
At the selfoscillation point the poles of the system are located right on the
imaginary axis and we can “hit” them with an input sinusoidal signal of fre-
quency ωc. Since H(±jωc) = ∞, the steady-state response H(s)X(s)est be-
comes inﬁnite too and we need a diﬀerent choice of the steady-state response
signal.
A real sinusoidal signal of frequency ωc consists of two complex sinusoidal
signals of frequencies ±ωc. Each of these two signals hits the respective complex
pole of the system at p1,2 = ±jωc. As we should recall from the discussion in
Section 2.15, when a system pole p is hit by an input ept, the output of the
system consists of a linear combination of partials ept and tept, where we cannot
unambigously select the steady-state response part. From two conjugate poles
p1 and p2 we’ll get a linear combination of ep1t and tep1t and another one of
ep2t and tep2t. After these signals are further combined by (4.32) we’ll get a
real signal of the form
y(t) = a1 · cos(ωct + ϕ1) + a2 · t cos(ωct + ϕ2)
Thus, the output signal is a sinusoid of frequency ωc with the amplitude asymp-
totically growing as a linear function of time.18 Clearly, this is a marginal case
between the sinusoidal output stabilizing with time if R > 0, as e.g. shown in
Fig. 4.27, and exponentially exploding if R < 0.
Coinciding poles
A special situation occurs if R = ±1 and thus p1 = p2. The denominator p1 − p2
therefore turns to zero, but we can treat this as a limiting case of R → ±1. Let
p1,2 = p ± ∆ (where p1,2 → p and ∆ → 0). Noticing that
G1(s) →
−p
s − p
G2(s) →
−p
s − p
we can replace Gn(s) in (4.34) with −p/(s − p) before taking the limit:
˙y(0) − p2(y(0) − −p
s−p x(0))
˙y(0) − p1(y(0) − −p
s−p x(0))
· ep1t −
yt(t) =
p1 − p2
ep1t − ep2t
p1 − p2
= ˙y(0) ·
(cid:18)
+
y(0) −
(cid:19)
x(0)
·
−p
s − p
p1 − p2
−p2ep1t + p1ep2t
p1 − p2
· ep2t =
(4.37)
In the ﬁrst term of (4.37) we have
ep1t − ep2t
p1 − p2
=
eΔt − e−Δt
2∆
· ept =
17The other boundary R = −1 is hardly ever being reached, therefore we won’t introduce a
special name for it.
18Notice that as the ratio of the amplitudes of the two sinusoids changes, the phase of their
sum (which in principle is a sinusoid of the same frequency but of a different amplitude and
phase) will slightly drift.
130
CHAPTER 4. STATE VARIABLE FILTER
=
eΔt − e−Δt
2∆t
· tept =
sinh ∆t
∆t
· tept → tept
(∆ → 0)
and in the second term respectively
−p2ep1t + p1ep2t
p1 − p2
=
= −p
= −p
eΔt − e−Δt
2∆
sinh ∆t
∆t
−(p − ∆)eΔt + (p + ∆)e−Δt
2∆
eΔt + e−Δt
2∆
· ept + ∆
· ept =
· ept =
· tept + cosh ∆t · ept → −ptept + ept
(∆ → 0)
and (4.37) at ∆ = 0 can be rewritten as
yt(t) = ˙y(0) · tept +
(cid:18)
y(0) −
(cid:19)
x(0)
−p
s − p
· (−ptept + ept) =
(cid:18)
=
y(0) −
−p
s − p
(cid:19)
x(0)
· ept +
(cid:18)
(cid:18)
˙y(0) − p ·
y(0) −
(cid:19)(cid:19)
x(0)
· tept
−p
s − p
Thus, in the case of p1 = p2 the terms contained in the transient response are
having the form ept or tept.
The change (4.35) in the inverse Laplace transform in the steady-state re-
sponse as we take the integral to the left or to the right of the poles of H(s)
respectively becomes
Res
s=p+Δ
H(s)X(s)est + Res
s=p−Δ
H(s)X(s)est ∼
p2
2∆
X(p + ∆)e(p+Δ)t − X(p − ∆)e(p−Δ)t(cid:17)
(cid:16)
∼
= p2 X(p + ∆)e(p+Δ)t − X(p − ∆)e(p−Δ)t
2∆
→ p2 X (cid:48)(p)ept + X(p)tept + X (cid:48)(p)ept + X(p)tept
2
= p2 (cid:0)X (cid:48)(p)ept + X(p)tept(cid:1)
(∆ → 0)
→
=
=
where we have used l’Hˆopital’s rule.19 Thus, the change is again solely in the
amplitudes of the transient response partials.
It is important to realize that the diﬀerent form of the transient response
components at R = ±1 doesn’t imply that the ﬁlter behavior is abruptly
switched at this point. The switching of the mathematical expression is solely
due to the limitations of the mathematical notation, but doesn’t correspond to
a jump in any of the signals.
The same result could have been obtained formally by introducing the helper
variables u1 and u2 diﬀerently:20
u1 = ˙y − py
19More rigorously speaking, we have used l’H^opital’s rule as a short way to express the
following: we expand X(p ± Δ) and e(p±Δ)t into Taylor series with respect to Δ, followed by
expanding the respective products and cancelling the terms containing Δ with the denomi-
nator. One also could expand just X(p ± Δ) into Taylor series with respect to Δ and then
convert e(p±Δ)t into sinh and cosh in the same way as in the transient response derivation.
20This corresponds to using Jordan normal form in the state space representation.
SUMMARY
131
(where p = p1 = p2) thereby obtaining the equations
u2 = y
˙u1 − pu1 = p2x
˙u2 − pu2 = u1
which can be solved using 1-pole techniques. Since u2 is the input signal for u1
we have a serial connection of 1-poles, building up a Jordan chain. As we should
remember from the discussion of Jordan chains in Section 2.15, the transient
response will consist of the partials of the form ept and tept. However, due to a
completely diﬀerent substitution of variables, we wouldn’t have known, whether
the output is changing in a continuous way as R crosses the point R = 1. On
the other hand, obtaining the result as a limiting case, as we did earlier, gives
an answer to that question.
Bandpass and highpass
Notice that (4.25) can be applied separately to steady-state and transient re-
sponses (in the sense that the results will still give correct separation of the
signal into the steady-state and transient parts). Indeed, e.g. applying (4.25a)
to a complex exponential yLP = Y (s)est we obtain
˙yLP/ωc = sY (s)est/ωc = yLP · s/ωc
which matches HBP(s) = s/ωc · HLP(s). Therefore ˙yLP/ωc, when applied to a
lowpass steady-state response yLPs(t), will give bandpass steady-state response,
etc.
This means that the transient response for the bandpass and highpass signals
can be obtained by diﬀerentiating the lowpass transient response according to
(4.25), resulting in a sum of the same kind of exponential terms ep1t and ep2t (or
ept and tept in case p1 = p2). We won’t write the resulting expressions explicitly
here.
SUMMARY
The state-variable ﬁlter has the structure shown in Fig. 4.1. Contrarily to the
ladder ﬁlter, the resonance strength in the SVF is controlled by controlling the
damping signal. The multimode outputs have the transfer functions
HHP(s) =
HBP(s) =
HLP(s) =
s2
s2 + 2Rs + 1
s
s2 + 2Rs + 1
1
s2 + 2Rs + 1
and can be combined to build further ﬁlter types.
132
CHAPTER 4. STATE VARIABLE FILTER
Chapter 5
Ladder filter
In this chapter we are going to discuss the most classical analog ﬁlter model:
the transistor ladder ﬁlter. The main idea of this structure, which is to create
resonance by means of a feedback loop, is encountered in many other ﬁlter
designs, some of which we are also going to discuss. We will be referring to the
class of such ﬁlters as simply ladder filters.1
5.1 Analog model
The most classical example of a ladder ﬁlter is transistor ladder ﬁlter, which
implements a 4-pole lowpass structure shown in Fig. 5.1.2 The structure in
Fig. 5.1 is not limited to transistor-based analog implementations. Particularly,
there are many implementations of the same structure based on OTAs (oper-
ational transconductance ampliﬁers). The diﬀerence between transistor- and
OTA-based ladders is, however, lying in the nonlinear behavior, which we are
not touching at this point yet. The linear aspects of both are identical.
The LP1 blocks denote four identical (same cutoﬀ) 1-pole lowpass ﬁlters
(Fig. 2.2). The k coeﬃcient controls the amount of negative feedback, which
creates resonance in the ﬁlter. Typically k ≥ 0, although k < 0 is also sometimes
used.
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
LP1
LP1
LP1
•(cid:47)
y(t)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 5.1: Transistor (4-pole lowpass) ladder ﬁlter.
1Quite unfortunately, there is already another class of filter structures commonly referred
to as “ladder filters”. Fortunately, this class is not so widely encountered in the synth filter
context, on the other hand “transistor ladder” is also a commonly used term. Therefore we’ll
stick with using the term “ladder filters” for the flters based on a resonating feedback loop.
2A widely known piece of work describing this linear model is Analyzing the Moog VCF
with considerations for digital implementation by T.Stilson and J.Smith.
133
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
134
Let
CHAPTER 5. LADDER FILTER
H1(s) =
1
1 + s
be the 1-pole lowpass transfer function. Assuming complex exponential x and
y we write
y = H 4
1 (s) · (x − ky)
from where
1 (s)) = H 4
and the transfer function of the ladder ﬁlter is
y(1 + kH 4
1 (s) · x
H(s) =
y
x
=
H 4
1 (s)
1 + kH 4
1 (s)
=
1
(1+s)4
1
(1+s)4
1 + k
=
1
k + (1 + s)4
(5.1)
At k = 0 the ﬁlter behaves as 4 serially connected 1-pole lowpass ﬁlters.
The poles of the ﬁlter are respectively found from
giving
k + (1 + s)4 = 0
s = −1 + (−k)1/4
where the raising to the 1/4th power is understood in the complex sense, there-
fore giving 4 diﬀerent values:
s = −1 +
±1 ± j
√
2
k1/4
(k ≥ 0)
(5.2)
(this time k1/4 is understood in the real sense). Thus there are 4-poles and we
can also refer to this ﬁlter as a 4-pole lowpass ladder ﬁlter.
At k = 0 all poles are located at s = −1, as k grows they move apart in 4
straight lines,all going at “45◦ angles” (Fig. 5.2). As k grows from 0 to 4 the
two of the poles (at s = −1 + 1±j√
k1/4) are moving towards the imaginary axis,
2
producing a resonance peak in the amplitude response (Fig. 5.3). At k = 4 they
hit the imaginary axis:
(cid:18)
Re
−1 +
(cid:19)
41/4
= 0
1 ± j
√
2
and the ﬁlter becomes unstable.3
In Fig. 5.3 one could notice that, as the resonance increases, the ﬁlter gain at
low frequencies begins to drop. Indeed, substituting s = 0 into (5.1) we obtain
H(0) =
1
1 + k
This is a general issue with ladder ﬁlter designs.
3This time we will not develop an explicit expression for the transient response, since it’s
getting too involved. Still, the general rule, which we will develop in Section 7.7, is that the
transient response is always a linear combination of partials of the form epnt (and tν epnt in
case of repeated poles), where pn are the filter poles. Respectively, as soon as some of the
poles leave the left complex semiplane, the filter becomes unstable.
5.2. FEEDBACK AND RESONANCE
135
Im s
j
−1
Re s
−j
Figure 5.2: Poles of the 4-pole lowpass ladder ﬁlter.
|H(jω)|, dB
k = 0
0
-6
-12
-18
k = 3
ωc/8
ωc
8ωc
ω
Figure 5.3: Amplitude response of the 4-pole lowpass ladder ﬁlter
for various k.
5.2 Feedback and resonance
Before we continue with discussing more practical aspects of the ladder ﬁlter,
we’d like to make one important observation considering the resonance peaks
created by the ladder ﬁlter feedback.
In Fig. 5.3 we can see that, similarly to the 2-pole case, the resonance fre-
quency is approaching the ﬁlter cutoﬀ frequency as the ﬁlter approaches selfos-
cillation at k = 4. This is a manifestation of a more general principle concerning
ladder ﬁlters as such. Consider a general ladder ﬁlter in Fig. 5.4, where G(s) de-
notes a more or less arbitrary structure, whose transfer function is G(s). Notice
that the feedback in Fig. 5.4 is not inverted.
136
CHAPTER 5. LADDER FILTER
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
G(s)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
•(cid:47)
y(t)
Figure 5.4: Structure of a generic ladder ﬁlter.
The transfer function of the entire structure is therefore
H(s) =
G(s)
1 − kG(s)
=
1
G−1(s) − k
and the poles are deﬁned by the equation
G−1(s) = k
(5.3)
(5.4)
That is at k = 0 the poles of H(s) are the zeros of G−1(s) (the latter obviously
simply being the poles of G(s)). As k begins to deviate from zero, the solutions
of (5.4) will move in the s-plane, usually in a continuous fashion. E.g. for the
4-pole lowpass ladder (Fig. 5.1) we had G−1(s) = (s + 1)4 and (5.4) takes the
form (s + 1)4 = −k, where we take −k instead of k because of the inverted
feedback in Fig. 5.1.
The value of k at which the ﬁlter starts to selfoscillate should correspond
to some of the poles being located on the imaginary axis. At this moment the
inﬁnitely high resonance peak in the amplitude response is occurring exactly
at these pole positions. Denoting a purely imaginary pole position as jω, we
rewrite (5.4) for such poles as
G−1(jω) = k
or
kG(jω) = 1
(5.5)
We can refer to (5.5) as the selfoscillation equation for a feedback loop. This
equation implies that selfoscillation appears at the moment when the total fre-
quency response across the feedback loop kG(jω) exactly equals 1 at some
frequency ω. That is the total amplitude gain must be 1, and the total phase
shift must be 0◦.
This is actually a pretty remarkable result. Of course it is quite intuitive that
selfoscillation tends to occur at frequencies where the feedback signal doesn’t
cancel the input signal, but rather boosts it. And such boosting tends to be
strongest at frequencies where we have a 0◦ total phase shift across the feedback
loop. However, what is quite counterintuitive, is that selfoscillation can appear
(as k is reaching the respective threshold value) only at such frequencies.4
Therefore for k > 0 the selfoscillation appears at frequencies where the phase
response of G(s) is 0◦. For k < 0 the selfoscillation appears at frequencies where
4As k continues to grow into the unstable range, the frequencies of the exploding (or still
selfoscillating, if the filter is nonlinear) sinusoidal transient response partials can change, since
the imaginary part of the resonating poles can change as the poles move beyond the imaginary
axis.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
5.3. DIGITAL MODEL
137
the phase response of G(s) is 180◦. The respective value of k can be found from
(5.5) giving
k =
1
G(jω)
(5.6)
(5.7)
or, rewriting (5.6) in terms of the amplitude response of G(s):
k = ±
1
|G(jω)|
where we take the plus sign if the phase response of G(s) at ω is 0◦ and the
minus sign if the phase response of G(s) at ω is 180◦.
The just discussed eﬀects are the reason that we used negative feedback in
the 4-pole lowpass ladder ﬁlter. We want the resonance to occur at the ﬁlter’s
cutoﬀ. The phase response of a single 1-pole lowpass at the cutoﬀ frequency
is −45◦, respectively the phase response of a chain of four 1-poles is −180◦,
exactly what we need for the resonance peak, if we use negative feedback.
At the same time, the amplitude response of a 1-pole lowpass at the cutoﬀ
2, respectively the amplitude response of a chain of four
2)4 = 1/4. According to (5.7), the inﬁnite resonance is attained
√
is |1/(1 + j)| = 1/
1-poles is (1/
at k = 1/(1/4) = 4.
√
At ω = 0 a chain of four 1-pole lowpasses will have a phase shift of 0◦, while
the amplitude response at ω = 0 is 1. Therefore, in Fig. 5.1 the “selfoscillation”
at ω = 0 will occur at k = −1. However the amplitude response peak at ω = 0
hardly can count as resonance.
5.3 Digital model
A naive digital implementation of the ladder ﬁlter shouldn’t pose any problems.
We will therefore immediately skip to the TPT approach.
Recalling the instantaneous response of a single 1-pole lowpass ﬁlter (3.29),
we can construct the instantaneous response of a serial connection of four of
such ﬁlters. Indeed, let’s denote the instantaneous responses of the respective
1-poles as fn(ξ) = gξ + sn (obviously, the coeﬃcient g is identical for all four,
whereas sn depends on the ﬁlter state and therefore cannot be assumed identi-
cal). Combining two such ﬁlters in series we have
f2(f1(ξ)) = g(gξ + s1) + s2 = g2ξ + gs1 + s2
Adding the third one:
f3(f2(f1(ξ))) = g(g2ξ + gs1 + s2) + s3 = g3ξ + g2s1 + gs2 + s3
and the fourth one:
f4(f3(f2(f1(ξ)))) = g(g3ξ + g2s1 + gs2 + s3) =
= g4ξ + g3s1 + g2s2 + gs3 + s4 = Gξ + S
where
G = g4
138
CHAPTER 5. LADDER FILTER
S = g3s1 + g2s2 + gs3 + s4
Using the obtained instantaneous response Gξ + S of the series of 4 1-poles, we
can redraw the ladder ﬁlter structure as in Fig. 5.5.
x[n]
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
u[n]
Gξ + S
•(cid:47)
y[n]
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 5.5: TPT 4-pole ladder ﬁlter in the instantaneous response
form.
Rather than solving for y, let’s solve for the signal u at the feedback point.
From Fig. 5.5 we obtain
u = x − ky = x − k(Gu + S)
from where
u =
x − kS
1 + kG
(5.8)
We can then use the obtained value of u to process the 1-pole lowpasses one
after the other, updating their state, and computing y[n] as the output of the
fourth lowpass.
Apparently the total instantaneous gain of the zero-delay feedback loop in
Fig. 5.5 and in (5.8) is −kG. As we should recall from the discussion of 1-pole
lowpass ﬁlters, 0 < g < 1 for positive cutoﬀ settings. Respectively 0 < G < 1
and the ﬁlter doesn’t become instantaneously unstable provided k ≥ −1.
5.4 Feedback shaping
We have observed that at high resonance settings the amplitude gain of the ﬁlter
at low frequencies drops (Fig. 5.3). An obvious way to ﬁx this problem would be
e.g. to boost the input signal by the (1+k) factor.5 However there’s another way
to address the same issue. We could “kill” the feedback for the low frequencies
only by introducing a highpass ﬁlter into the feedback path (Fig. 5.6). In the
simplest case this could be a 1-pole highpass.
The cutoﬀ of the highpass ﬁlter can be static or vary along with the cutoﬀ
of the lowpasses. The static version has a nice feature that it kills the resonance
eﬀect at low frequencies regardless of the master cutoﬀ setting, which may be
desirable if the resonance at low frequencies is considered rather unpleasant
(Fig. 5.7).
In principle one can also use other ﬁlter types in the feedback shaping. One
has to be careful though, since this changes the total phase and amplitude re-
sponses of the feedback path, thus the frequency of the resonance peak and the
5We boost the input rather than the output signal for the same reason as when preferring
to place the cutoff gains in front of the integrators.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
5.4. FEEDBACK SHAPING
139
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
LP1
LP1
LP1
•
y(t)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
HP
Figure 5.6: Transistor ladder ﬁlter with a highpass in the feedback.
|H(jω)|, dB
0
-6
-12
-18
ωHP/8
ωHP
8ωHP
ω
Figure 5.7: Amplitude response of the ladder ﬁlter with a static-
cutoﬀ highpass in the feedback for various lowpass cutoﬀs.
value of k at which selfoscillation is reached may be changed. E.g., quite coun-
terintuitively, inserting a 1-pole lowpass into the feedback path can destabilize
an otherwise stable ﬁlter.
In order to establish and analyse the latter fact mathematically, we’d need
to ﬁnd the total amplitude response across the feedback loop at the point where
the total phase shift is 180◦. Let H1(s) = 1/(1 + s) be the underlying 1-pole
lowpass of the ladder ﬁlter and let Hf (s) = 1/(1 + s/ωcf ) be the lowpass in
the feedback, with a generally speaking diﬀerent cutoﬀ ωcf . The 180◦ point is
found from the equation
4 arg H1(jω) + arg Hf (jω) = 4 arg
= −4 arctan ω − arctan
ω
ωcf
= −π
1
1 + jω
+ arg
1
1 + jω/ωcf
=
(5.9)
where we have used (2.8). The equation (5.9) looks a bit daunting, if having
an analytic solution at all. Fortunately, we don’t actually need to know the
frequency of the 180◦ point, it would suﬃce to know the respective amplitude
responses.
Let ϕ1(ω) be the negated phase response of H1(s):
ϕ1(ω) = − arg H1(jω) = arctan ω > 0
∀ω
Expressing ω as a function of ϕ1 we have ω = tan ϕ1. Respectively, expressing
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
140
CHAPTER 5. LADDER FILTER
the amplitude response as a function of the (negated) phase response we have
A1 = |H1(jω)| =
√
1
1 + ω2
=
1
(cid:112)1 + tan2 ϕ2
1
= cos ϕ1
(5.10)
Thus, the total amplitude response of the four 1-poles in the feedforward path
of the ladder ﬁlter is
A4
1(ω) =
1
(1 + ϕ2
1)2
and the total phase response of the feedforward path is 4ϕ1.
Since (5.10) is cutoﬀ-independent, it also holds for Hf (s):
Af = cos ϕf
where Af = |Hf (jω)|, ϕf = − arg Hf (jω). Now let ω0 be the (unknown to us)
solution of (5.9), that is the total phase shift at ω0 is 180◦. In terms of the just
introduced functions ϕ1(ω) and ϕf (ω) equation (5.9) can be rewritten as
4ϕ1(ω0) + ϕf (ω0) = π
(5.11)
Since ϕf (ω) > 0 ∀ω, the 180◦ phase shift is achieved earlier than without the
feedback ﬁlter, that is ω0 < 1 (whatever the value of ωcf is).
Computing the total amplitude response of all ﬁve 1-pole lowpasses at ω0
we have
A4
1(ω0) · Af (ω0) = cos4 ϕ1(ω0) · cos ϕf (ω0) = cos4
(cid:18) π
4
−
ϕf (ω0)
4
(cid:19)
· cos ϕf (ω0)
Considering only the ﬁrst factor we have
cos4 (cid:16) π
4
−
(cid:17)
ϕf
4
1 + cos

=

(cid:16) π
2
2
−
ϕf
2
(cid:17)
2



=

1 + sin
2
ϕf
2
2


(where we dropped the argument ω0, understanding it implicity). Respectively
A4
1 · Af =
(cid:16)
·
1
4
1 + sin
(cid:17)2
ϕf
2
· cos ϕf
(ω = ω0)
(5.12)
Fig. 5.8 contains the graph of (5.12). The interpretation of this graph is like
follows. Suppose the feedback lowpass’s cutoﬀ ωcf is very large (ωcf → +∞).
In the limit the feedback lowpass has no eﬀct and
ω0 = 1 ϕf (ω0) = 0 Af (ω0) = 1 A4
1(ω0)Af (ω0) =
1
4
(for ωcf = +∞)
As we begin to lower ωcf back from the inﬁnity, the value of ϕf (ω0) grows
from zero into the positive value range. The graph in Fig. 5.8 plots the total
amplitude response of the ﬁve 1-pole lowpasses in the feedback loop against the
growing ϕf (ω0). We see that the amplitude response grows for quite a while.
As long as it is above 1/4, the ﬁlter will explode at k = 4. The zero amplitude
response at ϕf = π/2 corresponds to ωcf = 0, where the extra lowpass is fully
closed, thus the entire feedback loop is muted.
5.5. MULTIMODE LADDER FILTER
141
A4
1Af
1
4
0
π/5
π/2
ϕf
Figure 5.8: Total amplitude response of the four feedforward low-
pass 1-poles plus the feedback lowpass 1-pole at the 180◦ phase
shift point, plotted against the phase shift by the feedback 1-pole.
At ωcf = 1 (equal cutoﬀs of all 1-poles) from (5.11) we have ϕf (ω0) =
ϕ1(ω0) = π/5. In Fig. 5.8 one can see that this is the “most unstable” situation
among all possible ωcf .
In comparison, if we had a 1-pole highpass in the feedback, then we would
have arg Hf (jω) > 0 and respectively ϕf (ω) < 0 ∀ω. Therefore the 180◦ point
1(ω0) < A4
would be shifted to the right: ω0 > 1. Therefore A4
1(1) < 1/4, while
Af (ω) < 1 ∀ω, thus the total amplitude response A4
1Af at the 180◦ point would
decrease and the ﬁlter won’t become “more unstable” than it was before the
introduction of the extra highpass ﬁlter.
5.5 Multimode ladder filter
Warning! The multimode functionality of the ladder filter is a somewhat special
feature. There are more straightforward ways to build bandpass and highpass
ladders, discussed later in this chapter.
By picking up intermediate signals of the ladder ﬁlter as in Fig. 5.9 we obtain
the multimode version of this ﬁlter. We then can use linear combinations of
signals yn to produce various kinds of ﬁltered signal.6
Suppose k = 0. Apparently, in this case, the respective transfer functions
6Actually, instead of y0 we could have used the input signal x for these linear combinations.
However, it doesn’t matter. Since y0 = x − ky4, we can express x via y0 or vice versa. It’s just
that some useful linear combinations have simpler (independent of k) coefficients if y0 rather
than x is being used.
142
x
CHAPTER 5. LADDER FILTER
y1
•(cid:47)
LP1
LP1
y0
•(cid:47)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
y3
•(cid:47)
LP1
y2
•(cid:47)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 5.9: Multimode ladder ﬁlter.
LP1
•
y4
associated with each of the yn outputs are
Hn(s) =
1
(1 + s)n
(n = 0, . . . , 4)
(5.13)
If k (cid:54)= 0 then from
H4(s) =
1
k + (1 + s)4
using the obvious relationship Hn+1(s) = Hn(s)/(s + 1) we obtain
Hn(s) =
(1 + s)4−n
k + (1 + s)4
(5.14)
4-pole highpass mode
Considering that the 4th order lowpass transfer function (under the assumption
k = 0) is built as a product of four 1st order lowpass transfer functions 1/(1 + s)
HLP(s) =
1
(1 + s)4
we might decide to build the 4th order highpass transfer function as a product
of four 1st order highpass transfer functions s/(1 + s):
HHP(s) =
s4
(1 + s)4
Let’s attempt to build HHP(s) as a linear combination of Hn(s). Apparently,
a linear combination of Hn(s) must have the denominator k + (1 + s)4, so let’s
instead construct
HHP(s) =
s4
k + (1 + s)4
(5.15)
which at k = 0 will turn into s4/(1 + s)4. We also have HHP(∞) = 1 while the
four zeros at s = 0 provide a 24dB/oct rolloﬀ at ω → 0, thus we are still having
a more or less reasonable highpass. In order to express HHP(s) as a sum of the
modes we write
s4
k + (1 + s)4 =
a0(1 + s)4 + a1(1 + s)3 + a2(1 + s)2 + a3(1 + s) + a4
k + (1 + s)4
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
5.5. MULTIMODE LADDER FILTER
143
that is
s4 = a0(1 + s)4 + a1(1 + s)3 + a2(1 + s)2 + a3(1 + s) + a4
We need to ﬁnd an from the above equation, which generally can be done by
equating the coeﬃcients at equal powers of s in the left- and right-hand sides.
However, for the speciﬁc equation that we’re having here we could do a shortcut
by simply formally replacing s + 1 by s (and respectively s by s − 1):
(s − 1)4 = a0s4 + a1s3 + a2s2 + a3s + a4
from where immediately
a0 = 1, a1 = −4, a2 = 6, a3 = −4, a4 = 1
The amplitude response corresponding to (5.15) is plotted in Fig. 5.10.
|H(jω)|, dB
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 5.10: Amplitude response of the highpass mode of the lad-
der ﬁlter for various k.
4-pole bandpass mode
A bandpass ﬁlter can be built as
HBP(s) =
s2
k + (1 + s)4
(5.16)
The two zeros at s = 0 will provide for a −12dB/oct rolloﬀ at low frequencies and
will reduce the −24dB/oct rolloﬀ at high frequencies to the same −12dB/oct.
Notice that the phase response at the cutoﬀ is zero:
HBP(j) =
−1
k + (1 + j)4 =
1
4 − k
The coeﬃcients are found from
s2 = a0(1 + s)4 + a1(1 + s)3 + a2(1 + s)2 + a3(1 + s) + a4
(s − 1)2 = a0s4 + a1s3 + a2s2 + a3s + a4
The amplitude response corresponding to (5.16) is plotted in Fig. 5.11.
CHAPTER 5. LADDER FILTER
144
|H(jω)|, dB
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 5.11: Amplitude response of the bandpass mode of the lad-
der ﬁlter for various k.
Lower-order modes
Recalling the transfer functions of the modal outputs yn in the absence of the
resonance (5.13), we can consider the modal signals yn and their respective
transfer functions (5.14) as a kind of “n-pole lowpass ﬁlters with 4-pole reso-
nance”.
“Lower-order” highpasses can be build by considering the zero-resonance
transfer functions
HHP(s) =
sN
(s + 1)N =
(s + 1)4−N sN
(s + 1)4
which for k (cid:54)= 0 turn into
HHP(s) =
(s + 1)4−N sN
k + (s + 1)4
In a similar way we can build a “2-pole” bandpass
HBP(s) =
HBP(s) =
(s + 1)2s
(s + 1)4
s
(s + 1)2 =
(s + 1)2s
k + (s + 1)4
(k = 0)
(k (cid:54)= 0)
Other modes
Continuing in the same fashion we can build further modes (the transfer func-
tions are given for k = 0):
s
(s + 1)3
s2
(s + 1)3
3-pole bandpass, 6/12 dB/oct
3-pole bandpass, 12/6 dB/oct
5.6. HP LADDER
(s + 1)4 + Ks2
(s + 1)4
s4 − 1
(s + 1)4
(s2 + 1)2
(s + 1)4
(s2 + 2Rs + 1)2 + (s2 − 2Rs + 1)2
2(s + 1)4
s2 + 1
(s + 1)4
(1 + 1/s2)s4
(s + 1)4
(s + 1/s)s2
(s + 1)4
145
band-shelving
notch
notch
2 notches, neutral setting R = 1
2-pole lowpass + notch
2-pole highpass + notch
2-pole bandpass + notch
etc. The principles are more or less similar. We are trying to attain a desired
asymptotic behavior at ω → 0 and ω → +∞ by having the necessary orders and
coeﬃcients of the lowest-order and highest-order terms in the numerator. E.g.
by having s2 as the lowest-order term of the numerator we ensure a 12dB/oct
rolloﬀ at ω → 0, or by having s4 as the highest-order term we ensure H(∞) = 1.
The notch at ω = 1 is generated by placing a zero at s = ±j. The 2-notch version
is obtained by explicitly writing out the transfer function of a 4-pole multinotch
described in Section 11.3.
5.6 HP ladder
Performing an LP to HP transformation on the lowpass ladder ﬁlter we ef-
fectively perform it on each of the underlying 1-pole lowpasses, thus turning
them into 1-pole highpasses. Thereby we obtain a “true” highpass ladder ﬁl-
ter (Fig. 5.12). Obviously, the amplitude response of the ladder highpass is
symmetric to the amplitude response of the ladder lowpass (Fig. 5.13).
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
HP1
HP1
HP1
HP1
•(cid:47)
y(t)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 5.12: A “true” highpass ladder ﬁlter.
The instantaneous gain of a 1-pole highpass is complementary to the instan-
taneous gain of the 1-pole lowpass:
1 −
g
1 + g
=
1
1 + g
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
CHAPTER 5. LADDER FILTER
146
|H(jω)|, dB
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 5.13: Amplitude response of the 4-pole highpass ladder ﬁlter
for various k.
where g = ωcT /2. Thus the instantaneous gain of a single 1-pole highpass
is varying within the range (0,1) and so does the gain of the chain of four
highpasses: 0 < G < 1. Therefore, 4-pole highpass ladder doesn’t get instanta-
neously unstable for k > −1.
5.7 BP ladder
In order to build a “true” 4-pole bandpass ladder, we replace only half of the
lowpasses with highpasses (it doesn’t matter which two of the four 1-pole low-
passes are replaced). The total transfer function of the feedforward path is
thereby
s
(1 + s)2 ·
where each of the s/(1 + s)2 factors is built from a serial combination of a 1-pole
lowpass and a 1-pole highpass:
s
(1 + s)2
s2
(1 + s)4 =
s
(1 + s)2 =
s
1 + s
·
1
1 + s
Apparently s/(1 + s)2 = s/(1 + 2s + s2) is a 2-pole bandpass with damping
R = 1 and a serial combination of two of them makes a 4-pole bandpass. The
frequency response of s/(1 + s)2 at ω = 1 is 1/2, that is there is no phase-shift.
Respectively the frequency response of s2/(1 + s)4 at ω = 1 is 1/4, also without
a phase shift. Therefore we need to use positive rather than negative feedback
(Fig. 5.14), the selfoscillation still occuring at k = 4, the same as with lowpass
and highpass ladders.
Noticing that the ﬁlter structure is invariant relative to the LP to HP trans-
formation, we conclude that its amplitude response must be symmetric (around
ω = 1) in the logarithmic frequency scale (Fig. 5.15).
The question of instantaneous instability is more critical for the bandpass
ladder, since the feedback is positive. The instantaneous gain of a lowpass-
highpass pair is a product of the instantaneous gains of a 1-pole lowpass and a
5.7. BP LADDER
147
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
HP1
LP1
HP1
•(cid:47)
y(t)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 5.14: A “true” bandpass ladder ﬁlter.
|H(jω)|, dB
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 5.15: Amplitude response of the 4-pole bandpass ladder
ﬁlter for various k.
1-pole highpass:
g
1 + g
·
1
1 + g
(where g = ωcT /2). It’s not diﬃcult to verify that the maximum gain of this
pair is attained at g = 1 and is equal to 1/4. The maximum instantaneous gain
of two of these pairs is therefore 1/16, and thus the instantaneously unstable
case doesn’t occur provided k < 16.
Bandwidth control
Using (5.3) and the fact that the frequency response of s2/(1 + s)4 at ω = 1
is 1/4 we obtain the frequency response of the 4-pole bandpass ladder at the
cutoﬀ
H(j) =
1
4 − k
Therefore, by multiplying the output (or the input signal) of the 4-pole bandpass
ladder by 4 − k we can turn it into a normalized bandpass, where the bandwidth
is controlled by varying k.
There is another way, however. Recall that the normalized 2-pole bandpass
(4.15) is an LP to BP transformation of the 1-pole lowpass 1/(1 + s). At the
same time,
1
1 + s
·
s
1 + s
=
s
(1 + s)2 =
s
1 + 2s + s2 =
1
2
·
2s
1 + 2s + s2
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
148
CHAPTER 5. LADDER FILTER
is simply a halved version of (4.15) taken at R = 1 and therefore is an LP to BP
transformation fo the halved 1-pole lowpass 1/2(1+s). This means that Fig. 5.14
can be replaced by Fig. 5.16 which in turn is an LP to BP transformation of
Fig. 5.17.
x(t)
BPn
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
BPn
y(t)
Figure 5.16: 4-pole bandpass ladder ﬁlter expressed in terms of
normalized 2-pole bandpasses.
x(t)
LP1
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
LP1
y(t)
Figure 5.17: LP to BP transformation applied to this structure
produces the 4-pole bandpass ladder in Fig. 5.16.
We don’t even speciﬁcally care to analyse the structure Fig. 5.17. What is
important is that the damping parameter of the LP to BP transformation con-
trols the transformation bandwidth and thereby the bandwidth of the bandpass
ladder in Fig. 5.16. Thus, introducing the damping control into the normalized
2-pole bandpasses in Fig. 5.16 we can control the bandpass ladder’s bandwidth
by simply varying the damping parameter of the underlying 2-pole bandpasses.
At the same time we still have the k parameter available, which we still can
use to control the bandwidth of the normalized bandpass (Fig. 5.18). Thus, k
and R provide two diﬀerent ways of bandwidth control, resulting in somewhat
diﬀerent amplitude response shapes (Fig. 5.19).7
Obviously, normalized 2-pole bandpasses with damping control could be im-
plemented using an SVF. If nonlinearities are involved, however, using TSK/SKF
2-pole bandpasses might be a better option. Since we didn’t introduce the latter
yet, we need to postpone the respective discussion. We will return to this ques-
tion, however, in the discussion of 8-pole bandpass ladder in Section 5.9, where
the bandwidth control via the 2-pole bandpass damping will be a particularly
desired feature compared to being somewhat academic in the case of a 4-pole
bandpass.
7In principle, k and R have very similar effects. Fundamentally, they both affect the band-
width and the resonance peak height. In Fig. 5.18 their effect on the resonance peak height is
compensated, the compensation for k being the 4 − k gain at the output, the compensation
for R being embedded into the normalized bandpasses. By removing the normalization from
the bandpasses we effectively introduce the 1/R2 gain into the feedback, and the damping R
thereby will control the resonance peak height too.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
5.8. SALLEN–KEY FILTERS
149
x(t)
BPn
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
4 − k
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
BPn
y(t)
Figure 5.18: 4-pole normalized bandpass ladder ﬁlter expressed in
terms of normalized 2-pole bandpasses.
|H(jω)|, dB
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 5.19: Amplitude response of the 4-pole normalized bandpass
ladder ﬁlter in Fig. 5.18 for two diﬀerent combinations of k and R
resulting in comparable bandwidths.
5.8 Sallen–Key filters
In this section we are going to introduce two special kinds of 2-pole bandpass
ladder ﬁlters, the Sallen–Key ﬁlter and its transpose.8 They are important
because of their nonlinear versions, since, as linear digital 2-pole ﬁlters go, the
SVF ﬁlter could be suﬃcient for most applications, and it also provides probably
the best performance among diﬀerent TPT 2-poles.
For now we shall develop the linear versions of these ﬁlters. The Sallen–Key
ﬁlter is more famous than its transpose, but we’ll start with the transpose, for
the sake of a more systematic presentation of the material.
Transposed Sallen–Key (TSK) filters
Attempting to build a 2-pole lowpass ladder ﬁlter (Fig. 5.20) we don’t end up
with a useful ﬁlter.
8Despite essentially being bandpass ladder filters, the Sallen–Key filter and its transpose
can be (and are) used to deliver lowpass and highpass responses as well.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
150
CHAPTER 5. LADDER FILTER
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
LP1
•(cid:47)
y(t)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 5.20: 2-pole lowpass ladder ﬁlter (not very useful).
Indeed, the transfer function of this ﬁlter is
H(s) =
1
k + (1 + s)2
and the poles are respectively at
√
s = −1 ±
−k = −1 ± j
√
k
(k ≥ 0)
Interpreting these pole positions in terms of 2-pole cutoﬀ and damping (which
we can do using (4.13)), we obtain
ωc =
R =



√
√
k
(cid:12)
(cid:12)
(cid:12) =
(cid:12)
(cid:12)
(cid:12)−1 ± j
(cid:16)
−1 ± j
√
− Re
(cid:12)
(cid:12)
(cid:12)−1 ± j
k
(cid:12)
(cid:12)
(cid:12)
1 + k
(cid:17)
√
k
=
√
1
1 + k
Thus, ﬁrstly, there is coupling between the feedback amount and the eﬀective
cutoﬀ of the ﬁlter. Secondly, as k grows, R stays strictly positive, thus the ﬁlter
poles never go into the right semiplane (and, as with the 4-pole ladder ﬁlter,
this would be quite desired once we make the ﬁlter nonlinear). So, all in all, not
a very useful structure.
A similar situation occurs in an attempt to use two 1-pole highpasses instead
of two 1-pole lowpass in the same structure (the readers may wish verify this
on their own as an exercise).
This result is no wonder, considering that the transfer function of a chain
of two 1-pole lowpasses is 1/(1 + s)2, with the phase response being 0◦ only
at ω = 0 and being 180◦ only at ω = ∞ (for the highpasses the situation is
opposite, we have 180◦ only at ω = 0 and 0◦ only at ω = ∞, which doesn’t
make a big diﬀerence for our purposes). Thus we don’t get a good resonance
peak at any ﬁnite location. This however hints at the idea that we might still
try to build a 2-pole bandpass ladder ﬁlter from a chain of a 1-pole lowpass and
a 1-pole highpass, as the total phase shift at the cutoﬀ would be 0◦ in this case:
(cid:18) 1
1 + s
·
s
1 + s
(cid:19) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)s=j
=
s
(1 + s)2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)s=j
=
j
(1 + j)2 =
1
2
The respective structure is shown in Fig. 5.21. Notice that we don’t invert the
feedback.
Computing the transfer function of this ﬁlter we have
H(s) =
s
(1 + s)2
s
(1 + s)2
1 − k
=
s
(1 + s)2 − ks
=
s
s2 + (2 − k)s + 1
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
5.8. SALLEN–KEY FILTERS
151
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
HP1
•(cid:47)
y(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)
k
Figure 5.21: 2-pole bandpass ladder ﬁlter.
The obtained expression is identical to the transfer function of a 2-pole bandpass
ﬁlter with a damping gain 2R = 2 − k. That is, the ﬁlter in Fig. 5.21 is pretty
much the same as a linear 2-pole SVF bandpass, at least from the frequency
response perspective. Notice that k = 0 corresponds to the resonance-neutral
setting (R = 1) while k = 2 is the self-oscillation point (R = 0). As we should
remember from the 4-pole bandpass ladder discussion, the maximum possible
instantaneous gain of the lowpass-highpass pair is 1/4, therefore under the condi-
tion k < 4 the TPT implementation of Fig. 5.21 doesn’t become instantaneously
unstable.
It might seem that we have failed to construct a 2-pole lowpass ﬁlter using
the above approach, but in fact with a slight modiﬁcation we can obtain one
from the bandpass ﬁlter in Fig. 5.21. Let’s replace the 1-pole highpass with a
1-pole multimode with highpass and lowpass outputs (Fig. 5.22).
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
MM1
LP
HP
•(cid:47)
y1(t)
y(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)
k
Figure 5.22: 2-pole bandpass ladder ﬁlter with an extra output
mode.
Obviously, the signal y(t) is not aﬀected by this replacement. Let’s ﬁnd out
what kind of signal is y1(t). In order to simplify the computation of the transfer
function of the entire structure at y1, consider ﬁrst the transfer functions of the
1-pole multimode ﬁlter used in isolation:
HLP(s) =
1
1 + s
HHP(s) =
s
1 + s
or, for complex sinusoidal signals of the form est
YLP(s) =
1
1 + s
X(s)
YHP(s) =
s
1 + s
X(s)
where X(s)est is the input signal of the multimode 1-pole and YLP(s)est and
YHP(s)est are the respective output signals. This means that
YLP(s) =
YHP(s)
s
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
152
CHAPTER 5. LADDER FILTER
Therefore a similar relationship exists between the outputs y1(t) and y(t) of the
ﬁlter in Fig. 5.22:
Y1(s) =
Y (s)
s
and there is the same relationship between their respective transfer functions
H1(s) =
H(s)
s
=
1
s
·
s
s2 + (2 − k)s + 1
=
1
s2 + (2 − k)s + 1
where H1(s) the the transfer function for the signal y1(t) in respect to the input
signal x(t). Therefore y1(t) is an ordinary 2-pole lowpass signal with damping
gain 2R = 2 − k.
Thus we have obtained a multimode 2-pole ladder ﬁlter with the lowpass and
bandpass outputs. We redraw the structure in Fig. 5.22 once again as Fig. 5.23
to reﬂect what we have just found out about this structure.
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
MM1
LP
HP
•(cid:47)
yLP(t)
yBP(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)
k
Figure 5.23: Transposed Sallen–Key (TSK) ﬁlter.
The structure in Fig. 5.23 happens to be a transpose of the Sallen–Key
ﬁlter, therefore we will refer to it as the transposed Sallen–Key (TSK) ﬁlter.9
The transfer functions of the TSK ﬁlter are, as we have found out:
HLP(s) =
HBP(s) =
1
s2 + (2 − k)s + 1
s
s2 + (2 − k)s + 1
A 2-pole highpass output mode cannot be picked up in a straightforward way,
but can be obtained with some extra eﬀort. Let’s also turn the ﬁrst lowpass into
a multimode (Fig. 5.24). It is not diﬃcult to realize that the transfer function
for the signal at the LP output of MM1a, which is simultaneously the input
signal of MM1b, is
HMM1aLP(s) = HLP(s) ·
(cid:18) 1
(cid:19)−1
s + 1
=
s + 1
s2 + (2 − k)s + 1
respectively for the signal at the HP output of MM1a we have
HMM1aHP(s) = s · HMM1aLP(s) =
(s + 1)s
s2 + (2 − k)s + 1
9The author has used the works of Tim Stinchcombe as the information source on the
Sallen–Key filter. The idea to introduce TSK filters as a systematic concept arose from
discussions with Dr. Julian Parker.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
5.8. SALLEN–KEY FILTERS
153
Thus we obtain
HHP(s) = HMM1aHP(s) − HBP(s) =
(s + 1)s
s2 + (2 − k)s + 1
−
s
s2 + (2 − k)s + 1
=
=
s2
s2 + (2 − k)s + 1
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
LP
HP
MM1b
yHP(t)
yLP(t)
yBP(t)
x(t)
MM1a
HP
LP
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)
k
Figure 5.24: Fully multimode TSK ﬁlter.
Alternative representations
Recall that 1-pole highpass signal can be obtained as the diﬀerence of the 1-pole
lowpass ﬁlter’s input and output signals:
s
1 + s
= 1 −
1
1 + s
Then we can replace the multimode 1-pole in Fig. 5.23 by a 1-pole lowpass,
constructing the highpass signal “manually” by subtracting the lowpass output
from the lowpass input (Fig. 5.25). A further modiﬁcation of Fig. 5.25 is formally
using negative feedback (Fig. 5.26)
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
•(cid:47)
LP1
•(cid:47)
yLP(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
•(cid:111)
(cid:15) −(cid:111)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
yBP(t)
Figure 5.25: TSK ﬁlter (alternative representation).
Highpass TSK filter
Let’s take the ﬁlter in Fig. 5.21 and switch the order of lowpass and highpass
1-pole ﬁlters (Fig. 5.27). Since this doesn’t change the transfer function of
the entire chain of 1-poles, the ﬁlter output stays the same, it is still a 2-pole
bandpass.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
154
CHAPTER 5. LADDER FILTER
LP1
•(cid:47)
yLP(t)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
•(cid:47)
− (cid:15)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 5.26: TSK ﬁlter (alternative representation, negative feed-
back form).
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
HP1
LP1
•(cid:47)
y(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)
k
Figure 5.27: 2-pole bandpass ladder ﬁlter with a diﬀerent order of
1-pole lowpass and highpass ﬁlters.
Turning the 1-pole lowpass into a multimode we obtain the structure in
Fig. 5.28.
It’s not diﬃcult to see that the signal at the other output of the
multimode is a 2-pole highpass one. Therefore, in order to distinguish between
the ﬁlters in Figs. 5.23 and 5.28 we will refer to the former more speciﬁcally as
a lowpass TSK filter and to the latter as a highpass TSK filter. If necessary, we
can add the lowpass output, using a way similar to Fig. 5.24.
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
HP1
MM1
HP
LP
•(cid:47)
yHP(t)
yBP(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)
k
Figure 5.28: Highpass TSK ﬁlter.
The highpass versions of Fig. 5.25 and Fig. 5.26 could have been built by
performing transformations of Fig. 5.28 similarly to how we did with Fig. 5.23.
However it’s easier just to apply the LP to HP substitution (s ← 1/s) to
Figs. 5.25 and 5.26.
Sallen–Key filter (SKF)
We could take the structure in Fig. 5.27 and convert the 1-pole highpass ﬁlter
into a tranposed multimode 1-pole (Fig. 5.29). By doing this one obtains a
transpose of Fig. 5.23 which is (apparently) called Sallen–Key filter or shortly
SKF. If necessary, the highpass input can be added, turning Fig. 5.23 into a
transpose of Fig. 5.24.
If instead we take the structure in Fig. 5.21 and convert the lowpass into a
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
5.8. SALLEN–KEY FILTERS
155
xLP(t)
xBP(t)
(cid:47)LP
HP
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
MM1
LP1
•(cid:47)
y(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
Figure 5.29: Sallen–Key ﬁlter.
transposed multimode 1-pole, we can obtain the structure in Fig. 5.30. In order
to distinguish between Fig. 5.29 and Fig. 5.30, we will, as we did with their
transposes, refer to the structure in Fig. 5.29 more speciﬁcally as a lowpass
Sallen–Key filter and to the structure in Fig. 5.30 as a highpass Sallen–Key
filter. The lowpass input can be added to the highpass SKF using the transposed
version of the idea of Fig. 5.24.
xHP(t)
xBP(t)
(cid:47)HP
LP
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
MM1
HP1
•(cid:47)
y(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
Figure 5.30: Highpass SKF.
The transposed versions of Fig. 5.25 and Fig. 5.26 make alternative repre-
sentations of the lowpass SKF. E.g. by transposing the structure in Fig. 5.25 we
obtain the one in Fig. 5.31.
LP1
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
xLP(t)
xBP(t)
LP1
•(cid:47)
y(t)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 5.31: Sallen–Key ﬁlter (alternative representation).
MIMO Sallen–Key filters
By turning both 1-poles in Fig. 5.27 into multimodes we’ll obtain a MIMO
(multiple input multiple output) Sallen–Key ﬁlter, as illustrated in Fig. 5.32.
Note that the labelling of the inputs and outputs xLP, xHP, yLP, yHP is
thereby formal. The actual transfer functions are deﬁned for signal paths from
a given input to a given output:
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
156
CHAPTER 5. LADDER FILTER
xLP(t)
xHP(t)
(cid:47)LP
HP
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
MM1a
MM1b
HP
LP
•(cid:47)
yHP(t)
yLP(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
Figure 5.32: MIMO Sallen–Key ﬁlter (HP-LP).
yLP
2-pole lowpass
2-pole bandpass
yHP
2-pole bandpass
2-pole highpass
xLP
xHP
By putting the feedback path around lowpass-highpass chain rather than
lowpass-highpass, Fig. 5.32 is turned into Fig. 5.33.
xHP(t)
xLP(t)
(cid:47)HP
LP
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
MM1a
MM1b
LP
HP
•(cid:47)
yLP(t)
yHP(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
Figure 5.33: MIMO Sallen–Key ﬁlter (LP-HP).
Allpass TSK/SKF
Consider again the 2-pole bandpass ladder ﬁlter structure in Fig. 5.21. Suppose
that we use 1-pole allpasses (1 − s)/(1 + s) instead of low- and highpass ﬁlters.
We also use negative, rather than positive feedback, although this is more a
matter of convention. The result is shown in Fig. 5.34, where we also prepared
the modal outputs.
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
y0(t)
y1(t)
•(cid:47)
AP1
•(cid:47)
AP1
•(cid:47)
y2(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)
k
Figure 5.34: 2-pole ladder ﬁlter based on allpasses (not so useful).
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
5.8. SALLEN–KEY FILTERS
157
The transfer function of the main output is
H2(s) =
(cid:17)2
(cid:16) 1−s
1+s
(cid:16) 1−s
1+s
1 + k
(cid:17)2 =
(1 − s)2
(1 + s)2 + k(1 − s)2 =
=
(1 − s)2
(1 + k)s2 + 2(1 − k)s + (1 + k)
=
1
1 + k
·
(1 − s)2
1 − k
1 + k
s2 + 2
s + 1
which is not exactly a 2-pole allpass transfer function. The denominator of H(s)
however looks pretty usable, it’s a classical 2-pole transfer function denominator
with damping R = (1 − k)/(1 + k).
The transfer functions at the other two outputs can be obtained by “reverse
application” of the transfer functions of the 1-pole allpasses to H2(s):
H1(s) =
(cid:19)−1
(cid:18) 1 − s
1 + s
· H2(s) =
H0(s) =
(cid:19)−1
(cid:18) 1 − s
1 + s
· H1(s) =
1
1 + k
1
1 + k
·
·
(1 + s)(1 − s)
1 − k
1 + k
s2 + 2
s + 1
(1 + s)2
1 − k
1 + k
s2 + 2
s + 1
We can try building the desired transfer function
H(s) =
s2 − 2Rs + 1
s2 + 2Rs + 1
=
s2 − 2
s2 + 2
1 − k
1 + k
1 − k
1 + k
s + 1
s + 1
as a linear combination of H0(s), H1(s) and H2(s):
a0H0(s) + a1H1(s) + a2H2(s) = H(s)
Noticing that the denominators of H0(s), H1(s), H2(s) are all identical to the
desired denominator already, we can discard the common denominator from the
equation and simply write:
a0
(1 + s)2
1 + k
+ a1
(1 + s)(1 − s)
1 + k
+ a2
(1 − s)2
1 + k
= s2 − 2
1 − k
1 + k
s + 1
or
a0(1 + 2s + s2) + a1(1 − s2) + a2(1 − 2s + s2) = (1 + k)s2 − 2(1 − k)s + (1 + k)
From where a0 = k, a1 = 0, a2 = 1. Thus
H(s) = H0(s) + kH2(s) =
s2 − 2
s2 + 2
1 − k
1 + k
1 − k
1 + k
s + 1
s + 1
and the corresponding structure is shown in Fig. 5.35.1011 The main idea of
this structure is very similar to the one of a TSK ﬁlter with some “embedded”
158
CHAPTER 5. LADDER FILTER
k
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
AP1
AP1
•(cid:47)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
y(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 5.35: Allpass TSK ﬁlter.
modal mixture. For that reason we can refer to the ﬁlter Fig. 5.35 as a allpass
TSK ﬁlter, or we could call it a 2-pole allpass ladder filter.
The 2-pole damping parameter R is related to k via
R = (1 − k)/(1 + k)
k = (1 − R)/(1 + R)
so that for k = −1 . . . + ∞ the damping varies from +∞ to −1. The stable
range R = +∞ . . . 0 corresponds to k = −1 . . . 1.
Transposing the structure in Fig. 5.35 we obtain the structure Fig. 5.36
which for obvious reasons we will refer to as an allpass SKF.
x(t)
•(cid:47)
k
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
AP1
AP1
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 5.36: Allpass SKF.
•(cid:47)
y(t)
5.9 8-pole ladder
Connecting eight 1-pole lowpass ﬁlters in series instead of four we can build an
8-pole lowpass ladder ﬁlter (Fig. 5.37).
The transfer function of the 8-pole lowpass ladder is obviously
H(s) =
1
k + (1 + s)8
10It is easy to notice that this structure is very similar to the one of a multinotch filter with
some specific dry/wet mixing ratio.
11The same structure can be obtained from a direct form II 1-pole allpass filter by the
allpass substitution z−1 ← (1 − s)2/(1 + s)2. It is also interesting to notice that, applying
the allpass substitution principle to the structure in Fig. 5.35, we can replace the series of the
two 1-pole allpass filters in Fig. 5.35 by any other allpass filter, and the modified structure
will still be an allpass filter.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
5.9. 8-POLE LADDER
159
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
8 × LP1
•(cid:47)
y(t)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 5.37: 8-pole lowpass ladder ﬁlter.
and the pole positions are deﬁned by
giving
k + (1 + s)8 = 0
s = −1 + (−k)1/8
where (−k)1/8 is understood in the multivalued complex root sense:
(−k)1/8 = |k|1/8ejα
where
α =
π + 2πn
8
The main diﬀerence from the 4-pole ladder lowpass, besides the steeper cutoﬀ
slope, is that the 180◦ phase shift by the chain of 1-pole lowpasses is no longer
occurring at the cutoﬀ.
Instead, the phase response of the lowpass chain at
the cutoﬀ is 360◦. In order to ﬁnd the frequency at which 180◦ phase shift is
occurring we need to solve
arg
(cid:18) 1
(cid:19)8
1 + jω
= −π
that is
arg(1 + jω) = π/8
or
arg(1 + jω) = 3π/8
(apparently the values 5π/8 an larger cannot be attained by arg(1 + jω)). This
gives
ω = tan π/8
or ω = tan 3π/8
The value of tan π/8 can be easily found using the formula for the tangent of
double angle:
tan 2α =
where letting α = π/8 we obtain
2 tan α
1 − tan2 α
2 tan π/8
1 − tan2 π/8
= 1
2 tan π/8 = 1 − tan2 π/8
tan2 π/8 + 2 tan π/8 − 1 = 0
√
ω = tan π/8 =
2 − 1 ≈ 0.4142
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
160
CHAPTER 5. LADDER FILTER
For tan 3π/8 we can use the formula for the tangent of the complementary angle:
ω = tan 3π/8 = tan(π/2 − π/8) =
1
tan π/8
=
√
1
2 − 1
√
=
2 + 1 ≈ 2.4142
2 ± 1. Let’s ﬁnd
Thus the resonance peak can occur at ω = tan(π/4 ± π/8) =
the values of k at which the respective poles hit the imaginary axis. According
to (5.7), k is the reciprocal of the amplitude amplitude response of the chain of
eight 1-pole lowpasses at the respective frequencies:
√
1
1 + jω
8(cid:33)−1
(cid:18)
√
=
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:18)(cid:113)
1
1 + ω2
(cid:19)8
(cid:19)−8
(cid:16)(cid:112)
1 + ω2
(cid:17)−8
=
=
1 + tan2(π/4 ± π/8)
= cos−8(π/4 ± π/8)
k =
=
ﬁnally giving
ω1 ≈ 0.4142
ω2 ≈ 2.4142
k1 ≈ 1.884
k2 ≈ 2174
Thus the selfoscillation at ω1 is occurring way much earlier than the one at ω2.
It is very unlikely that even in a nonlinear version of this ﬁlter, which allows
going into unstable range of k, we will use k as large as 2174.
It also hints
to the fact that the second resonance is way much weaker than the ﬁrst one.
Therefore, for practical purposes we will simply ignore the second resonance and
2 − 1 ≈ 0.4142 at k ≈ 1.884.
say that the inﬁnite resonance is occuring at ω =
Fig. 5.38 illustrates the amplitude response behavior for various k.
√
|H(jω)|, dB
+6
0
-6
-12
-18
ωc/8
√
(
2 − 1)ωc
ωc
8ωc
ω
Figure 5.38: Amplitude response of the 8-pole lowpass ladder ﬁlter
for various k.
Considering that at k = 0 the amplitude response of a chain of eight 1-
√
2)8 = 1/16, which is ca. −24dB, we could treat the
poles at the cutoﬀ is (1/
√
resonance frequency ω =
2 − 1 as the “user-facing” cutoﬀ frequency instead,
and in practical implementations of the ﬁlter let the cutoﬀ of the underlying 1-
2+1 ≈ 2.4142.
poles equal the “user-facing” cutoﬀ multiplied by 1/(
2−1) =
√
√
5.9. 8-POLE LADDER
161
One could ask the following question: the phase response of the chain of
eight 1-poles at ω = 1 is 0◦, therefore why don’t we simply use positive feed-
back to create the resonance peak at ω = 1? The problem is that the phase
response at ω = 0 is also 0◦. Since the amplitude response at ω = 0 is 1, the
selfoscillation will occur already at k = 1, whereas at ω = 1 it will occur only
at k = 1/(1/
2)8 = 16.
√
The instantaneously unstable range of k is found similarly to the 4-pole
lowpass ladder and is k < −1.
Various modal mixtures for the 8-pole lowpass ladder ﬁlter can be built in
a similar way to the 4-pole ladder ﬁlter. However the fact that the resonance
frequency is noticeably lower than the cutoﬀ frequency of the underlying 1-poles
will aﬀect the shapes of the resulting modal mixtures. Some smart playing
around with the modal mixture coeﬃcients can sometimes reduce the eﬀect of
this discrepancy.
8-pole highpass ladder
Replacing the 1-pole lowpasses with highpasses we obtain an 8-pole highpass
ladder ﬁlter (Fig. 5.39). As we already know from the discussion of the 4-pole
highpass, it essentially the same as lowpass except for the s ← 1/s substitution.
The instantaneously unstable range of k is found similarly to the 4-pole highpass
ladder and is k < −1.
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
8 × HP1
•(cid:47)
y(t)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 5.39: 8-pole highpass ladder ﬁlter.
8-pole bandpass ladder
Replacing half of the lowpasses with highpasses in Fig. 5.37 we obtain the 8-pole
bandpass ladder ﬁlter, where we shouldn’t forget that in a bandpass ladder the
feedback shouldn’t be inverted (Fig. 5.40).
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
4 × LP1HP1
•(cid:47)
y(t)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 5.40: 8-pole bandpass ladder ﬁlter.
The total gain at the cutoﬀ of the 1-pole chain is
(cid:18)
s
(1 + s)2
(cid:19)4 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)s=j
(cid:19)4
=
(cid:18) 1
2
=
1
16
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
162
CHAPTER 5. LADDER FILTER
therefore selfoscillation occurs at k = 16. Fig. 5.41 illustrates the amplitude
response behavior at various k. Note that the amplitude response is pretty low
(particularly, for k = 0 it peaks at −24dB), therefore additional boosting of the
output signal may be necessary in practical usage.
|H(jω)|, dB
-12
-18
-24
-30
-36
ωc/8
ωc
8ωc
ω
Figure 5.41: Amplitude response of the 8-pole bandpass ladder
ﬁlter for various k ≥ 0.
The instantaneously unstable range of k is found similarly to the 4-pole
bandpass ladder and is k ≥ 28 = 256.
An interesting feature of the 8-pole bandpass ladder is that at negative k
the ﬁlter obtains two resonance peaks (Fig. 5.42).12
Indeed, notice that the
phase response of the 8-pole lowpass-highpass chain is the same as the one of
the 8-pole lowpass chain:
arg
s4
(1 + s)8 = arg
1
(1 + s)8
s = jω, ω ∈ R
Thus we still have a 180◦ phase shift at ω =
√
2 ± 1.
The amplitude response of a single lowpass-highpass pair at ω =
√
2 ± 1 is


(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
s
(1 + s)2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)s=j(
√
2±1)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
√
=
1 + (
2 ± 1
√
2 ± 1)2
=
√
(
1
2 ∓ 1) + (
√
2 ± 1)
=
1
√
2
2
therefore selfoscillation occurs at k = −(2
√
2)4 = −64.
8-pole bandpass ladder with bandwidth control
The occurence of two resonance peaks in an 8-pole bandpass ladder at k < 0
motivates the introduction of the possibility to control the distance between
these two peaks. In Section 5.7 we have introduced two diﬀerent approaches
to control the 4-pole bandpass ladder’s bandwidth. Apparently, the approach
using the k parameter is not good for our goal here, since we don’t want to aﬀect
12In nonlinear versions of this filter this can generate a particularly complex sound, as the
two resonance peaks and the input signal fight for the saturation headroom.
5.9. 8-POLE LADDER
163
|H(jω)|, dB
-24
-30
-36
-42
-48
ωc/8
√
(
2 − 1)ωc
ωc
√
(
2 + 1)ωc
8ωc
ω
Figure 5.42: Amplitude response of the 8-pole bandpass ladder
ﬁlter for various k < 0.
the amplitude response shape in the vertical direction. Also, from Fig. 5.42
it seems that the variation of k in the negative range has little eﬀect on the
actual bandwidth. On the other hand, the approach using the damping of the
underlying 2-pole bandpasses looks much more promising.
Representing the 8-pole bandpass ladder in terms of normalized 2-pole band-
passes (Fig. 5.43) we notice that it is an LP to BP transformation of the ﬁlter
in Fig. 5.44. The ﬁlter in Fig. 5.44 is essentially the same as the ordinary 4-pole
lowpass ladder (Fig. 5.1), except that
- the feedback is positive, so that selfoscillation at ω = 1 occurs at some
negative value of k
- the output signal amplitude and the feedback amount are 16 times lower,
thus selfoscillation at ω = 1 doesn’t occur at k = −4 but at k = −64
(which matches the already established fact of selfoscillation of Fig. 5.40
and equivalently Fig. 5.43 at k = −64).
Therefore by controlling the bandwidth of the LP to BP transformation, we will
control the distance between the resonance peaks in Fig. 5.42.
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
×4 times
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
BPn
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
•(cid:47)
y(t)
Figure 5.43: 8-pole bandpass ladder ﬁlter expressed in terms of
normalized 2-pole bandpasses.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
164
CHAPTER 5. LADDER FILTER
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
×4 times
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
LP1
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
•(cid:47)
y(t)
Figure 5.44: 4-pole lowpass ladder ﬁlter with positive feedback and
additional gains of 1/2. The LP to BP substitution applied to this
ﬁlter produces the ﬁlter in Fig. 5.43.
Since the resonance peak in Fig. 5.44 is occurring at ω = 1, the formula
(4.20) expresses R in terms of the distance between the two images of this peak
after the LP to BP transformation. Therefore we can directly use the formula
(4.20) to control the distance between the resonance peaks in Fig. 5.43. The
prewarping techniques described in Section 4.6 also apply, thereby allowing us
to achieve the exact positioning of the resonance peaks (in the limit k → −64).
There is an important question concerning the choice of the speciﬁc topology
for the normalized bandpasses BPn. Of course, the most obvious choice would
be to use an SVF. This should work completely ﬁne in the linear case. In a
nonlinear case, however, we might want to use a diﬀerent topology. Particularly,
we might want that at R = 1 our controlled-bandwidth topology becomes fully
identical to Fig. 5.40 (therefore obtaining the sound, which is identical to the
one of the structure in Fig. 5.40 even in the presence of nonlinear eﬀects).
Assuming that Fig. 5.40 implies interleaved 1-pole low- and highpasses (as
shown in Fig. 5.45), a good solution is provided by the TSK/SKF ﬁlters. E.g.
considering the structure in Fig. 5.21 (which is essentially the TSK ﬁlter from
Fig. 5.23), we can notice that at k = 0 it becomes fully equivalent to a sin-
gle lowpass-highpass pair. This suggests that we could use this structure to
construct a halved normalized bandpass (Fig. 5.46), where expressing the TSK
feedback k in terms of damping R we have k = 2(1 − R). Note that at R = 1
not only the feedback path in Fig. 5.46 is disabled, but also the output gain
element R is becoming transparent. Using the halved normalized bandpass in
Fig. 5.46, we could reimplement Fig. 5.45 as Fig. 5.47.
5.10 Diode ladder
In the diode ladder ﬁlter the serial connection of four 1-pole lowpass ﬁlters (im-
plemented by the transistor ladder) is replaced by a more complicated structure
of 1-pole ﬁlters (implemented by the diode ladder). The block diagram of the
diode ladder is shown in Fig. 5.48, while the diode ladder ﬁlter adds the feed-
back loop around that structure, feeding the fourth output of the diode ladder
into the diode ladder’s input (Fig. 5.49).
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
5.10. DIODE LADDER
165
×4 times
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
HP1
•(cid:47)
y(t)
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 5.45: Fig. 5.40 implemented by interleaved 1-pole low- and
high-passes.
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
HP1
•(cid:47)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
R
y(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)
2(1 − R)
Figure 5.46: Halved normalized TSK bandpass.
×4 times
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
HP1
•(cid:47)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
R
•(cid:47)
y(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)
2(1 − R)
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 5.47: 8-pole bandpass ladder ﬁlter expressed in terms of
halved normalized TSK bandpasses.
It is instructive to write out the 1-pole equations implied by Fig. 5.48:
˙y1 = ωc
˙y2 = ωc
˙y3 = ωc
˙y4 = ωc
(cid:1)
(cid:0)(x + y2) − y1
(cid:0)(y1 + y3)/2 − y2
(cid:0)(y2 + y4)/2 − y3
(cid:0)y3/2 − y4
(cid:1)
(cid:1)
(cid:1)
(5.18)
In this form it’s easier to guess the reason for the gain elements 1/2 used in
Fig. 5.48, they perform the averaging between the feedforward and feedback
signals. However this averaging in (5.18) and Fig. 5.48 is not done fully consis-
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
166
CHAPTER 5. LADDER FILTER
+
LP1
(cid:47) +
•(cid:47)
(cid:77)(cid:77)(cid:113)(cid:113)(cid:47)
1/2
LP1
(cid:47) +
•(cid:47)
(cid:77)(cid:77)(cid:113)(cid:113)(cid:47)
1/2
LP1
•(cid:47)
(cid:77)(cid:77)(cid:113)(cid:113)(cid:47)
1/2
LP1
•(cid:47)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
x(t)
y1(t)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
y2(t)
y3(t)
y4(t)
Figure 5.48: Diode ladder.
Diode ladder
y4(t)
•(cid:47)
y(t)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 5.49: Diode ladder ﬁlter.
tently. It would have been more consistent to have no 1/2 gain element at the
input of the fourth lowpass, rather than of the ﬁrst one:
˙y1 = ωc
˙y2 = ωc
˙y3 = ωc
˙y4 = ωc
(cid:0)(x + y2)/2 − y1
(cid:1)
(cid:0)(y1 + y3)/2 − y2
(cid:0)(y2 + y4)/2 − y3
(cid:1)
(cid:0)y3 − y4
(cid:1)
(cid:1)
(5.19)
in which case the ﬁrst lowpass would take (x + y2)/2 as its input, the second
lowpass would take (y1 + y3)/2 as its input, the third lowpass would take (y2 +
y4)/2 as its input, and the fourth lowpass would take y3 as its input. However,
(5.18) is a more traditional way to implement a diode ladder ﬁlter. Anyway, the
diﬀerence between (5.18) and (5.19) is actually not that large, since (as we are
going to show below) they result in one and the same transfer function,
The more complicated connections between the 1-pole lowpasses present in
the diode ladder “destroy” the frequency response of the ladder in a remarkable
form, which, is responsible for the characteristic diode ladder ﬁlter sound.13
Generally, the behavior of the diode ladder ﬁlter is less “straightforward” than
the one of the transistor ladder ﬁlter.
Transfer function
We are going to develop the transfer function for the diode ladder in a gen-
eralized form (Fig. 5.50), where Hn(s) denote blocks with respective transfer
functions. In the case of Fig. 5.48 and (5.18) we would have
H1(s) = G(s)
H2(s) = H3(s) = H4(s) =
G(s)
2
(5.20)
13One could argue that the characteristic sound of diode ladder filters is due to nonlinear
behavior, however the nonlinear aspects do not show up unless the filter is driven hot enough.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
5.10. DIODE LADDER
167
while in the case of (5.19) we would respectively have
H1(s) = H2(s) = H3(s) =
where
G(s) =
G(s)
2
1
1 + s
H4(s) = G(s)
(5.21)
(5.22)
H1(s)
•(cid:47)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
H2(s)
•(cid:47)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
H3(s)
•(cid:47)
H4(s)
•(cid:47)
x(t)
y1(t)
y2(t)
y3(t)
y4(t)
Figure 5.50: Generalized diode ladder in transfer function form.
Assuming complex exponential signals est, for the H4(s) block we have
(where H4 is short for H4(s)), therefore
y4 = H4y3
1
H4
y4 = y3
(5.23)
For the H3(s) block we have
Substituting (5.23) we have
y3 = H3(y2 + y4)
1
H4
y4 = H3(y2 + y4)
y4 = y2 + y4
1
H34
1 − H34
H34
y4 = y2
(5.24)
where H34 is a short notation for H3H4.
For the H2(s) block we have
Substituting (5.23) and (5.24) we have
y2 = H2(y1 + y3)
1 − H34
H34
(cid:18)
y4 = H2
y1 +
(cid:19)
1
H4
y4
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
168
CHAPTER 5. LADDER FILTER
1
H4
y4
y4 = y1 +
1 − H34
H234
1 − H34 − H23
H234
y4 = y1
(5.25)
For the H1(s) block we have
Substituting (5.24) and (5.25) we have
y1 = H1(x + y2)
1 − H34 − H23
H234
(cid:18)
y4 = H1
x +
(cid:19)
1 − H34
H34
y4
1 − H34 − H23
H1234
y4 = x +
1 − H34
H34
y4
1 − H34 − H23 − H12(1 − H34)
H1234
1 − H12 − H23 − H34 + H1234
H1234
y4 = x
y4 = x
∆(s) =
y4
x
=
H1234
1 − H12 − H23 − H34 + H1234
(5.26)
where ∆(s) is the diode ladder’s transfer function. It is easy to see that sub-
stituting (5.20) or (5.21) into (5.26) gives identical results, therefore transfer
functions arising out of (5.18) and (5.19) are identical. Formula (5.26) also
gives one more hint at the reason to use a 1/2 gain with all 1-poles except the
ﬁrst or the last one, as in this case we get unit amplitude response at ω = 0:
∆(0) =
1 −
1
2
−
1
8
1
4
= 1
−
1
4
+
1
8
Since we are speciﬁcally interested in Fig. 5.48, let’s write its transfer func-
tion in a more detailed form. Substituting ﬁrst (5.20) and then (5.22) into (5.26)
we have
∆(s) =
G4/8
1 − G2 + G4/8
=
1
8G−4 − 8G−2 + 1
=
=
1
8(1 + s)4 − 8(1 + s)2 + 1
=
1
T4(s + 1)
(5.27)
where T4(x) = 8x4 − 8x2 + 1 is the fourth-order Chebyshev polynomial.14 The
poles of ∆(s) are therefore found from s + 1 = xn or s = −1 + xn where
xn ∈ (−1, 1) are the roots of the Chebyshev polynomial T4(x):
xn = ±
1
2
±
1
√
2
2
14Although the denominator of Δ(s) is a Chebyshev polynomial, this has nothing to do
with Chebyshev filters, despite the name.
5.10. DIODE LADDER
Therefore the poles of ∆(s) are purely real and located within (−2, 0):
pn = −1 ±
1
2
±
1
√
2
2
169
(5.28)
Since the poles of ∆(s) are located on the negative real semiaxis and there are
no zeros, |∆(jω)| is monotonically decreasing to zero on ω ∈ [0, +∞). Thus
∆(s) is a lowpass.15
In Section 2.16 we have seen that two linear systems sharing the same trans-
fer function are equivalent as long as the only modulation which is happening is
the cutoﬀ modulation. Therefore, as long as our implementation is purely linear,
we could replace the complicated diode ladder feedback system in Fig. 5.50 with
simply a serial connection of four 1-poles, whose cutoﬀs are deﬁned by (5.28).16
Further details of replacement of the diode ladder by a series of 1-poles can be
taken from Section 8.2 where general principles of building serial ﬁlter chains
are discussed.
The transfer function of the diode ladder ﬁlter is obtained from (5.27) giving
H(s) =
∆
1 + k∆
=
1
k + ∆−1 =
1
k + T 4(1 + s)
=
1
8(1 + s)4 − 8(1 + s)2 + 1 + k
(5.29)
The corresponding amplitude response is plotted in Fig. 5.51.
k = 0
|H(jω)|, dB
0
-6
-12
-18
-24
k = 16
ωc/8
ωc
8ωc
ω
Figure 5.51: Amplitude response of the diode ladder ﬁlter for var-
ious k.
The poles of the diode ladder ﬁlter, if necessary, can be obtained by solving
8(1 + s)4 − 8(1 + s)2 + 1 + k = 0
which is a biquadratic equation in (1 + s).
15The amplitude response of Δ(s) can be seen in Fig. 5.51 at k = 0.
16Note that such replacement only gives a correct modal output y4, which is the one we
usually need. Other modal outputs, if needed at all, would have to be obtained in a more
complicated way by combining the output signals of the 1-poles.
170
CHAPTER 5. LADDER FILTER
In regards to the multimode diode ladder ﬁlter, notice that the transfer
functions corresponding to the yn(t) outputs are diﬀerent from the ones of the
transistor ladder, therefore the mixing coeﬃcients which worked for the modes
of the transistor ladder ﬁlter, are not going to work the same for the diode
ladder.
Resonance
In order to obtain the information about the resonating peak, we need to ﬁnd
frequencies at which the phase response of ∆(s) is 0◦ or 180◦. Therefore we are
interested in the solutions to the equation
Im(cid:0)8(1 + s)4 − 8(1 + s)2 + 1(cid:1) = 0
where s = jω, ω ∈ R
Substituting jω for s we have
Im(cid:0)8(1 + s)4 − 8(1 + s)2 + 1(cid:1) = 8 Im(cid:0)(1 + jω)4 − (1 + jω)2(cid:1) =
= Im(cid:0)(1 − ω2 + 2jω)2 − (1 − ω2 + 2jω)(cid:1) = 4(1 − ω2)ω − 2ω = 0
The solution ω = 0 is not very interesting. Therefore we cancel the common
factor 2ω obtaining
and therefore
2(1 − ω2) = 1
ω = ±
1
√
2
Now, in order to ﬁnd the selfoscillation boundary value of k we need to ﬁnd the
√
frequency response of ∆(s) at ω = 1/
2 into (5.27)
and using (5.6) we have
2. Substituting s = j/
√
k = 8(1 + s)4 − 8(1 + s)2 + 1 = 8
(cid:18)
1 +
j
√
2
(cid:19)4
(cid:18)
− 8
1 +
(cid:19)2
j
√
2
+ 1 =
= 8
(cid:18) 1
2
(cid:18)
= 8
−
√
(cid:19)2
2
+ j
− 8
(cid:19)
√
2
+ j
7
4
− 8
(cid:18) 1
2
(cid:18) 1
2
√
(cid:19)
2
+ j
+ 1 =
√
+ j
(cid:19)
2
+ 1 = 1 − 14 − 4 = −17
Now, since we are already having negative feedback in Fig. 5.48, the selfoscilla-
tion occurs at k = 17.
Note that the amplitude response in Fig. 5.51 is matching the above analysis
results.
TPT model
Converting Fig. 5.48 to the instantaneous response form we obtain the structure
in Fig. 5.52. From Fig. 5.52 we wish to obtain the instantaneous response of
the entire diode ladder. Then we could use this response to solve the zero-delay
feedback equation for the main feedback loop of Fig. 5.49.
The structure in Fig. 5.52 looks a bit complicated to solve. Of course we
could always write a system of linear equations and solve it in a general way,
e.g. using Gauss elimination, but this has its own complications. Therefore we
5.10. DIODE LADDER
171
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
2gξ+s1
(cid:47) +
•(cid:47)
gξ+s2
(cid:47) +
•(cid:47)
gξ+s3
•(cid:47)
gξ+s4
•(cid:47)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
y1
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
y2
y3
y4
Figure 5.52: Diode ladder in the instantaneous response form.
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
2gξ+s1
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:47) +
•(cid:47)
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
•(cid:47)
•(cid:47)
(cid:47) +
•(cid:47)
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
gξ+s2
gξ+s3
gξ+s4
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
y1
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
y2
y3
y4
x
x
Figure 5.53: Diode ladder in the nested instantaneous response
form.
would rather like to see if we somehow could still use the approach of nested
zero-delay feedback loops, as we have been doing with other ﬁlters until now.
Introducing the nested systems, as shown in Fig. 5.53 by dashed lines, we
can ﬁrst treat the innermost system which has input y2 and outputs y3 and y4.
The equations for this system are
y3 = g(y2 + y4) + s3
y4 = gy3 + s4
Solving for y3, we obtain
y3 =
g
1 − g2 y2 +
gs4 + s3
1 − g2 = g23y2 + s23
where g23 and s23 are new variables introduced as shown above. Since gξ + sn
denote 1-pole lowpasses with halved input signals, 0 < g < 1/2. Respectively
0 < g2 < 1/4 and thus the zero-delay feedback loop doesn’t get instantaneously
unstable. The range of g23 is
0 < g23 =
g
1 − g2 <
1/2
1 − (1/2)2 =
1/2
3/4
=
2
3
Going outside to the next nesting level we have
y2 = g(y1 + y3) + s2 = gy3 + gy1 + s2 = g(g23y2 + s23) + gy1 + s2
Solving for y2:
y2 =
g
1 − gg23
y1 +
gs23 + s2
1 − gg23
= g12y1 + s12
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
172
CHAPTER 5. LADDER FILTER
where 0 < gg23 < 1/2 · 2/3 = 1/3, thus the zero-delay feedback loop doesn’t get
instantaneously unstable. The range of g12 is
0 < g12 =
g
1 − gg23
<
1/2
1
2
·
1 −
=
1/2
1 − 1/3
=
1/2
2/3
=
3
4
2
3
Going outside to the outermost level we have
y1 = 2g(x + y2) + s1 = 2gy2 + 2gx + s1 = 2g(g12y1 + s12) + 2gx + s1
Solving for y1:
y1 =
2g
1 − 2gg12
x +
2gs12 + s1
1 − 2gg12
= g01x + s01
where 0 < 2gg12 < 2 · 1/2 · 3/4 = 3/4, thus the zero-delay feedback loop doesn’t
get instantaneously unstable. The range of g01 is
0 < g01 =
2g
1 − 2gg12
<
1
1 − 2 ·
1
2
·
3
4
=
1
1 − 3/4
=
1
1/4
= 4
Introducing for consistency the notation y4 = gy3 + s4 = g34y3 + s34, we
obtain the instantaneous response for the entire ladder
y4 = g34y3 + s34 =
= g34(g23y2 + s23) + s34 = g34g23y2 + (g34s23 + s34) = g24y2 + s24 =
= g24(g12y1 + s12) + s24 = g24g12y1 + (g24s12 + s24) = g14y1 + s14 =
= g14(g01x + s01) + s14 = g14g01x + (g14s01 + s14) = g04x + s04
it’s not diﬃcult to realize that
0 < g04 = g01g12g23g34 < 4 ·
3
4
·
2
3
·
1
2
= 1
Now g04 is the instantaneous gain of the entire diode ladder. Respectively the
total gain of the of the zero-delay feedback loop in Fig. 5.49 is −kg04 and thus
the feedback doesn’t get instantaneously unstable provided k ≥ −1.
SUMMARY
The transistor ladder ﬁlter model is constructed by placing a negative feedback
around a chain of four identical 1-pole lowpass ﬁlters. The feedback amount
controls the resonance.
The same idea of a feedback loop around a chain of several ﬁlters also results
in further ﬁlter types such as 8-pole ladder, diode ladder and SKF/TSK.
Chapter 6
Nonlinearities
The ﬁlters which we were discussing until now were all linear. Formally this
means that if we consider a ﬁlter as an operator, this operator is a linear one.
Practically this meant that the structures of our ﬁlters were consisting of gains,
summators and integrators. However, ﬁlters used in synthesizers often show no-
ticeably nonlinear behavior. In terms of block diagrams, introducing nonlinear
behavior means that we should add nonlinear elements to the set of our block
diagram primitives.
Nonlinear ﬁlters have more complicated behavior and are capable of produc-
ing richer sound than the linear ones. Usually they exhibit complex overdriving
eﬀects, when driven with an input signal of a suﬃciently high level. Another
special feature of many nonlinear ﬁlters is their ability to increase the resonance
beyond a formally inﬁnite amount, entering the so-called self-oscillation.
6.1 Waveshaping
We just mentioned that in order to build non-linear ﬁlters we need to introduce
nonlinear elements into the set of our block diagram primitives. In fact we are
going to introduce just one new type of element, the waveshaper :
x(t)
f (x)
y(t)
A waveshaper is simply applying a given function to its input signal, and sends
the respective function value as its output signal:
y(t) = f (x(t))
The function f (x) can be any “reasonable” function, e.g. f (x) = |x| or f (x) =
sin x etc.
Usually the function f cannot vary with time, that is, the function’s pa-
rameters, if it has any, are ﬁxed. E.g. if f (x) = sin ax, then a is usually ﬁxed
to some particular value, e.g. a = 2, which doesn’t vary. Often, this is just a
matter of convention. e.g. the waveshaper sin ax can be represented as a serial
connection of a gain element and the waveshaper itself:
x(t)
a
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
173
f (x)
y(t)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
174
CHAPTER 6. NONLINEARITIES
in which case the waveshaper itself is time-invariant.
Still, if necessary, it’s no problem for the waveshaper to contain time-varying
parameters, as long as the time-varying parameters are “externally controlled”
(in the same way how e.g. ﬁlter cutoﬀ is controlled). That is, the waveshaper’s
parameters cannot depend on the values of the signals within the block diagram.
If one needs the parameter dependency on the signals of the block diagram, then
one should consider such dependencies as additional inputs of the nonlinear
element and we end up with a multi-input element of the block diagram. It is
no problem to use such elements, but normally we should not refer to them as
waveshapers, since commonly, waveshapers have one input and one output.
In order to be representable as a function of the input signal, a waveshaper
clearly shouldn’t have any dependency on its own the past. That is waveshaper
is a memoryless element.
6.2 Saturators
The probably most commonly used category of waveshapers is saturators. There
is no precise deﬁnition of what kind of waveshaper is referred to as saturator,
it’s easier to give an idea of what a saturator is by means of example.
Bounded saturators
One of the most classical saturators is the hyperbolic tangent function:
y(t) = tanh x(t)
(6.1)
(Fig. 6.1). Even if the input signal of this saturator is very large, the output
never exceeds ±1. Thus, this element saturates the signal, which is the origin
of the term saturator.
y
1
0
−1
−2
−1
1
2
x
Figure 6.1: Hyperbolic tangent y = tanh x.
Other saturators with shapes similar to the hyperbolic tangent include:
y = sin arctan x = x/
(cid:112)
1 + x2
(6.2a)
6.2. SATURATORS
175
(cid:40)
y =
x · (1 − |x|/4)
sgn x
if |x| ≤ 2
if |x| ≥ 2
(Parabolic saturator)
(6.2b)
y = x/(1 + |x|)
(Hyperbolic saturator)
(6.2c)
(this list is by no means exhaustive). Is is not diﬃcult to see that the values of
the hyperbolic tangent (6.1) and the saturators (6.2) do not exceed 1 in absolute
magnitude. That is, their ranges are bounded. We are going to refer to such
saturators as bounded-range saturators or simply bounded saturators.
From the four introduced saturation functions the parabolic saturator (6.2b)
stands out in that the full saturation is achieved at |x| = 2, whereas for other
shapes it’s not achieved at ﬁnite input signal levels. Thus, the range of (6.2b)
is [−1, 1], therefore being compact. We will refer to such saturators as compact-
range monotonic saturators.
Another important distinction of the parabolic saturator is that it has three
discontinuities of the second derivative (at x = 0 and x = ±2) and the hy-
perbolic saturator has one discontinuity of the second derivative (at x = 0).
Even though such discontinuities are not easily visible on the graph, they af-
fect the character of the saturator’s output signal. Usually such discontinuities
are rather undesired, as they represent abrupt irregularities in the saturator’s
shape, so it’s generally better to avoid those.1 A common reason to tolerate
derivative discontinuities in a saturator, though, is performance optimization.
Transparency at low signal levels
A property commonly found with saturators is that at low levels of input signals
the saturator is transparent: f (x) ≈ x for x ≈ 0. Equivalently this condition
can be written as
f (0) = 0
f (cid:48)(0) = 1
Visually it manifests itself as the function’s graph going at 45◦ through the
origin. Clearly, all the previously introduced saturators have this property.
(6.3)
The property (6.3) is not really a must, but it’s quite convenient if the
saturators have it, particularly for the analysis of system behavior at low signal
levels. For that reason it’s common to represent a non-unit derivative at the
origin via a separate gain. Given a saturation function f (x) such that f (0) = 0
but f (cid:48)(0) (cid:54)= 1 we introduce a diﬀerent saturation function ˜f (x) such that ˜f (0) =
0 and ˜f (cid:48)(0) = 1. E.g. we can take
so that
˜f (x) =
f (x)
f (cid:48)(0)
f (x) = f (cid:48)(0) ˜f (x)
The coeﬃcient f (cid:48)(0) is then represented as a separate gain element.
x(t)
˜f (x)
f (cid:48)(0)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
y(t)
1Sometimes the effect created by discontinuities is explicitly being sought after, e.g. in a
rectification waveshaper f (x) = |x|.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
176
CHAPTER 6. NONLINEARITIES
Apparently this representation is not available if f (cid:48)(0) = 0, however in such cases
the saturator eﬀectively breaks the connection at low signal levels, working as
a zero gain.
Saturators with f (0) (cid:54)= 0 can be represented by separation of the value f (0)
into a DC oﬀset signal:
f (x) = f (0) + ˜f (x)
which is treated as another input signal with a ﬁxed value f (0):
f (0)
x(t)
˜f (x)
Unbounded saturators
y(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Sometimes we want saturation behavior, but do not want a hard bound on the
output signal’s level. One function with this property is inverse hyperbolic sine:
y = sinh−1 x = ln
(cid:16)
x +
(cid:112)
x2 + 1
(cid:17)
(6.4)
(Fig. 6.2) While having the usual transparency property (6.3), it is not bounded.
The asymptotic behavior of the hyperbolic sine is similar to the one of the
logarithm function:
sinh−1 x ∼ sgn x · ln |2x|
x → ∞
Another saturator with a similar behavior can be obtained as an inverse of
y = x(1 + |x|), which is
y =
2x
1 + (cid:112)1 + |4x|
(6.5)
behaving as (cid:112)|x| at x → ∞.
Such kind of waveshapers are also referred to saturators, even though the
saturation doesn’t have a bound. We will refer to them as unbounded-range or
unbounded saturators.
Apparently, unbounded saturators represent a weaker kind of saturation
than bounded ones. The weakest possible kind of saturation is achieved if y
grows as a linear function of x at x → ∞. Such saturators can be built by
introducing a linear term into the saturator’s function. Given a saturator f (x)
where f (x) can be any of the previously discussed saturators, we build a new
saturator by taking a mixture of y = f (x) and y = x:
y = (1 − α)f (x) + αx
(0 < α < 1)
(6.6)
where we needed to multiply f (x) by 1 − α to keep the transparency property
(6.3) (provided it was holding for f (x)). Apparently y ∼ αx for x → ∞. We
can refer to such saturators as asymptotically linear saturators. The previously
discussed saturators such that y = o(x) for x → ∞ can be respectively referred
to as slower-than-linear saturators.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
6.2. SATURATORS
177
y
2
1
0
-1
-2
−4
−3
−2
−1
1
2
3
4
x
Figure 6.2: Inverse hyperbolic sine y = sinh−1 x.
Soft- and hard-clippers
One special but important example of a saturator is the hard clipper, shown in
Fig. 6.3.2 In contrast, we will be referring to all previously discussed saturators
as soft clippers.3
y
1
0
−1
−2
−1
1
2
x
Figure 6.3: Hard clipper.
2Apparently, hard clipper is a compact-range saturator.
3There doesn’t seem to be a universally accepted definition of which kinds of saturators are
referred to as soft clippers, and which aren’t. E.g. the set of soft clippers could be restricted
to contain only bounded saturators. In this book we will understand the term soft clipper in
the widest possible sense.
178
CHAPTER 6. NONLINEARITIES
Saturation level
The previously introduced bounded saturators (6.1) and (6.2) were all saturating
at y = ±1. But that is not always desirable. Given a bounded saturator f (x)
with the saturation level y = ±1 we can change the saturation level to y = ±L
by simultaneouly scaling the x and y coordinates:
y(t) = L · f (x/L)
(6.7)
(Fig. 6.4). The simultaneous scaling of x and y preserves the transparency
property (6.3).
y = 2tanh(x/2)
y = tanh x
1
2
3
4
x
y
2
1
0
-1
-2
−4
−3
−2
−1
Figure 6.4: Changing the saturation level.
Saturator as variable gain
Sometimes it is useful to look at saturators as at variable gain elements. E.g.
we can rewrite y = tanh x as
y = tanh x = x ·
tanh x
x
= g(x) · x
The graph of the function g(x) = tanh x
x
is shown Fig. 6.5. Thus
g(x) ≈ 1
for x ≈ 0
g(x) ∼ 1/|x|
for x → ∞
(6.8)
(6.9a)
(6.9b)
That is at low signal levels the saturator is transparent, at high signal levels is
reduces the input signal’s amplitude by a factor of approximately 1/|x|. Ap-
parently, this kind of behavior is shown by all bounded saturators. Unbounded
saturators give a similar picture, as long as they are slower than linear. For
asymptotically linear saturators, such as (6.6), we have g(x) → α instead.
6.3. FEEDBACK LOOP SATURATION
179
g(x)
2
1
−4
−3
−2
−1
0
1
2
3
4
x
Figure 6.5: g(x) =
tanh x
x
.
6.3 Feedback loop saturation
In the ladder ﬁlter and its variations, such as 4-pole and 8-pole ladders and
SKF/TSK ﬁlters, the resonance is implemented by means of a feedback loop.
By this we mean that when the feedback loop is disabled (by setting the feedback
gain to zero), there is no resonance, and the resonance amount is increased by
increasing the amount of the feedback (thus e.g. the SVF ﬁlter doesn’t fall into
this category). With such ﬁlter structures, when the feedback amount goes
above a certain threshold (e.g. k = 4 for the 4-pole lowpass ladder or k = 2 for
the SKF) the ﬁlter becomes unstable and “explodes” (the ﬁlter’s state and the
output signal indeﬁnitely grow). By putting a saturator anywhere within such
feedback loop we can prevent the signals in the feedback loop from the inﬁnite
growth, making the ﬁlter stable again.
Feedforward path saturation
One of the common positions for the feedback loop saturator is in the feed-
forward path right after the feedback merge point (Fig. 6.6). Given that the
saturator is a bounded one (such as tanh x), the output signal of such saturator
is guaranteed to be bounded. Since the rest of the feedforward path in Fig. 6.6
is known to be BIBO-stable (independently of the feedback setting), the output
of the ﬁlter is bounded too and thus the entire ﬁlter is BIBO-stable.
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
tanh
4 × LP
•(cid:47)
y(t)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 6.6: Ladder ﬁlter with a saturator in the feedforward path.
We could also view the saturator in Fig. 6.6 as a variable gain g (6.8).
Apparently, as the amplitude of the signal grows, the average value of g is
In those terms, the saturator is eﬀectively reducing the
decreasing to zero.
feedback gain from k to k · (cid:104)g(cid:105) (where (cid:104)g(cid:105) is the average value of g). Since at
large signal amplitudes (cid:104)g(cid:105) can get arbitrarily close to zero, the value of k · (cid:104)g(cid:105)
goes below 4, which, intuitively, prevents the ﬁlter from exploding. Unbounded
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
180
CHAPTER 6. NONLINEARITIES
saturators are therefore having the same eﬀect, as long as they are slower than
linear.
With asymptotically linear saturators the ﬁlter will still explode at some
point. Assuming the saturator has the form (6.6) we have g(x) → α and thus
(cid:104)g(cid:105) → α and k · (cid:104)g(cid:105) → αk. Thus, we could expect that for something like αk < 4
the ﬁlter should not explode.
Effects of transient response
As we should remember, one possible way to look at a ﬁlter getting unstable is
that its transient response grows instead of decaying with time. Each pair of
conjugate poles pn and p∗
n of the ﬁlter contributes a transient component of the
form
Aepnt + A∗ep∗
nt = aet Re pn cos(t Im pn + ϕn)
Thus at Re pn = 0 we have a sinusoid of frequency ω = Im pn. At Re pn > 0 we
have the same sinusoid of an exponentially growing amplitude. This sinusoid
will be present in the ﬁlter’s output even in the absence of the input signal.4
Since at Re pn ≥ 0 this sinusoid is self-sustaining, the ﬁlter is said to self-
oscillate. The saturator in the feedback loop prevents the self-oscillation from
inﬁnite growth.5
Suppose the system in Fig. 6.6 is at k ≈ 4, that is it is selfoscillating or at
least strongly resonating. Let
u(t) = x(t) − ky(t)
(6.10)
denote the input signal of the saturator and recall the representation of a satu-
rator as a variable gain element (6.8). Then the output signal of the saturator
is
v(t) = tanh u(t) = g(u)·u = g(u)x(t)−g(u)k ·y(t) = g(u)x(t)− ˜k(u)y(t) (6.11)
u
where g(u) = tanh u
. Comparing (6.10) to the last expression in (6.11) we
see that the eﬀect of the saturator can be seen as the “replacing” x(t) with
g(u)x(t) and ky(t) with ˜k(u)y(t). Thus, ˜k(u) = g(u)k is the new “eﬀective
feedback amount”. Now, by increasing the amplitude of the input signal x(t)
we increase the amplitude of u(t) and thus reduce the magnitude of g(u) and
thereby reduce the eﬀective feedback amount ˜k, which in turn shows up as
reduction of resonance. That is, at high amplitudes of the input signal the
resonance oscillations kind of disappear.
One intuitive way to look at this is to say that the input signal and the
resonance are “ﬁghting” for the saturator’s headroom, and if the input signal
4If the system is in the zero state, then in the absence of the input signal it will stay
forever in this state of “unstable equilibrium”. In analog circuits, however, there are always
noise signals present in the system, which will excite the transient response components,
thus destroying the equilibrium. In the digital implementation such excitations need to be
performed manually. This can be done by initializing the system to a not-exactly-zero state,
or by sending a short excitation impulse into the system at the initialization, or by mixing
some low-level noise at one or multiple points into the system. Often a very small constant
DC offset will suffice instead of such noise.
5As the saturator is effectively reducing the total gain of the feedback loop, at k = 4 the
selfoscillation will first have an infinitely small signal level, where the saturator is transparent.
Increasing the value of k further we can bring the selfoscillation to an audible level.
6.3. FEEDBACK LOOP SATURATION
181
has a very high level, it “pushes” the resonating component of the signal out.
On the other hand, if the input signal level is low, then the entire headroom is
taken by the resonating component which will be therefore much louder than
the input signal. There is usually some “sweet spot” in the input signal’s level,
where the ﬁghting doesn’t kill the resonance, but results in a nice interaction
between the input signal and the resonance.
Feedback path saturation
The amount of ﬁghting (at the same input signal level) can be decreased by
putting the saturator into the feedback path, either prior to the feedback gain
(Fig. 6.7) or past the feedback gain (Fig. 6.8).6 In this case the input signal x(t)
doesn’t directly enter the saturator but ﬁrst goes through the four 1-pole lowpass
ﬁlters, which somewhat reduces its amplitude (depending on the signal and on
the ﬁlter’s cutoﬀ). The diﬀerence between Figs. 6.7 and Fig. 6.8 is obviously
that in one case the eﬀective saturation function is y = k tanh x whereas in the
other one it’s y = tanh kx. This means that in the ﬁrst case the saturation level
is ±k whereas in the second one it’s ﬁxed to ±1 (Fig. 6.9).
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
4 × LP
•(cid:47)
y(t)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
tanh
Figure 6.7: Ladder ﬁlter with a saturator in the feedback path
(pre-gain).
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
4 × LP
•(cid:47)
y(t)
tanh
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 6.8: Ladder ﬁlter with a saturator in the feedback path
(post-gain).
The amount of ﬁghting will also be decreased by using a weaker saturation
curve. E.g. using an unbounded saturator instead of a bounded one, in the most
extreme case having an asymptotically linear saturator. A classical example
of this approach is ecountered in the nonlinear Sallen–Key ﬁlter (Fig. 6.10).
It is an interesting observation that the sound of nonlinear Sallen–Key ﬁlter
signiﬁcantly diﬀers from the sound of nonlinear transposed Sallen–Key ﬁlter
(Fig. 6.11) since in one case the saturator’s output goes through a highpass and
a lowpass, while in the other case it goes through two lowpasses before reaching
the ﬁlter’s output.7
6Notice that, with the saturator positioned in the feedback path, at k = 0 the filter
effectively becomes linear.
7This observation was made by Dr. Julian Parker.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
182
CHAPTER 6. NONLINEARITIES
−3
−2
−1
y
2
1
0
-1
-2
1
2
3
x
Figure 6.9: Pre-gain (y = k tanh x, solid) vs. post-gain saturation
(y = tanh kx, dashed).
xLP(t)
xBP(t)
(cid:47)LP
HP
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
MM1
LP1
•(cid:47)
y(t)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
αx + (1 − α) sinh−1 x
Figure 6.10: Sallen-Key ﬁlter with an asymptotically linear satu-
rator.
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
MM1
LP
HP
•(cid:47)
yLP(t)
yBP(t)
αx + (1 − α) sinh−1 x
Figure 6.11: Transposed Sallen-Key ﬁlter with an asymptotically
linear saturator.
Transfer function
For systems containing nonlinear elements the complex exponentials est are no
longer system eigensignals. That is, given an input signal of the form Aest the
output will not have a similar form. Therefore the idea of the transfer function
as well as amplitude and phase responses doen’t work anymore.
Still, given that the nonlinear elements satisfy the transparency condition
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
6.4. NONLINEAR ZERO-DELAY FEEDBACK EQUATION
183
(6.3), at low signal levels the nonlinearities have almost no eﬀect and the system
is approximately linear. In that sense the transfer function stays applicable to a
certain degree and still can be used to analyse the ﬁlter’s behavior, although the
error is growing stronger at higher signal levels. Nevertheless, as a rule, qualita-
tively the ﬁlters retain their main properties also in the presence of saturators.
The lowpass ﬁlters stay lowpass, bandpass ﬁlters stay bandpass etc.
6.4 Nonlinear zero-delay feedback equation
The introduction of the nonlinearity in the feedback path poses no problems
for a naive digital model. In the TPT case however this complicates the things
quite a bit. Consider Fig. 5.5 redrawn to contain the feedback nonlinearity
(Fig. 6.12).
x[n]
u[n]
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
tanh
Gξ + S
•(cid:47)
y[n]
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
Figure 6.12: Nonlinear TPT ladder ﬁlter in the instantaneous re-
sponse form.
Writing the zero-delay feedback equation we obtain
u = x − k(G tanh u + S)
(6.12)
Apparently, the equation (6.12) is a transcendental one. It can be solved only
using numerical methods. Also, the linear zero-delay feedback equation had
only one solution, but how many solutions does (6.12) have? In order to answer
the latter question, let’s rewrite (6.12) as
(x − kS) − u = kG tanh u
(6.13)
If k ≥ 0 then v(u) = kG tanh u is a nonstrictly increasing function of u,8 while
v(u) = (x − kS) − u is a strictly decreasing function of u. Thus, (6.13) (and
respectively (6.12)) has a single solution (Fig. 6.13). At k < 0 we also typically
have one solution (Fig. 6.14) unless kG > −1, in which case (6.13) has three
solutions Fig. 6.15. Fortunately, kG > −1 corresponds to instantaneously un-
stable feedback, and thus normally we are not so much interested in this case
anyway. However, if needed, one could use the concept of the instantenous
smoothing to ﬁnd out the applicable solution among the three formal ones.
Having found the zero-delay equation solution u, we proceed in the usual
way, ﬁrst letting u through the tanh waveshaper and then letting it through the
1-pole lowpasses (denoted as Gξ + S in Fig. 6.12), updating the 1-pole states
along the way and ultimately obtaining the value of y.
Now we are going to discuss some possible approaches for ﬁnding u. This
discussion is by no means exhaustive and the reader is advised to consult the
literature on numerical methods for further information.
8Recall that for a series of 1-pole lowpasses (which Gξ + S denotes in Fig. 6.12) 0 < G < 1.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
184
CHAPTER 6. NONLINEARITIES
v
0
kG tanh u
(x
−kS)−
u
Figure 6.13: The solution of (6.13) for k > 0.
v
(x
−kS)−
u
kG tanh u
0
u
u
Figure 6.14: The solution of (6.13) for −1 < kG < 0.
6.5 Iterative methods
Fixed-point iteration
Starting with some initial value u = u0 we compute iteratively the left-hand
side of (6.12) from the right-hand side:
un+1 = x − k(G tanh un + S)
(6.14)
and hope that this sequence converges quickly enough.9 Intuitively, the conver-
gence gets worse at larger absolute magnitudes of kG, that is at high cutoﬀs
(large G) and/or high resonance values (large k). Conversely, it gets better as
9In a realtime situation it would be a good idea to artificially bound the number of iterations
from above.
6.5.
ITERATIVE METHODS
185
v
0
u
kG tanh u
(x
−kS)−
u
Figure 6.15: Solutions of (6.13) for kG < −1.
the sampling rate increases (since G becomes smaller in this case). Generally,
the convergence fails for |kG| ≥ 1.
The process deﬁned by (6.14) has a strong similarity to the naive approach
to time discretization. Indeed, for the frozen values of G and S one can treat
Fig. 6.12 as stateless zero-delay feedback system (Fig. 6.16). And then we simply
implement this system in the naive way by introducing a unit delay at the
point of the signal u (Fig. 6.17) and letting this system run for some number of
discrete-time ticks. This is a bit like oversampling of the instantaneous feedback
loop part of the system.10
x
u
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
tanh
S
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
G
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
•(cid:47)
y
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 6.16: Zero-delay feedback equation (6.12) as a stateless zero-
delay feedback system.
So, it is as if we introduce a “nested” discrete time into a single tick of the
“main” discrete time. This suggests a natural choice of the initial value of u
for (6.14), namely, taking the previous value of u (that is the value from the
previous sample of the “main” discrete time) as the iteration’s initial value u0.
Under the consideration of the concept of the instantaneous smoothing (in-
troduced in Section 3.13), the interpretation in Fig. 6.17 also suggests a way to
10Of course this is not exacty oversampling, because the state of the system (manfesting
itself in the S variable) is frozen.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
186
CHAPTER 6. NONLINEARITIES
x
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
u
z−1
tanh
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
S
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
G
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
y
Figure 6.17: Interpretation of the equation (6.14) as an “oversam-
pled” naive discrete-time model of the stateless feedback loop in
Fig 6.16.
improve the convergence of the method by introducing a smoother into the feed-
back loop of Fig. 6.17. In a practical implementation such smoother can be a
naive 1-pole lowpass, like in Fig. 6.18, which eﬀectively lowers the total feedback
gain from kG to a smaller value.11 However, even though such smoother may
improve the convergence at high kG, obviously it can deteriorate the conver-
gence in good situations. Particularly at k = 0 the iteration process is supposed
to immediately converge, however in the presence of the lowpass smoother it
will converge exponentially instead.
x
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
LP1
z−1
u
tanh
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
S
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
G
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
y
Figure 6.18: Using a 1-pole lowpass smoother to improve conver-
gence of signals in Fig. 6.17.
Newton–Raphson iteration
A very popular approach in practical DSP is Newton–Raphson method, which
is based on the idea of linearization of the function around the current point un
by the tangent line. Instead of solving (6.13) we solve
(x − kS) − un+1 = kG(tanh un + (un+1 − un) tanh(cid:48) un)
(6.15)
for un+1 to obtain the next guess and repeat the iteration (6.15) until it con-
verges. Fig. 6.19 illustrates the idea.
11Clearly, the smoother will not help in the instantaneously unstable case, occurring when
kG ≤ −1.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
6.5.
ITERATIVE METHODS
187
v
0
un
un+1
kG tanh u
(x
−kS)−
u
u
Figure 6.19: Newton–Raphson method:
gent line.
linearization by the tan-
The textbook version of Newton–Raphson method is formulated in terms
of searching for a zero-crossing of a function (Fig. 6.20). By subtracting the
left-hand side of (6.13) from the right-hand side we obtain the equation
f (u) = u + k(G tanh u + S) − x = 0
(6.16)
Respectively, the iterations are generated by solving
f (un) + (un+1 − un)f (cid:48)(un) = 0
(6.17)
Apparently (6.15) and (6.17) (and respectively Figs. 6.19 and 6.20) are equiva-
lent, both giving
un+1 = un −
= un −
un + k(G tanh un + S) − x
1 + kG/ cosh2 un
= un −
f (un)
f (cid:48)(un)
un + k(G tanh un + S) − x
1 + kG(1 − tanh2 un)
=
Newton–Raphson method converges very nicely in almost linear areas of
f (u), the convergence getting worse as f (u) becomes more nonlinear. As with
ﬁxed-point iteration, the convergence deteriorates at large |kG|, as the predic-
tion error of un increases.12
As in the ﬁxed-point iteration method, the value of u from the previous
sample tick is a natural choice for the iteration’s initial value as well. This
choice usually leads to fast convergence if the new solution lies close to the
value of u on the previous sample. However in excessive situations (such as high
cutoﬀ and/or high input signal frequency) the old solution could lie within the
right-hand side saturation range of tanh u (that is u (cid:29) 0) and the new solution
could lie within the left-hand side saturation range of tanh u (that is u (cid:28) 0).
12There are a number of tricks which can be employed to improve the convergence of
Newton–Raphson, but even those might not help in all situations. The specific tricks can be
found in the literature on numerical methods and fall outside the scope of this book.
188
CHAPTER 6. NONLINEARITIES
v
un
0
un+1
u
u+k(Gtanhu+S)−x
Figure 6.20: Texbook version of Newton–Raphson method (note
that the aspect ratio of the graph is not 1:1)
The solution search by Newton–Raphson iterations will need to traverse both
“knees” (areas of higher curvature) of tanh along the way, which usually has
a negative impact on the convergence. The neutral choice of u = 0 as initial
value might somewhat improve this worst-case scenario, while simultaneously
deteriorating the convergence in “nice” situation.
Other, more advanced approached to the choice of the initial point may be
used. Often one uses Newton–Raphson to reﬁne the result of another method,
so that the initial point is alredy suﬃciently close to the true solution.
There is also some freedom of the choice of the variable to solve for. E.g. in
Fig. 6.19 we could have been solving for v instead of u. This means that we are
having
from where
v = (x − kS) − u
v = kG tanh u
u = (x − kS) − v
u = tanh−1 (v/kG)
and (6.13) turns into
(x − kS) − v = tanh−1 (v/kG)
In this speciﬁc case v is hardly a better choice compared to u. For one we have
a division by zero if k = 0.13 Worse, one could see in Fig. 6.19 that vn+1 is
located above the horizontal asymptote of kG tanh u, which means that we are
getting outside of the domain of tanh−1(v/kG). And even if we’re not outside of
the domain, there still could be large precision losses when evaluating tanh−1 at
points close to ±1. The convergence speed is likely to be aﬀected too. Therefore
a good choice of the variable to solve for is important.
13Taking ¯v = tanh u as the unknown to solve for addresses the division by zero issue, but
the other issues are similar to the choice of v = kG tanh u.
6.5.
ITERATIVE METHODS
189
Bisection
Newton–Raphson method usually converges better than ﬁxed-point iteration,
but the potential convergence problems of the former can be diﬃcult to predict.
Often there can be good ways to address the convergence issues in Newton–
Raphson method, but it might be worth it to have an alternative approach,
which is not suﬀering from such issues at all.
From Fig. 6.13 we could notice that for k ≥ 0 we are looking for an inter-
section point of a monotonically decreasing straight line with a (nonstrictly)
monotonically increasing curve. Therefore, if we somehow initially bracket the
solution point of (6.13) we can search for it using bisection.
Given the bracketing range u ∈ [an, bn] we take the middle point un+1 =
(an + bn)/2 and compare the values of the left- and right-hand sides of (6.13)
at un+1. Depending on which of the two sides has a larger value, we take
either [un+1, bn] or [an, un+1] as the new bracketing range [an+1, bn+1]. Fig. 6.21
illustrates.
v
0
a
=
u
2
/
)
b
+
a
(
=
u
b
=
u
kG tanh u
u
(x
−kS)−
u
Figure 6.21: Bisection method.
Obviously the size of the bracketing range halves on each step and we re-
peat the procedure until the bracketing range becomes suﬃciently small. The
convergence speed therefore doesn’t depend on values of ﬁlter’s parameters or
signals and the iteration is guaranteed to converge. However we need to be able
to somehow ﬁnd the initial bracketing range [a0, b0].
Fortunately, with monotonic saturation shapes such as tanh u this is not
very diﬃcult. We can construct the initial bracketing range by noticing that
the graph of the function v = kG tanh u lies between v ≡ 0 and v = kG sgn u
(Fig. 6.22).
With unbounded saturators such as inverse hyperbolic sine one needs to get
slightly more inventive. One possible idea is shown in Fig. 6.23. This however
doesn’t work for k < 0. In that case we could reuse the approach of Fig. 6.22
by taking a vertically oﬀset version of (6.5) as a bound on sinh−1 u (Fig. 6.24).
The intersection on v = (x − kS) − u with this bound can be found by solving
a quadratic equation (more on this in Section 6.7). Obviously, the same idea
works for k ≥ 0 too.
190
CHAPTER 6. NONLINEARITIES
v
0
a
=
u
b
=
u
kGsgn u
kG tanh u
u
(x
−kS)−
u
Figure 6.22: Initial bracketing for bisection.
v
0
a
=
u
b
=
u
kGsinh−1u
u
(x
−kS)−
u
Figure 6.23: Initial bracketing for bisection in the case of an un-
bounded saturator and k ≥ 0. First we ﬁnd the right bracket b
and then use v = kG sinh−1 b to ﬁnd the left bracket a.
If nothing else helps to ﬁnd the initial bracketing range for a (monotonic)
nonlinearity f (u), one could simply start at some point, such as e.g. the zero-
crossing of v = (x−kS)−u, determine the direction of other bracket by compar-
ing v = (x − kS) − u to kG · f (u) and then take steps of progressively increasing
size (exponential increasing of steps is usually a good idea) until the comparison
result of v = (x − kS) − u and kG · f (u) ﬂips.
Even though bisection method guarantees convergence, the convergence speed
might be a bit too low for our purposes. Let’s assume that the magnitude order
of the signals in the ﬁlter is 100 and let’s assume that the length of the initial
bracketing segment [a0, b0] has about the same order of magnitude. Then, to
6.6. APPROXIMATE METHODS
191
kG·
(cid:16)
1+
2u
√
1+|4u| +0.6·sgnu
(cid:17)
kGsinh−1u
v
0
(x
−kS)−
u
u
Figure 6.24: Initial bracketing for bisection in the case of f (u) =
sinh−1 u and k < 0.
reach a −60dB SNR14 corresponding to the order of magnitude of 10−3, we’ll
need about 10 iterations. This might be a bit too expensive for a realtime audio
processing algorithm on modern computers.15
6.6 Approximate methods
We might also attempt to ﬁnd a rough approximate solution of (6.12) without
running an iterative scheme. Having found u, we would simply pretend it’s a
true solution, and proceed as usual in the zero-delay feedback solution scheme,
sending u through the tanh waveshaper and further through the 1-pole low-
passes, updating their state along the way. Several approximation approaches
seem to be in (more or less) common use:
Linearization at zero. At small signal levels the nonlinearity is almost trans-
parent:
tanh u ≈ u
Hoping that our signal level is “suﬃciently small”, whatever that means,
we could replace tanh u by u and solve the resulting linear equation:
u = x − k(Gu + S)
Note that this is equivalent to one step of Newton–Raphson with u = 0
as the initial guess.
14Treating the error in the numerically computed solution as noise, we can define the signal-
to-noise ratio (SNR) as the ratio of the absolute magnitudes of the error and the signal,
expressed in decibels.
15Whether this is too expensive or not depends on a number of factors. E.g. in Newton–
Raphson method we needed to compute both tanh u and tanh(cid:48) u. With the hyperbolic tangent
function we were quite fortunate in that the derivative of the function is trivially computable
from the function value (tanh(cid:48) u = 1−tanh2 u) and thus doesn’t create significant computation
cost. Had the derivative computation been expensive, the computation cost of 10 iterations
of bisection could have been comparable to 5 iterations of Newton–Raphson.
192
CHAPTER 6. NONLINEARITIES
Linearization at operating point. Hoping that the signals within the ﬁlter
do not change much during one sample tick, we replace tanh u with its
tangent line at the current point:
tanh u ≈ tanh u−1 + (u − u−1) · tanh(cid:48) u−1
where u−1 is the value of u at the previous discrete time moment. This is
equivalent to one step of Newton–Raphson with u−1 as the initial guess.
Usually this approximation provides a better result, however in the exces-
sive (but not so unusual) situations of high cutoﬀ, high feedback amount
and/or high signal frequencies this can work worse than the linearization
at zero. Thus, the linearization at zero might provide a better “worst case
performance”.
Linearization by secant line16 On the graph of tanh u we draw a straight
line going through the origin (0, 0) and the operating point (u−1, tanh u−1)
and use this line as our linearization to obtain the value of u. Being a
mixture of the previous two approaches, in moderately excessive situations
this could work better than the linearization at the operating point, but
at more excessive settings could work worse than the linearization at zero.
The readers are however encouraged to gain their own experience and
judgement in the choice of the initial guess approach.
All the above quick approximation approaches share the same idea of replac-
ing the nonlinearity with a straight line. In that regard it is important that we
have chosen to solve for the signal u at the saturator’s input, so that the signal
obtained through the approximation is then really sent through the nonlinear-
ity before reaching the 1-poles and the output. One can view this as if, after
having obtained the approximated result, we are doing one step of ﬁxed-point
iteration.17 Had we instead chosen to solve for the signal at the saturator’s
output, the results would have been more questinable. Particularly, in the case
of linearization at zero there would have been no diﬀerence to the linear case
whatsoever.
The above approximation approaches work reasonably well with saturation
type of nonlinearities. Obviously, the error increases as kG becomes larger and
thus the system becomes “more non-linear”. Notably, G, being monotonically
growing in respect to ωcT , decreases as the sampling rate grows, thus the ap-
proximation error is smaller at higher sampling rates.
6.7
2nd-order saturation curves
It is possible to avoid the need of solving the transcendental equation by using
a saturator function which still allows analytic solution. This is particularly
the case with second-order curves, such as hyperbolas. E.g. f (x) = tanh x can
be replaced by f (x) = x/(1 + |x|) (which consists of two hyperbolic segments),
thereby turning (6.13) into:
(x − kS) − u = kG
u
1 + |u|
(6.18)
16Proposed for usage in the zero-delay feedback context by Teemu Voipio.
17This is a particular case of a more general idea, where we would use the result obtained
by one of the above approximations as an initial point for an iterative algorithm.
6.7. 2ND-ORDER SATURATION CURVES
193
The inverse of f (x) = sinh x can be replaced by the inverse of f (x) = x(1 + |x|),
consisting of two parabolic segments.
In order to solve (6.18), which graphically is an itersection between the lines
v = (x − kS) − u and v = kG · f (u) (same as in Figs. 6.13, 6.14, 6.15), we ﬁrst
need to ﬁnd out, whether the intersection is occuring at u > 0 or u < 0 (the
case u = 0 can be included into either of the cases). Looking at Figs. 6.13 and
6.14, it’s not diﬃcult to realize that for kG > −1 this is deﬁned solely by the
sign of the value which (x − kS) − u takes at u = 0. Thus (6.18) turns into
(x − kS) − u = kG
(x − kS) − u = kG
u
1 + u
u
1 − u
if x − kS ≥ 0
if x − kS ≤ 0
(6.19a)
(6.19b)
Each of the equations (6.19) is a quadratic equation in respect to u.
Choosing the appropriate one of the two solutions of the quadratic equation
is easy. E.g. for (6.19a) the choice can be made with the help of Fig. 6.25.
Taking into account the restriction x − kS ≥ 0, we see that we should be alway
interested in the larger of the two solutions u1, u2. The choice of the appropriate
solution for (6.19b) can be done using similar considerations.
v
v =kGu/(1+u)
v
=(x
−kS)−
u
v = kGu/(1 + u)
u
0
Figure 6.25: Choice of the solution of the quadratic equation for
f (u) = u/(1+u). The dashed line shows the graph of v = kGu/(1+
u) for kG < 0.
In solving the quadratic equation Ax2 − 2Bx + C = 0 one has not only to
choose the appropriate one of the two roots of the equation, but also to choose
194
CHAPTER 6. NONLINEARITIES
the appropriate one of the two solution formulas:
√
B ±
x =
B2 − AC
A
=
√
C
B2 − AC
B ∓
(6.20)
Mathematically the two formulas are equivalent, however numerically there is a
B2 − AC results in ad-
precision loss (which may become very strong) if B ±
dition of two values of opposite sign, or, conversely, subtraction of two values of
the same sign. This consideration yields the following formulas for the solutions
of the quadratic equation:
√
√
B + sgn B ·
A
x1 =
B2 − AC
x2 =
C
B + sgn B ·
√
B2 − AC
2nd-order soft clippers of the most general form
We could generalize the previously used idea of turning the nonlinear zero-delay
feedback equation into a quadratic one by considering a waveshaper made of
the most general form of a second-order curve y = f (x) deﬁned by18
Φ(x, f (x)) = Φ(x, y) = ax2 − 2bxy + cy2 − 2px − 2qy + r = 0
(6.21)
Equation (6.21) has 6 parameters and 5 degress of freedom. After subtituting
the nonlinearity (6.21) into (6.13), the equation (6.13) turns into
(cid:18)
Φ
u,
(x − kS) − u
kG
(cid:19)
= 0
or, equivalently,
k2G2au2 − 2kGbu((x − kS) − u) + c((x − kS) − u)2−
− 2k2G2pu − 2kGq((x − kS) − u) + k2G2r = 0
(6.22)
Obviously, (6.22) is a quadratic equation in respect to u. Particularly, under
the “typical soft clipping curve” conditions
f (0) = 0
f (cid:48)(0) = 1 f (∞) = 1
f (cid:48)(∞) = 0
(6.23)
equation (6.21) turns into a family of hyperbolas: with a single parameter:
2y1 − 1
y2
1
y2 − xy + x − y = 0
(6.24)
Four of ﬁve freedom degrees in (6.21) has been taken by the conditions (6.23).
The ﬁfth remaining degree is represented by the parameter y1, which is the
value19 of y that the curve has at x = 1 (Fig. 6.26). A reasonable choice for
the range of y1 is [0.5, 1], where at y1 = 0.5 we obtain the already familiar
y = x/(1 + x) curve, at y1 = 1 the curve (6.24) turns into a hardclipper.
18We use the implicit form, because the explicit form has some ill-conditioning issues. Be-
sides, in order to solve (6.13) for the specific second-order shaper function f (x), we will need
to effectively go from explicit to implicit form during the algebraic transformations of the
resulting equation anyway, thus using the explicit form wouldn’t have simplified the solution,
but on the contrary, would have made it longer.
19More precisely, one of the two values.
6.7. 2ND-ORDER SATURATION CURVES
195
y
1
0
1
x
Figure 6.26: A family of soft clippers generated by (6.24) for y1 =
0.5, y1 = 0.7829 and y1 = 0.9. The two dashed curves above
the line y = 1 are the second (unused) branches of the respective
curves (the second branch for y1 = 0.5 is not visible because it is
outside the picture boundaries). The thin dashed curve close to
the main branch of the curve for y1 = 0.7829 is the hyperbolic
tangent y = tanh x.
By making the odd extension of the curve:
fext(x) =
(cid:40)
f (x)
−f (−x)
if x ≥ 0
if x ≤ 0
we obtain a proper soft clipping saturator shape, where we should remember to
pick the appropriate branch of the curve, when solving the quadratic zero-delay
feedback equation (6.22).
This time the selection of the appropriate solution of the quadratic equation
is still simple for k ≥ 0, where we can just pick the larger of the two solutions u1,
u2, however for k < 0 it becomes more complicated (Fig. 6.27). From Fig. 6.27
one can see that our choice of the larger or smaller of the two solutions is
switched once when kG changes sign and once again when the oblique asymptote
of kG · f (u)20 goes at −45◦, thereby becoming parallel to to the line v = (x −
kS) − u.21
20It can be shown, that f (u) ∼ u · (cid:0)(2y1 − 1)/y2
1
of the asymptote.
(cid:1)−1 at u → ∞ which defines the steepness
21In the previously discussed case f (u) = u/(1 + u) we didn’t have a switch between larger
and smaller solutions. But f (u) = u/(1 + u) is a limiting case of (6.24) at y1 → 0.5, so why
is there no switch? It turns out that both switches occur simultaneously at kG = 0 (since
196
CHAPTER 6. NONLINEARITIES
By writing out the expressions for the solutions of the resulting quadratic
equation, one could see that, if we deﬁne the choice of the solution in terms of
B2 − AC (which is
the choice of the plus or minus sign in (6.20) in front of
actually what we care about), then the solution is switched only when kG = 0,
at which moment B2 − AC = 0 and respectively both solutions become equal
to each other. The (negative) value of kG, at which the oblique asymptote
of kG · f (u) goes at −45◦, doesn’t correspond to another solution switch but
solely to the unused solution disappearing into the inﬁnity from one side and
reappering from the other.
√
v
v
=(x
−kS)−
u
v = kG · f(u)
v = kG · f(u)
0
u
Figure 6.27: Choice of the solution of the quadratic equation for
f (u) which is a member of the family of hyperbolas (6.24). Solid
line corresponds to kG > 0, dashed lines correspond to two diﬀer-
ent values of kG < 0.
Other 2nd-order saturators
Apparently, mixing in a linear component (6.6) into a saturator deﬁned by (6.21)
still can be expressed in the general form (6.21), thus the zero-delay feedback
equation is still quadratic equation and we can use the same solution techniques.
Instead of using hyperbolas, we could also use parabolas, such as the one in
(6.5) or its mixture with a linear term. Ellipses, having ﬁnite support in terms
the oblique asymptote of f (u) becomes vertical), and thus we simply always choose the larger
solution.
6.8. TABULATION
197
of both x and y, are not lending themselves for this kind of usage, unless used
in a piecewise approximation, which we discuss later.
6.8 Tabulation
Tabulation is one of the standard ways of reducing the computation cost of
functions.
Instead of computing the function using some numerical method
(which might be too expensive) we store function values at certain points in a
lookup table. To compute the function value in between the points, interpolation
(most commonly linear) is used.
Tabulation is worth a dedicated discussion in the context of nonlinear zero-
delay feedback equations, because in this case it can be combined with the
bisection method in a special way, making this combination more eﬃcient. Also
the same ideas provide a general framework for applying piecewise saturation
curves in a zero-delay feedback context, even if the number of segments is so
low that using a real table is not practical.
Imagine the saturator function in Fig. 6.13 was represented by tabulation
combined with linear interpolation, which eﬀectively means that we are having
a piecewise-linear function f (u) (Fig. 6.28).
In order to solve (6.13) we ﬁrst
would need to determine the applicable segment of f (u). Having found the
linear segment we just need to solve a linear zero-delay equation.
v
0
kG · f(u)
u
(x
−kS)−
u
Figure 6.28: The solution of (6.13) for a piecewise-linear saturator.
From Fig. 6.28 is should be clear that the bisection method for a piecewise-
linear curve can be implemented by simply comparing the values of v = (x −
kS) − u and v = kG · f (u) at the breakpoints un, thereby sparing the need
for linear interpolation. We would start with some initial bracketing of the
breakpoint range n ∈ [L, R] and then compare the two curves at the breakpoint
in the middle of the range uM (where M = (L + R)/2, rounding the result of
division by 2 up or down, if necessary). Depending on the comparison outcome
we pick either [L, M ] or [M, R] as the next range. We repeat until we are left
with a single segment, and then simply solve the linear zero-delay feedback
198
equation.22
CHAPTER 6. NONLINEARITIES
The very ﬁrst and very last linear segments will require special care, because
they do not go from one table point to the other, but extend from the outermost
entries of the table to u = ±∞. We can either assume that they horizontally
extend from the ﬁrst and last points in the table, or store their slope separately.
As a very simple example of the just introduced concepts we could consider
a hard clipper
f (x) =



1
x
−1
if x ≥ 1
if −1 ≤ x ≤ 1
if x ≤ −1
(Fig. 6.29). We don’t need a real table to store the breakpoints, but the same
ideas apply. First comparing v = (x − kS) − u and v = kG · f (u) at u = 1 we
ﬁnd out whether the intersection occurs in the right-hand saturation segment
u ≥ 1. If not, then we perform the same comparison at u = −1, thereby ﬁnding
out whether the intersection occurs in the left-hand saturation segment u ≤ −1.
Otherwise the interesection occurs in the middle segment −1 ≤ u ≤ 1.23.
v
0
kG · f(u)
u
(x
−kS)−
u
Figure 6.29: The solution of (6.13) for a hard clipper.
The tabulation approach is not limited to piecewise-linear segments. We
could e.g. use the 2nd-order segments of the form (6.21). Since the latter have 5
degrees of freedom, we could use 4 of those to specify the values of the function
and its ﬁrst derivative at the segments ends (like we would do for a Hermi-
tian interpolating segment and like we did for (6.24)) and use the 5th degree of
freedom e.g. to minimize the remaining error. In fact, in Section 6.7 we have
22Note that the described binary search process doesn’t rely on the regular spacing of
breakpoints un along the u axis. This suggests that we might use an irregular spacing, e.g.
placing the breakpoints more densely in the areas of higher curvature.
Irregularly spaced
breakpoints might complicate the initial bracketing a bit, though.
23Treating the hard clipper as a piecewise-linear shaper is just a demonstration example. For
a hard clipper shape it might be simpler and more practical to simply perform a linearization
at zero (thereby treating the hard clipper as a fully transparent shaper f (u) = u) to find u.
As the very next step after that is sending u through the hard clipper, at the output of the
hard clipper we will get the true value, as if we properly solved the equation (6.13))
6.9. SATURATION IN 1-POLE FILTERS
199
done exactly this, building a piecewise-2nd-order curve consisting of two seg-
ments joined at the breakpoint at the origin. The saturator (6.2b), consisting
of four segments of an order not exceeding 2, could be another candidate for
this approach.
6.9 Saturation in 1-pole filters
The feedback in the 1-pole ﬁlter is not one creating the resonance. Therefore
the discussion from Section 6.3 does not apply and we need to address nonlinear
1-poles separately.
We are going now to discuss nonlinear 1-poles with the nonlinearity ideas de-
rived from diﬀerent analog variations of the 4-pole lowpass ladder ﬁlter discussed
in Section 5.1 These nonlinear 1-pole ﬁlters, however, are of generic nature and
are therefore not limited to the usage inside 4-pole lowpass ladder ﬁlters (or
inside ﬁlters of whatever speciﬁc kind, for that matter).
Transistor ladder’s 1-pole lowpasses
The linear model of transistor ladder discussed in Section 5.1 (Fig. 5.1) is a ﬁrst
level of approximation of the behavior of the respective analog structure, where
we ignore all nonlinear eﬀects. If we wish to take nonlinear eﬀects into account,
we could replace the underlying linear 1-pole lowpasses of the ladder ﬁlter with
nonlinear 1-pole lowpasses, the structure of such nonlinear lowpass being shown
in Fig. 6.30. In terms of the equations, (2.3) is transformed into
y = y(t0) +
(cid:90) t
t0
(cid:0)tanh x(τ ) − tanh y(τ )(cid:1) dt
ωc
(6.25)
The lowpass in (6.25) and Fig. 6.30 is a simple nonlinear model of the underlying
1-pole lowpass of the transistor ladder, directly arising out of the application of
Ebers–Moll transistor model.24
x(t)
tanh
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82)(cid:47)
•(cid:47)
y(t)
tanh
Figure 6.30: A nonlinear 1-pole lowpass element of the transistor
ladder ﬁlter.
Which eﬀect does the change from (2.3) to (6.25) have? Apparently, tanh x−
tanh y has a smaller absolute magnitude compared to x − y, the drop in magni-
tude becoming more noticeable of one or both of the signals x and y is suﬃciently
high. If both x and y have large values of the same sign, it’s possible that the
diﬀerence tanh x − tanh y is close to zero, even though the diﬀerence x − y is
very large. This means that the ﬁlter will update its state more slowly than
24A famous piece of work describing this specific nonlinear model of the transistor ladder
filter is the DAFx’04 paper Non-linear digital implementation of the Moog ladder filter by
Antti Huovilainen. Therefore this model is sometimes referred to as the “Antti’s model”.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
200
CHAPTER 6. NONLINEARITIES
in (2.3). Intuitively this feels like “cutoﬀ reduction” at large signal levels, or,
more precisely this can be seen as audio-rate modulation of the cutoﬀ, where
the cutoﬀ is being changed by the factor
K =
tanh x − tanh y
x − y
0 < K ≤ 1
where the equality K = 1 is attained at x = y = 0.
Connecting 1-poles from Fig. 6.30 in series (Fig. 6.31) can be optimized by
noticing that we don’t need to compute the tanh of the output of the ﬁrst
integrator twice (Fig. 6.32), thus sparing one tanh saturator. The entire ladder
ﬁlter thereby turns into one in Fig. 6.33.
tanh
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82)(cid:47)
•(cid:47)
tanh
tanh
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82)(cid:47)
•(cid:47)
tanh
Figure 6.31: Serial connection of two nonlinear 1-pole lowpass el-
ements from Fig. 6.30.
tanh
(cid:82)(cid:47)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
h
n
a
t
•(cid:15)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82)(cid:47)
•(cid:47)
tanh
Figure 6.32: Optimized serial connection of two nonlinear 1-pole
lowpass elements from Fig. 6.31.
The nonlinear 1-pole in Fig. 6.33 are normally suﬃcient to prevent the ﬁlter
from explosion in selfoscillation range. However, obviously, there is nothing
which should stop us from introducing additional nonlinearities, such as the
ones discussed in Section 6.3, not so much as a means from preventing the
ﬁlter explosion but rather for giving additional color to the sound. Apparently
feedfoward path of Fig. 6.33 already contains many nonlinear elements, therefore
adding nonlinearities to the feedback path could make more sense. Note that
while there are good reasons to keep the saturation levels of nonlinearities in the
feedfoward path of Fig. 6.33 (especially since we are employing the optimization
from Fig. 6.32, which shares one nonlinearity between two 1-pole lowpasses),
there is much less reason to have the same saturation level (or even the same
saturation curve) for the nonlinearity in the main feedback path.
The nonlinear version of the diode ladder ﬁlter (Figs. 5.48, 5.49) is using a
similar kind of nonlinear 1-poles, resulting in a structure shown in Fig. 6.34.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
6.9. SATURATION IN 1-POLE FILTERS
201
x(t)
tanh
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)(cid:49)(cid:49)(cid:49)(cid:79)
k
•(cid:47)
•(cid:47)
•(cid:47)
•(cid:47)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82)(cid:47)
tanh
(cid:82)(cid:47)
tanh
(cid:82)(cid:47)
tanh
(cid:82)(cid:47)
tanh
y1(t)
y2(t)
y3(t)
•(cid:47)
y4(t)
Figure 6.33: Nonlinear transistor ladder ﬁlter.
The equations (5.18) are respectively turned into:
˙y2 =
˙y1 = ωc
ωc
2
ωc
2
ωc
2
˙y4 =
˙y3 =
(cid:0)tanh x − tanh(y1 − y2)(cid:1)
(cid:0)tanh(y1 − y2) − tanh(y2 − y3)(cid:1)
(cid:0)tanh(y2 − y3) − tanh(y3 − y4)(cid:1)
(cid:0)tanh(y3 − y4) − tanh y4
(cid:1)
(compare to (6.25)).
OTA ladder 1-poles
The same idea of the ladder ﬁlter discussed in Section 5.1 and shown in Fig. 5.1
has been often implemented in analog form using OTA (operational transcon-
ductance ampliﬁers) instead of transistors. This generates another kind of non-
linear 1-pole structure (Fig. 6.35).
Formally we are having a feedback loop saturator here. However this feed-
back loop is not responsible for generating the resonance, therefore the eﬀect of
the saturator is diﬀerent from the one discussed in Section 6.3. We are having
a saturator at the integrator’s input, therefore we are performing soft clipping
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
202
x(t)
tanh
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)(cid:49)(cid:49)(cid:49)(cid:79)
k
CHAPTER 6. NONLINEARITIES
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82)(cid:47)
•(cid:47)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
tanh
(cid:82)(cid:47)
tanh
(cid:82)(cid:47)
tanh
(cid:82)(cid:47)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
tanh
y1(t)
y2(t)
y3(t)
•(cid:47)
y4(t)
Figure 6.34: Nonlinear diode ladder ﬁlter.
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
tanh
(cid:82)(cid:47)
•(cid:47)
y(t)
Figure 6.35: OTA-style nonlinear 1-pole lowpass.
on the speed of change of the ﬁlter’s output value, or, equivalently, we are do-
ing “soft slew limiting”. Alternatively, as shown by (6.8), this can be seen as
audio-rate cutoﬀ modulation, the cutoﬀ factor varying in agreement with (6.9).
Note that we have two diﬀerent options for picking the highpass signal in
In the
Fig. 6.35. We could do this either before or after the nonlinearity.
latter case the highpass signal will be saturated (which might be a bit over the
top, compared to the lowpass signal), in the former case we have the beneﬁt of
preserving the relationship HLP(s) + HHP(s) = 1. This also makes the former
option look like a particulary good candidate not only for a nonlinear 1-pole
highpass (and thereby, among other things, for ladder ﬁlter structures utilising
highpasses) but also for a nonlinear allpass. Fig. 6.36 shows the respective
nonlinear 1-pole multimode.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
6.9. SATURATION IN 1-POLE FILTERS
203
•
−(cid:15)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
yHP(t)
yAP(t)
yLP(t)
•(cid:47)
tanh
(cid:82)(cid:47)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 6.36: OTA-style nonlinear 1-pole multimode.
Saturated integration
The previously discussed ways of introduction of nonlinearities into 1-poles re-
sulted in relatively complicated nonlinear behavior of the ﬁlters. But what if
we want a simpler behavior? Let’s say we want to simply saturate the output.
Of course we simply could put a saturator at the output of the ﬁlter (Fig. 6.37)
but this doesn’t really feel like making the ﬁlter itself nonlinear.
(cid:82)(cid:47)
•(cid:47)
tanh
y(t)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 6.37: Putting a saturator at the ﬁlter’s output.
We could try putting the output nonlinearity inside the ﬁlter’s feedback loop
(Fig. 6.38). However, comparing this to equation (2.3) we should realize that
the main eﬀect of such nonlinearity will be that the diﬀerence x − y will be
changed to x − tanh y, leading to the capacitor in Fig. 2.1 continuing to charge
even after the output value has reached the input value. In other words, the
output will still grow even after reaching the input value. This feels more like a
mistake.
(cid:82)(cid:47)
tanh
•(cid:47)
y(t)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 6.38: Saturating the integrator’s output (not really a work-
ing idea).
What we rather want is to prevent the 1-pole’s capacitor in Fig. 2.1 from
charging beyond a certain level (that is we want to prevent the integrator state
from going beyond a certain maximum). In order to achieve that in a “proper
analog way”, we will need to introduce antisaturators, which we are going to
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
204
CHAPTER 6. NONLINEARITIES
do later in this chapter. However we could also do a “hack” and modify the
integrator structure, introducing the saturation into its internal accumulation
process. This works particularly well with direct form I (Fig. 6.39) and trans-
posed direct form II (Fig. 6.40) integrators. Obviously, this hack is not limited
to 1-poles, but can be applied to any structure which is based on integrators,
such as e.g. SVF.
x[n]
ωcT /2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
tanh
•(cid:47)
y[n]
z−1 (cid:111)
Figure 6.39: Saturating direct form I trapezoidal integrator.
x[n]
ωcT /2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
+
tanh
•(cid:47)
y[n]
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 6.40: Saturating transposed direct form II trapezoidal inte-
grator.
6.10 Multinonlinear feedback
We have seen that instantaneous responses of linear ﬁlters are linear functions
of their input, such as e.g. in (3.29). It is not diﬃcult to realize, particularly
from the previous discussion of the solution of the nonlinear zero-delay feed-
back equation (6.12), that instantaneous response of a nonlinear ﬁlter is some
nonlinear function of its input:
y = F (x, S)
(6.26)
(where we also explicitly notated the dependency on the ﬁlter’s state S, but the
dependency of F on the ﬁlter’s parameters is understood implicitly).
Consider the OTA-style 1-pole lowpass in Fig. 6.35 and imagine we build a
4-pole lowpass ladder ﬁlter (as in Fig. 5.1) from four idenitical 1-pole lowpasses
of this kind. Assuming (6.26) decribes the instantaneous response of Fig. 6.35,
we could redraw Fig. 5.1 in the instantaneous response form as Fig. 6.41.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
6.10. MULTINONLINEAR FEEDBACK
x
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
F (x, S1)
F (x, S2)
F (x, S3)
F (x, S4)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
205
•(cid:47)
y
Figure 6.41: Nonlinear ladder ﬁlter in the instantaneous response
form.
Let u denote the signal at the input of the ﬁrst 1-pole lowpass in Fig. 6.41.
The zero-delay feedback equation for the entire of Fig. 6.41 therefore becomes
u = x − k · F (F (F (F (u, S1), S2), S3), S4)
(6.27)
Intuitively we can expect F (x, S) to be monotonically increasing with respect to
x, thus F (F (F (F (x, S1), S2), S3), S4) should be monotonically increasing too,
and we could use most of the previously described methods of solving nonlinear
zero-delay feedback equations to solve (6.27). Theoretically.
Practically the evaluation of F (x, S) is usually very expensive, because it
means a numerical solution of the zero-delay feedback equation for the respective
1-pole, possibly running several rounds of an iterative method. Now, if we
are going to use an iterative method to solve (6.27), these expenses will be
multiplied by the number of the “outer” iterations. Besides, if we are using
Newton–Raphson to solve (6.27) then we need not only to evaluate F (x, S) but
also its derivative with respect to x, which further increases the computation
cost of solving (6.27).
Therefore usually such “nesting” approach, where we express the higher-
level zero-delay feedback equation in terms of the solutions of the lower-level
zero-delay feedback equations, is not very practical for nonlinear systems. In-
stead, let’s “ﬂatten” the entire structure, and write the equation describing the
instantaneous response signals within this structure. E.g. for the 4-pole ladder
built out of 1-poles in Fig. 6.35 the ﬂattened structure is shown in Fig. 6.42. Or,
representing the integrators by their instantaneous responses (which are fully
linear), we obtain Fig. 6.43.
Denoting the input and output signals of each of the 1-poles as xn and yn,
we write the 1-pole zero-delay feedback equations:
yn = g tanh(xn − yn) + sn
Or, since xn+1 = yn we can denote the input of the ﬁrst lowpass as y0 and write
yn = g tanh(yn−1 − yn) + sn
n = 1, . . . , 4
Plus, we are having the global feedback loop:
y0 = x − ky4
and thus we are having an equation system:
y0 = x − ky4
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
206
CHAPTER 6. NONLINEARITIES
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)(cid:49)(cid:49)(cid:49)(cid:79)
k
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
tanh
(cid:82)(cid:47)
tanh
(cid:82)(cid:47)
tanh
(cid:82)(cid:47)
tanh
(cid:82)(cid:47)
•
y(t)
Figure 6.42: Flattened OTA lowpass ladder ﬁlter structure.
y1 = g tanh(y0 − y1) + s1
y2 = g tanh(y1 − y2) + s2
y3 = g tanh(y2 − y3) + s3
y4 = g tanh(y3 − y4) + s4
We can get rid of the ﬁrst equation by simply substituting its right-hand side
for y0, obtaining:
y1 = g tanh(x − ky4 − y1) + s1
y2 = g tanh(y1 − y2) + s2
y3 = g tanh(y2 − y3) + s3
y4 = g tanh(y3 − y4) + s4
(6.28)
Equation (6.28) can be written in a more concise form by introducing the vector
y = (cid:0)y1 y2 y3 y4
(cid:1)T
and the vector-function of a vector argument Φ:
Φ(y) =




g tanh(x − ky4 − y1) + s1
g tanh(y1 − y2) + s2
g tanh(y2 − y3) + s3
g tanh(y3 − y4) + s4




In this notation (6.28) looks simply like
y = Φ(y)
(6.29)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
6.10. MULTINONLINEAR FEEDBACK
207
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)(cid:49)(cid:49)(cid:49)(cid:79)
k
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
tanh
gx + s1
tanh
gx + s2
tanh
gx + s3
tanh
gx + s4
•
y(t)
Figure 6.43: Flattened OTA lowpass ladder ﬁlter structure in the
instantaneous response form.
This is our nonlinear 4-dimensional (since we are having 4 unknowns yn) zero-
delay feedback equation.
The form (6.29) readily oﬀers itself for ﬁxed-point iteration. By rewriting
(6.29) as
Φ(y) − y = 0
the multidimensional form of Newton–Raphson algorithm can be used:
yn+1 = yn −
(cid:18) ∂(Φ(y) − y)
∂y
(cid:19)−1
(yn)
· (Φ(yn) − yn)
Also the quick approximate methods of Section 6.6 work out of the box.
The diﬀerence of solving (6.29) instead of (6.27) is that in (6.29) we are
simultaneously solving all zero-delay feedback equations in the system, thereby
not having the problem of nested iterations.
Actually, choosing the 1-pole output signals as the unknowns is not necessar-
ily the best choice. It would have been more convenient to solve for the inputs
of the integrators, so that we can directly reuse the obtained signals to update
the integrator states.25 On the other hand, e.g. for the transposed direct form
II integrator (Fig. 3.11) one could deduce the new state from the old state and
the new output signal, thus yn also work pretty eﬃciently (this trick has been
used in the digital implementation of an SVF in Section 4.4). A consideration
25It might be a good idea to write the equation system in terms of integrator input signals
as an exercise.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
208
CHAPTER 6. NONLINEARITIES
of a bigger importance therefore could be that the choice of the unknowns may
aﬀect the convergence of the iteration scheme.
Usually for multidimensional zero-delay feedback cases the iterative methods
need to be further reﬁned and/or a combination of diﬀerent methods need to be
used to have a reliable and quick convergence of an iterative process of ﬁnding
the solution of (6.29). However, often simply using the approximate methods
of Section 6.6, will deliver reasonable results.
6.11 Antisaturators
In Section 6.9 we made some attempts to make the 1-pole lowpass ﬁlter state
saturate, the most successful attempt being the modiﬁcation of the internals of
an integrator. In a real analog circuit we wouldn’t have been able to do the
same, as e.g. a capacitor, which is used as an integrator for the current, doesn’t
have “built-in saturation functionality”. Therefore diﬀerent means have to be
used to achieve the integrator state’s saturation.
Diode clipper
A common trick is to shorten the 1-pole ﬁlter’s capacitor with a nonlinear re-
sistance, this resistance being high at low voltages and dropping down at high
voltages on the capacitor. That is the short path is disabled at low voltages but
progressively “turns on” at higher voltages. This can be done by using a diode
pair (Fig. 6.44). The structure in Fig. 6.44 is commonly referred to as diode
clipper.
(cid:128)(cid:129)(cid:129)(cid:129) (cid:130)(cid:240)x(t)
R
(cid:1)
y(t)
(cid:128)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129) (cid:130)(cid:255)(cid:128)(cid:129)(cid:129) (cid:130)(cid:255)(cid:128)(cid:129)(cid:129)(cid:129)(cid:129) (cid:130)(cid:242)
(cid:255)
C
(cid:129)
(cid:254)
(cid:3)
(cid:1)
(cid:254)
(cid:254)
Figure 6.44: Diode clipper.
Using Shockley diode equation we can show that, qualitatively, the current
ﬂowing through the diode pair is related to the capacitor voltage as
ID = Is sinh
UC
UT
where Is and UT are diode parameters (Fig. 6.45 provides a graph of sinh as a
reference). This current is then subtracted from the current which is charging
the capacitor, thus acting as current leakage:
˙qC = I − ID = I − Is sinh
UC
UT
(please refer to equations (2.1) for the other details of the circuit’s model).
Since Is is very small, as long as UC is below or comparable to UT the leakage
(cid:131)
(cid:133)
(cid:132)
(cid:131)
(cid:133)
(cid:131)
(cid:133)
(cid:131)
(cid:133)
(cid:131)
(cid:133)
(cid:131)
(cid:133)
(cid:131)
(cid:133)
(cid:131)
(cid:133)
6.11. ANTISATURATORS
209
is negligible. As UC exceeds UT , the current grows exponentially and quickly
stops being negligible.
y
4
3
2
1
−4
−3
−2
−1
1
2
3
4
x
0
−1
−2
−3
−4
Figure 6.45: Hyperbolic sine y = sinh x.
In terms of the block diagram (Fig. 2.2) this current leakage can be expressed
as shown in Fig. 6.46, where we have assumed that the ﬁlter cutoﬀ is controlled
by the resitance R rather than capacitance C and thus the amount of the current
leakage is independent of the cutoﬀ.
(cid:82)(cid:47)
•(cid:47)
y(t)
x(t)
ωc
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
h
n
i
s
•
Figure 6.46: Diode clipper in the form of a block diagram.
“sinh” stands for some curve of the form “a sinh(x/b)”.
The fact that the leakage current is independent of the cutoﬀ is actually
having the opposite eﬀect: the eﬀects of the leakage become cutoﬀ-dependent
and the leakage more strongly aﬀects the ﬁlter at lower cutoﬀs. Particularly,
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
210
CHAPTER 6. NONLINEARITIES
given a constant input voltage, the stabilized output level will be larger at larger
cutoﬀs. For the purposes of generic application it is therefore more useful to
make the leakage cutoﬀ-independent, as in Fig. 6.47.
ωc
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:82)(cid:47)
•(cid:47)
y(t)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
h
n
i
s
•
Figure 6.47: Diode clipper with cutoﬀ-independent leakage.
“sinh” stands for some curve of the form “a sinh(x/b)”.
Or, using implied cutoﬀ notation and combining the two feedback paths
into a single one, we obtain the structure Fig. 6.48. Also, in Fig. 6.47 the cutoﬀ
parameter ωc was not the true cutoﬀ of the system, since at low signal levels the
gain of the feedback path was 1 + a/b. This made the system behave as if its
cutoﬀ was (1 + a/b)ωc and as if its input signal was reduced by (1 + a/b) factor
at the same time. In Fig. 6.48 we addressed this issue by scaling the linear path
of the feedback by the factor (1 − a/b). This doesn’t change the qualitative
behavior of the system, but aﬀects only the interpretation of the cutoﬀ ωc and
the input signal scale.
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82)(cid:47)
•(cid:47)
y(t)
(cid:0)1 − a
b
(cid:1) x + a sinh x
b
Figure 6.48: Diode clipper with cutoﬀ-independent leakage (sim-
pliﬁed diagram).
The structure in Fig. 6.48 is a good illustration of the idea that we could
employ to introduce saturation into 1-pole lowpass ﬁlter’s state: as sinh(x/b)
grows exponentially for large signals, the term a sinh(x/b) causes the negative
feedback to grow as well, thereby causing the integrator to “discharge”.
The same eﬀect is obviously obtained by putting any other quickly growing
function of a similar shape into the feedback path of a 1-pole lowpass. Good
options for such functions are provided by the inverses of the saturator functions
introduced in Section 6.2:
1
2
ln
1 + x
1 − x
y = tanh−1 x =
y = x/(1 − |x|)
y = sinh x
y = x(1 + |x|)
(inverse of (6.1))
(inverse of (6.2c))
(inverse of (6.4))
(inverse of (6.5))
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
6.11. ANTISATURATORS
211
A particularly important feature of the inverses of the saturators is that, same as
with saturators, they are transparent at low signal levels, thereby not aﬀecting
the cutoﬀ of the ﬁlter.
We will refer to the waveshapers having an inverse saturator kind of shape
as antisaturators. Fig. 6.49 shows another version of Fig. 6.48, this time using
a simpler antisaturator.
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82)(cid:47)
•(cid:47)
y(t)
sinh x
Figure 6.49: Lowpass ﬁlter’s state saturation by using an antisat-
urator.
An antisaturator in Fig. 6.49 is having a similar eﬀect on the ﬁlter’s state
saturation as its inverse (the respective saturator) would have had if directly
applied to a signal, or if being put in a resonating feedback path. Speciﬁcally,
using an unbounded saturator’s inverse as an antisaturator in Fig. 6.49 would
result in an unbounded saturation of the ﬁlter’s state, in the sense that by
making the amplitude of the input signal of the ﬁlter larger and larger one can
achieve arbitrarily large levels of the ﬁlter’s state. On the other hand, using
a bounded saturator’s inverse as an antisaturator (such as e.g. tanh−1) would
result in bounding of the ﬁlter state, the state not being able to exceed the
saturation level.
As with saturators, adding a linear term to an antisaturator f (x) doesn’t
change its antisaturating behavior, but simply weakens it a bit further, where
we assume that the addition should be done under the same considerations of
keeping the transparency at low signal levels:
y = (1 − α)f (x) + αx
(0 < α < 1)
The antisaturator in Fig. 6.48 is a kind of a reverse example of this principle,
which can be seen as if the (otherwise fully linear and transparent) shape y = x
was modiﬁed by an addition of a non-transparent antisaturator a sinh(x/b),
however the resulting curve has been made transparent again.
Antisaturation in SVF
As with 1-pole ﬁlters, the feedback in SVF is also not one creating the resonance,
respectively the discussion from Section 6.3 does not apply either, and thus we
can’t simply put a saturator into the feedback loop. Actually, the purpose of the
feedback in SVF is kind of an opposite of creating the resonance. The function
of the feedback path containing the bandpass signal is to dampen the otherwise
self-oscillating structure. This suggests the idea that if we put an antisaturator
into the bandpass signal path, this might actually do the trick of preventing the
signal levels from getting too high.
Our ﬁrst attempt to do so is shown in Fig. 6.50. After thinking a bit we,
however, realize that it can’t work. Indeed, at R = 0 there is no damping signal
whatsoever, the same as without the antisaturator. Furthermore, probably the
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
212
CHAPTER 6. NONLINEARITIES
main reason to introduce the antisaturator into the SVF is so that we could go
into the selfoscillation range R < 0, same as we did e.g. with nonlinear 4-pole
ladder by going into the range k > 4. However, at R < 0 the introduced anti-
saturator doesn’t cause any damping either, quite on the opposite, it ampliﬁes
the “antidamping” (the inverted damping signal). Obviously, putting the anti-
saturator after the 2R gain element instead of putting it before doesn’t change
much in this regard.
yHP(t)
yBP(t)
•(cid:47)
(cid:82)(cid:47)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82)(cid:47)
•(cid:47)
yLP(t)
•(cid:47)
h
n
i
s
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
2R
+
Figure 6.50: An attempt to introduce an antisaturator into an SVF
(not really working).
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
We could get a bit smarter and connect a saturator in parallel with the 2R
gain element (Fig. 6.51). This now does the job of saturating the signals, as
the damping feedback signal will grow exponentially at large levels of yBP, no
matter what the value of R is. However now the eﬀective gain of the damping
feedback path (at low signal levels, where sinh x ≈ x) is 2R + 1, rather than 2R.
The latter problem is ﬁxed in Fig. 6.52. In this structure, at the neutral
setting of R = 1 the entire damping signal goes through the antisaturator. This
exactly matches the same situation in our ﬁrst attempt in Fig. 6.50 (and is
the reason for the separation of the multiplication by 2 into an additional gain
element). As R gets away from 1, we send some of the damping signal through
the parallel linear path, still keeping the total gain of the damping path equal
to 2R at low signal levels.
The antisaturator in Fig. 6.52 eﬀectively makes the state of the ﬁrst inte-
grator saturate. This might result in the feeling that the level of the bandpass
signal yBP becomes too low. Therefore, instead one could pick the bandpass
signal from yBP(cid:48) output, where the antisaturator has increased the level of yBP
back. The yBP1 output provides the normalized bandpass signal.
Note that yHP + yBP1 + yLP = x, as for the linear SVF.
Zero-delay feedback equation with antisaturators
The introduction of antisaturators raises some new considerations for the solu-
tion of the zero-delay feedback equation. We will use the nonlinear 1-pole in
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
6.11. ANTISATURATORS
213
yHP(t)
yBP(t)
•(cid:47)
(cid:82)(cid:47)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
•(cid:15)
(cid:82)(cid:47)
•(cid:47)
yLP(t)
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
2R
h
n
i
s
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
Figure 6.51: A second attempt to introduce an antisaturator into
an SVF (works better, but R does no longer directly correspond
to damping).
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
yHP(t)
yBP(t)
•(cid:47)
(cid:82)(cid:47)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
•(cid:15)
(cid:82)(cid:47)
•(cid:47)
yLP(t)
h
n
i
s
•(cid:15)
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
R − 1
2
+
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:15)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
yBP(cid:48)(t)
yBP1(t)
Figure 6.52: An SVF with antisaturator.
Fig. 6.49 as a demonstration example, however it will also be more instructive
to consider an inverse hyperbolic tangent (Fig. 6.53) instead of a hyperbolic sine
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
214
CHAPTER 6. NONLINEARITIES
as an antisaturator.
−2
−1
y
2
1
0
−1
−2
1
2
x
Figure 6.53: Inverse hyperbolic tangent y = tanh−1 x.
Introducing the instantaneous response gx + s for the integrator in Fig. 6.49
and replacing sinh with tanh−1 we obtain Fig. 6.54. Writing the zero-delay
feedback equation for Fig. 6.54 we obtain
y = g(x − tanh−1 y) + s
(6.30)
x(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
gx + s
•(cid:47)
y(t)
tanh−1
Figure 6.54: Lowpass ﬁlter with a tanh−1 antisaturator in the
instantaneous response form.
We could start solving (6.30) using the usual methods, such as the ones dis-
cussed earlier in this chapter, however notice that tanh−1 has a limited support,
being deﬁned only on the (−1, 1) range. This might create serious problems if
we somehow arrive at a value of y outside of that range. Such values of y could
appear for a number of reasons, such as e.g.:
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
6.11. ANTISATURATORS
215
- from an approximate solution
- from an iterative method’s step
- from numerical errors, such as roundoﬀs.26
Even if we formally stay within the range y ∈ (−1, 1), we could still get out of
the range of representable values of tanh−1 y if tanh−1 y gets too large.
There are also related questions of convergence of iterative schemes, partic-
ulary of ﬁxed point iteration. Last but not least, close to the boundaries of the
range y ∈ (−1, 1) a small numerical error in the value of y will result in a huge
error in the value of tanh−1 y, which suggests that it might be generally a bad
idea to explicitly evaluate tanh−1 y at all. Similar issues also of course arise with
unbounded antisaturators, even though they are not as bad as with bounded
ones.
In order to avoid this kind of problems, we can solve for the antisaturator’s
output, rather than for the antisaturator’s input. Introducing variable u for the
antisaturator’s output signal:
u = tanh−1 y
we respectively have y = tanh u and can rewrite (6.30) in terms of u as
tanh u = g(x − u) + s
or, further rewriting it so that the linear function in the right-hand side is more
explicitly visible
tanh u = (gx + s) − gu
(6.31)
Equation (6.31) looks very much like the previously discussed zero-delay feed-
back equation (6.13). However, there are still important diﬀerences. Expressing
the left- and right-hand sides of (6.31) graphically in Figs. 6.55 and 6.56, we
see that, compared to Figs. 6.13, 6.14 and 6.15, multiple solutions can occur
already for g < 0. Fortunately, in Fig. 6.54 the value of g cannot get negative,
since that would require a negative cutoﬀ value for the integrator.
Having found u from (6.31) we can “send” it further through the feedback
loop, ﬁrst ﬁnding the integrator’s input value as x − u, then updating the in-
tegrator’s state and ﬁnding y as the output value of the integrator. Note that
thereby we never explicitly evaluated tanh−1 y.
For an antisaturator in the SVF (Fig. 6.52) the situation is more complicated.
We would like to solve for the antisaturator’s output yBP(cid:48), but then we would
be stuck immediately afterwards: since we don’t know the signal on the “R − 1”
path, we can’t add the output signals from sinh and R − 1. Furthermore, we
would have a similar problem of not knowing yLP at the next adder (which
computes yBP1 + yLP). These problems are not unexpected, considering that we
have been solving for a point in the signal path which is not shared among all
zero-delay feedback loops in the structure.
One way around this would be to try to introduce more unknowns into the
system and solve several equations at once. However, in this speciﬁc case we
26Going out of supported range of y due to numerical errors is more likely to happen in
more complicated structures than the one in Fig. 6.54. However since we are using Fig. 6.54
as a demonstration of general principles, we should mention this aspect as well.
216
CHAPTER 6. NONLINEARITIES
v
0
tanh u
u
(gx+s)−gu
Figure 6.55: The solution of (6.31) for g > 0.
v
0
tanhu
(gx+s)−gu
u
Figure 6.56: The solution of (6.31) for g < 0.
could simply “send the obtained signal through the antisaturator in the reverse
direction”. That is, knowing the antisaturator’s output, we can obtain the
antisaturator’s input by evaluating sinh−1 (which is completely okay, we don’t
want to explicitly evaluate the antisaturator function because it can increase
the computation error by a huge factor, but it is no problem to evaluate its
inverse), thereby ﬁnding the value of yBP. The signal yBP is shared among all
zero-delay feedback loops and therefore is suﬃcient to ﬁnd all other signals in
the structure.27
The general approach of avoiding the explicit evaluation of antisaturators
but rather dealing with their inverses instead also allows us to deal with a
27Of course, we should remember that we already know the output signal of the antisaturator
and not attempt to evaluate it again as sinh yBP, which was the whole point of solving for
yBP(cid:48) .
6.12. ASYMMETRIC SATURATION
217
certain class of antisaturators which are not functions in the normal sense. An
example of this are compact-range monotonic saturators such as (6.2b). The
inverse of such saturator is not really a function, since it would have inﬁnitely
many diﬀerent values at x = ±1 (Fig. 6.57). However we still can use it as
an antisaturator, since we never have to deal with the antisaturating function
explicitly, but are dealing with the respective saturating function instead.28
−2
−1
y
2
1
0
−1
−2
1
2
x
Figure 6.57: The inverse of (6.2b) is not a function in the normal
sense.
6.12 Asymmetric saturation
The saturators which we have been using so far were all having the odd symme-
try f (−x) = −f (x). A feature of all symmetric saturators is that when its input
signal amplitude is very high, the output signal basically alternates between pos-
itive and negative saturation levels f (x). If the input signal is something like a
sine or a sawtooth, the saturator would produce a square-like output. More gen-
erally, such saturators tend to produce signals containing mostly odd harmonics
(as the square wave does).
Sometimes this domination of odd harmonics can become too boring29 and
28Note that thereby we can even use an antisaturator which is an inverse of hard clipper.
29More likely so for a “standalone” saturator being used as an overdrive effect, rather than
for a saturator used in a complicated feedback loop structure in a filter.
218
CHAPTER 6. NONLINEARITIES
asymmetric saturation might be desired. Simply adding an oﬀset to the satura-
tor’s input (or, instead, performing a parallel translation of the saturator curve
by “sliding” it through the origin, to keep the property f (0) = 0) works only
for signals of average levels. At high signal levels the same square would be
produced for e.g. a sine or a sawtooth input.
The oﬀset idea would have worked, though, if the oﬀset had been somehow
made proportional to the input signal’s amplitude.30
It turns out that this
is a natural feature of a particular nonlinear 1-pole construct. Consider the
OTA-style nonlinear 1-pole in Fig. 6.36 and imagine that instead of a saturator
nonlinearity we have used the following shaper function:
f (x) =
(cid:40)
2x
x/2
if x ≥ 0
if x ≤ 0
(6.32)
(Fig. 6.58 illustrates). This would mean that whenever yHP = x − yLP > 0, the
cutoﬀ is eﬀectively doubled. When yHP = x − yLP < 0, the cutoﬀ is eﬀectively
halved. Therefore the integrator state will be more “willing” to change in the
positive direction than in the negative one.
y
1
0
−1
−2
−1
1
2
x
Figure 6.58: “Asymmetric cutoﬀ” nonlinearity.
Imagine such ﬁlter receives a steady periodic signal with a zero DC oﬀset
(meaning that the average value of the signal is zero, or, in other words, there is
an “equal amount” of signal above and below zero). And suppose this signal’s
fundamental frequency is well above the nominal cutoﬀ of the ﬁlter. In such
case a linear lowpass ﬁlter would have performed a kind of averaging of the
input signal, thereby producing a zero output signal.31 However in the case of
using the nonlinearity (6.32) the positive input values will have “more weight”
than the negative ones and the lowpass output will be nonzero.
30Clearly, by “amplitude” here we don’t mean the momentary value of the signal but rather
some kind of average or maximum.
31Formally the filter would have produced the DC offset of the signal at the output. The
fundamental and all other harmonics, being way above filter’s cutoff, would have been filtered
out.
6.12. ASYMMETRIC SATURATION
219
It should be inituitively clear that the lowpass output value will increase as
the input signal amplitude increases and vice versa (particularly it should be
obvious that in the case of the zero amplitude of the input signal the output
signal will also be zero). Therefore, qualitatively such lowpass ﬁlter works as an
envelope follower, the ﬁlter cutoﬀ in a way controlling the envelope follower’s
response time. Respectively, the highpass output will contain the input signal
with an added (or subtracted) DC oﬀset, such oﬀset being approximately pro-
portional to the input signal’s amplitude. This means that if initially 50% of
the signal were above zero and the other 50% below zero, we now have changed
this ratio to something like 80% to 20%, and this eﬀect is happening more or
less at any amplitude of the input signal.
Thus, asymmetric nonlinear shaping could be produced by the following
structure:
HPlin
HPnl
g
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(f )x
where HPlin is an initial lowpass, killing the DC oﬀset which might be previ-
ously contained in the input signal, HPnl is the asymmetric nonlinear highpass,
introducing the DC oﬀset into the signal, the signal is then boosted by the gain
g, controlling the amount of “drive”, and f (x) is a usual symmetric saturator.
The nonlinearity (6.32) has a drawback that it contains a discontinuity in
the 1st derivative at x = 0. Such discontinuity may add a noticeable amount
of new harmonic content into the signal. This eﬀect might be desired at times,
but for now we would rather at least reduce it, if not avoiding it altogether,
as the ﬁlter’s main purpose is to introduce the DC oﬀset into the signal. This
can be achieved by smoothing the discontinuity. E.g. we could replace (6.32)
with a hyperbola going at 45◦ through the origin, but having a similar to (6.32)
asymptotic behavior (Fig. 6.59).
y
1
0
−1
−2
−1
1
2
x
Figure 6.59: Replacing the nonlinearity (6.32) (Fig. 6.58) (dashed
line) by a hyperbola.
This kind of nonlinear highpass can occur easily in analog circuits, if non-
linear resistances are involved. E.g. consider Fig. 6.60. The eﬀective resistance
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
220
CHAPTER 6. NONLINEARITIES
connected in series with the capacitor varies between approximately R1 and R2
depending on the polarity of the output voltage. Respectively the cutoﬀ varies
between 1/R1C and 1/R2C.
(cid:128)(cid:129)(cid:129)(cid:129) (cid:130)(cid:240)x(t)
(cid:129)
C
y(t)
(cid:255)
(cid:128)(cid:129)(cid:129)(cid:129)(cid:129) (cid:130)(cid:255)(cid:128)(cid:129)(cid:129)(cid:129)(cid:129) (cid:130)(cid:242)
R1
(cid:1)
(cid:254)
(cid:3)
(cid:1)
(cid:254)
R2
Figure 6.60: Highpass ﬁlter with asymmetric cutoﬀ.
6.13 Antialiasing of waveshaping
Aliasing
When a signal goes through a waveshaper, the waveshaping introduces addi-
tional partials into the spectrum of the signal. These partials extend into the
entire frequency range ω ∈ [0, ∞) for almost any waveshaper. We can show that
in several steps.
First, let’s consider waveshapers of the form f (x) = xn (where n > 1),
starting with f (x) = x2. Let x(t) be a periodic signal. Therefore it can be
represented as a sum of its harmonics:
x(t) =
N
(cid:88)
n=−N
Xnejnωt
where N can be ﬁnite or iniﬁnity. Note that we are using complex-form Fourier
series, therefore, assuming a real x(t), we have an equal number of positive- and
negative-frequency partials. Then
y(t) = f (x(t)) = (x(t))2 =
(cid:32) N
(cid:88)
n=−N
(cid:33)2
Xnejnωt
=
N
(cid:88)
=
n1,n2=−N
Xn1Xn2ej(n1+n2)ωt =
2N
(cid:88)
n=−2N
Ynejnωt
(6.33)
Thus, the frequencies of partials of y(t) are all possible sums n1ω + n2ω of
frequencies of partials of x(t).32 Respectively the frequencies of the partials of
y(t) vary between −2N ω and 2N ω. That is the width of the spectrum of y(t)
is twice the width of the spectrum of x(t).
32Or, if we think in terms of real-form Fourier series, where only positive-frequency partials
are present, the frequencies of partials of y(t) are all possible sums and differences n1ω ± n2ω
of frequencies of partials of x(t).
(cid:131)
(cid:133)
(cid:132)
(cid:132)
(cid:131)
(cid:133)
(cid:131)
(cid:133)
(cid:131)
(cid:133)
(cid:131)
(cid:133)
6.13. ANTIALIASING OF WAVESHAPING
221
For f (x) = x3 we obtain
y(t) = f (x(t)) = (x(t))3 =
(cid:32) N
(cid:88)
n=−N
(cid:33)3
Xnejnωt
=
N
(cid:88)
=
n1,n2,n3=−N
Xn1 Xn2Xn3ej(n1+n2+n3)ωt =
3N
(cid:88)
n=−3N
Ynejnωt
that is the width of the spectrum is tripled. It’s not diﬃcult to generalize it to
an arbitrary power of x, concluding that f (x) = xn increases the width of the
spectrum of x(t) n times.
It should be clear by now that, if f (x) is a polynomial of order N :
f (x) = a0 + a1x + a2x2 + . . . + aN xN
the highest-order term xN will expand the spectrum of x n times, while the
lower-order terms will also expand the spectrum of x(t) but not as much, thus
f (x) expands the spectrum of x(t) N times.
Now suppose f (x) is a function of a more or less general form, expandable
into Taylor series around x = 0:
f (x) =
∞
(cid:88)
n=0
f (n)(0)
n!
xn =
∞
(cid:88)
n=0
anxn
We can consider such f (x) as a polynomial of an inﬁnite order and thus f (x)
expands the spectrum of x(t) by an inﬁnite number of times.33
Some of the waveshapers that we were previously discussed were constructed
as piecewise functions, including e.g. a piecewise polynomial saturator (6.2b).
Would the saturator (6.2b), which consists of polynomial segments of order not
higher than 2, thereby expand the spectrum of f (x) only 2 times? It turns
out that such piecewise function waveshapers also expand the spectrum an inﬁ-
nite number of times. Having discontinuous derivatives themselves (e.g. (6.2b)
has a continuous 1st derivative, but three discontinuities of the 2nd derivative),
such waveshapers also introduce discontinuous derivatives into their output sig-
nal y(t). The presence of discontinuities in a signal’s derivative automatically
implies an inﬁnite spectrum of the signal.34
Therefore all waveshapers which we have been considering until now (as well
as most of the ones we could even think of) expand the spectrum of the input
signal an inﬁnite number of times. This means that discrete-time waveshaping
produces aliasing.
Indeed, suppose we are given a waveshaper f (x) of a general shape, so that
it expands the spectrum of its input signal an inﬁnite number times. And
33If the Taylor expansion of f (x) has a finite convergence radius, we still can make the same
argument about spectrum expansion, at least for the signals x(t) which are small enough to
fit into the convergence radius of the Taylor series of f (x). Note that we also could expand
f (x) not around x = 0 but around some other point x = x0, making the same consideration
applicable for signals x(t) centered around x = x0.
34A discontinuity of N -th order derivative generates harmonics rolling off as 1/nN +1. Thus
a discontinuity in a 2nd derivative generates harmonics at 1/n3. A discontiuity in the function
itself (0th derivative) generates harmonics at 1/n (Fourier series of sawtooth and square signals
are examples of that).
222
CHAPTER 6. NONLINEARITIES
imagine we are having a sampled signal x[n] and its corresponding continuous
bandlimited version x(t). Assuming unit sampling period T = 1 we can write
x[n] = x(n) ∀n ∈ Z. A direct application of a waveshaper f (x) to discrete-time
signal x[n]:
y[n] = f (x[n])
(6.34)
is fully equivalent to sampling the continuous-time signal y(t) = f (x(t)), by
simply letting y[n] = y(n). However, since f (x) expands the spectrum of x(t)
inﬁnitely, the spectrum of y(t) is not bandlimited and simply letting y[n] = y(n)
will result in aliased frequencies contained in y[n].
Trying to use polynomial waveshapers doesn’t help much. We could deﬁ-
nitely construct polynomial antisaturators, e.g. f (x) = x3 + x, whereas a purely
polynomial saturator could be constucted only if we know that the input signal
has a limited range, which is a pretty heavy restriction. However even x3 + x
will triple the width of the spectrum, so that we’ll need e.g. to bandlimit x(t)
to one third of the Nyquist frequency, process it by an f (x) = x3 saturator and
add the result to the unprocessed signal:
BL/3
x3
•(cid:47)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(where BL/3 denotes a ﬁlter which bandlimits the signal to 1/3 of the Nyquist
frequency, and Lat denotes). Actually bandlimiting will introduce latency into
the signal, so we’ll need to add the same latency on the lower path
BL/3
x3
•(cid:47)
Lat
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(where Lat denotes a structure which artiﬁcially introduces the same latency as
introduced by BL/3).
This idea stll doesn’t work really well, since antisaturators are normally used
in feedback loops. We can’t perform the bandlimiting inside the feedback loop,
e.g.
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82)(cid:47)
•(cid:47)
x3 (cid:111)
BL/3 (cid:111)
•(cid:111)
Lat
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
because of the introduced latency. Doing it outside the feedback loop is also
problematic. For one, we’d need to bandlimit the entire signal x(t), not only
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:111)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
6.13. ANTIALIASING OF WAVESHAPING
223
the part of it which goes through the x3 shaper. This still could be done,
though, if the sampling rate is suﬃciently high (at least 3 × 44.1kHz). The
other problem is that the signal inside the feedback loop will go inﬁnitely many
times through the waveshaper. Therefore bandlimiting of the signal prior to
entering the feedback loop to 1/3 of Nyquist frequency won’t really prevent the
aliasing from happening.35
Antialiasing
The antialiasing of waveshapers is a diﬃcult problem, not having a universally
good solution at the time of writing this text. The only thing which is more or
less guaranteed to work is heavy oversampling.36 Unfortuntately, oversampling
introduces latency, thus, if e.g. a waveshaper is used in a ﬁlter feedback loop, we
cannot oversample locally just the waveshaper, but at least the entire feedback
loop must be oversampled.
There is however an approach37 which reduces aliasing by a noticeable
amount, so that the same quality of sound can be achieved at lower sampling
rates than otherwise.38 Suppose we are having a discrete-time signal x[n] going
through a waveshaper f (x). Instead of sending x[n] through the waveshaper in
discrete time, thereby producing the discrete time signal
y[n] = f (x[n])
(6.35)
let’s convert x[n] to continuous time by means of linear interpolation. Without
loss of generality we will consider the linear interpolating segment going between
x[0] and x[1]:
x(t) = (1 − t)x[0] + tx[1]
0 ≤ t ≤ 1
(6.36)
(where we assume unit sampling period T = 1). Applying the waveshaper f (x)
in contnuous time to this segment we obtain
y(t) = f (x(t)) = f ((1 − t)x[0] + tx[1])
Now we propose to compute the discrete time sample y[1] as
y[1] =
(cid:90) 1
0
y(τ ) dτ =
(cid:90) 1
0
f ((1 − τ )x[0] + τ x[1]) dτ =
F (x[1]) − F (x[0])
x[1] − x[0]
(6.37)
where F (x) is some antiderivative of f (x), that is F (cid:48)(x) = f (x). Or, more
generally
y[n] =
(cid:90) n
n−1
y(τ ) dτ =
F (x[n]) − F (x[n − 1])
x[n] − x[n − 1]
(6.38)
35However, it still might reduce the amount of aliasing.
36Higher sampling rates lead to smaller relative increments of integrator states (at the same
cutoff value in Hz). Thus, at some point higher computation precision will be required. 32
bit floats might happen to become insufficient pretty quickly, but 64 bit floats should still do
in a wide range of high sampling rates.
37The approach was proposed independently by A.Huovilainen, E.Le Bivic, Dr. J.Parker
and possibly others. The application of the approach within zero-delay feedback context has
been developed by the author.
38Still, 44.1kHz would be usually insufficient and one will need to go to 88.2kHz or even
higher.
224
CHAPTER 6. NONLINEARITIES
where y(t) = f (x(t)) and where x(t) is a piecewise linear continuous-time func-
tion arising out of linear interpolation of x[n]. It might seem that the averaging
of y(t) on t ∈ [n − 1, n] in (6.38) is a somewhat arbitrary operation. However, it
isn’t. In fact, such averaging can be considered as one of the simplest possible
forms of lowpass ﬁltering the continuous-time signal, aiming to suppress the
aliasing frequencies above Nyquist.39
The averaging (6.38) does a reasonable job of reducing the aliasing in y[n]
latency and ill-
compared to (6.35), however it is introducing two problems:
conditioning.
Latency
Assuming the transparency of the waveshaper at small signal levels f (x) ≈ x
we have F (x) ≈ x2/2 and (6.38) turns into
y[n] =
x2[n]/2 − x2[n − 1]/2
x[n] − x[n − 1]
=
x[n] + x[n − 1]
2
(6.39)
The expression (6.39) describes a discrete time 1-pole lowpass ﬁlter with a cutoﬀ
at half the Nyquist frequency. Indeed, let’s take the lowpass ﬁlter in Fig. 3.31.
At g = 1 (which corresponds to ωcT /2 = 1, which in turn corresponds to
prewarped half Nyquist frequency ωcT /2 = π/4) we have g/(g + 1) = 1/2 and
thus
v[n] =
x[n] − s[n]
2
y[n] = v[n] + s[n] =
x[n] + s[n]
2
s[n + 1] = y[n] + v[n] = x[n]
(6.40a)
(6.40b)
(6.40c)
Combining (6.40b) and (6.40c) we obtain
y[n] =
x[n] + x[n − 1]
2
which is the same as (6.39).
The lowpass ﬁltering eﬀect of (6.38) is actually another problem that we
didn’t mention so far. It arises out of the approximations that the method does
when converting from discrete-time signal x[n] to x(t) and back from y(t) to
y[n]. This problem is however not very noticeable at sampling rates of 88.2kHz
and higher. So, let’s concentrate on the latency introduced by (6.39).
The averaging in (6.39) can be seen as a mid-way linear interpolation between
x[n] and x[n − 1] and thus intuitively one could expect that it introduces a half-
sample delay. This is indeed the case. Taking x[n] = ejωn and assuming |ω| (cid:28) 1,
so that the signal’s frequency is far below the cutoﬀ of the lowpass ﬁlter (6.39),
we have
y[n] =
ejωn + ejω(n−1)
2
(cid:18)
= exp jω
n −
(cid:19)
1
2
·
ejω/2 + e−jω/2
2
=
39Similarly, linear interpolation can be interpreted in terms of continuous-time lowpass
filtering which suppresses the aliasing discrete time spectra.
6.13. ANTIALIASING OF WAVESHAPING
225
(cid:18)
= exp jω
n −
(cid:19)
1
2
· cos
ω
2
(cid:18)
≈ exp jω
n −
(cid:19)
1
2
where cos(ω/2) ≈ 1 since ω ≈ 0.
It can be shown that the source of this half-sample delay is the averaging of
y(t) on [n − 1, n] done in (6.38). Taking y(t) = ejωt where |ω| (cid:28) 1, we have
(cid:90) n
n−1
ejωτ dτ =
ejωn − ejω(n−1)t
jω
(cid:18)
(cid:19)
= exp jω
n −
(cid:18)
≈ exp jω
n −
1
2
1
2
(cid:19)
(cid:18)
= exp jω
n −
(cid:19)
·
1
2
ejω/2 − e−jω/2
2j · ω/2
(cid:19)
(cid:18)
1
2
=
ω
2
≈
= exp jω
n −
· sinc
·
sin(ω/2)
ω/2
(where sinc x = sin x
is the cardinal sine function). Thus (6.39) and (6.38)
x
indeed introduce a delay of half sample. Inside a zero-delay feedback loop this
would be a serious problem. In order to develop an idea of how to address this
problem, let’s look at a few examples.
Waveshaper followed by an integrator
Suppose a waveshaper is immediately preceding an integrator, as shown in
Fig. 6.61 (this particularly happens in the OTA-style 1-poles in Figs. 6.35 and
6.36). Normally we recommended to use transposed direct form II integrators
however this time we suggest to use a direct form I integrator (Fig. 6.62). Look-
ing at the ﬁrst half of the direct form I integrator (highlighted by the dashed
line in Fig. 6.62) we can notice that it exactly implements the formula (6.39).
So, the ﬁrst part of the integrator implements a half-sample delay too and does
so in exactly the same way as the antialiased waveshaper for low-level signals.
This therefore leads to an idea to simply drop this part of the integrator, as it is
done in Fig. 6.63, since we are getting the same half-delay from the waveshaper
already.
f (x)
(cid:82)(cid:47)
Figure 6.61: Waveshaper immedidately followed by an integrator.
It is interesting to notice that what thereby remains of the integrator is the
native integrator contained in Fig. 3.3). Thus, in order to implement an an-
tialiased waveshaper followed by a trapezoidal integrator, simply use a naive
integrator instead. This eﬀectively produces trapezoidal integrator, simultane-
ously “killing” the unwanted latency produced by the antialiased waveshaper.
The solution proposed in Fig. 6.63 works quite well. There is still one sub-
tlety though, which, depending on the circumstances, may be fully academic
or not. By “assigning” the functionality of the ﬁrst part of the integrator to
the antialiased waveshaper, we eﬀectively positioned the ωcT gain element into
the middle of the integrator (Fig. 6.64). This doesn’t aﬀect the time-invariant
behavior of the integrator, but will introduce some changes if the cutoﬀ ωc is
varying.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
226
CHAPTER 6. NONLINEARITIES
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
f (x)
ωcT
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
z−1 (cid:111)
z−1
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
Figure 6.62: Antialiased waveshaper combined with direct form I
integrator. The dashed line highlights the part of the integrator
which is equivalent to the waveshaper at low signals.
f (x)
ωcT
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1 (cid:111)
Figure 6.63: Antialiased waveshaper combined with direct form I
integrator, the ﬁrst part of the integrator being dropped, since its
implemented by the antialiased waveshaper already.
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
•(cid:47)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1 (cid:111)
Figure 6.64: Structure in Fig. 6.63 implies positioning the ωcT gain
element in the middle of the integrator. The dashed line highlights
the part which is being replaced by the antialiased waveshaper.
In order to avoid that eﬀect, we would need to somehow include the varying
ωcT into the averaging implemented by (6.38). A straightforward possibility
would be to change (6.37) into
u[1] =
(cid:90) 1
0
ωc(τ )y(τ ) dτ
(6.41)
(remember that we assume T = 1), where u(t) = ωc(t)y(t) = ωc(t)T y(t). Trape-
zoidal integration assumes (kind of) that the signals are varying linearly in
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
6.13. ANTIALIASING OF WAVESHAPING
227
between the samples, therefore (6.41) can be rewritten as
u[1] =
=
(cid:90) 1
0
(cid:90) 1
0
(cid:0)(1 − τ )ωc[0] + τ ωc[1](cid:1)y(τ ) dτ =
(cid:0)(1 − τ )ωc[0] + τ ωc[1](cid:1)f ((1 − τ )x[0] + τ x[1]) dτ
(6.42)
Unfortunately, the formula (6.42) is not fully convincing. At f (x) = x we would
expect (6.42) to turn into ordinary trapezoidal integration of ωcf (x) yielding
ωc[0]f (x[0]) + ωc[1]f (x[1])
2
However (6.42) gives in this case
ωc[0] + ωc[1]
2
·
f (x[0]) + f (x[1])
2
Of course, (6.42) can be further artiﬁcially amended. Whether one should at-
tempt anything like that, is an open question.
Waveshaper following an integrator
The opposite order of connection of a waveshaper and an integrator looks much
better at ﬁrst sight, since in this case we could use a transposed direct form I
integrator (Fig. 6.65), which won’t require us to reposition the cutoﬀ gain.40
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
•(cid:47)
z−1 (cid:111)
z−1
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
1/2
f (x)
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
Figure 6.65: Transposed direct form I trapezoidal integrator fol-
lowed by a waveshaper. The dashed line highlights the part of the
integrator which is about to be dropped.
A concern which this approach is raising though, is that, as we have seen,
the latency introduced by the waveshaper is caused by the averaging occurring
after the nonlinearity, whereas in Fig. 6.65 the averaging in the integrator,
which we are dropping, is occurring before the nonlinearity. On the other hand,
linear interpolation, which is used to construct x(t) from x[n] in (6.36), is also
having a lowpass ﬁltering eﬀect similar to the one of the averaging, while it
doesn’t actually matter, whether we compensate the latency before or after the
waveshaper. Therefore Fig. 6.65 may also provide an acceptable solution.
40Technically the integrator in Fig. 6.65 is, formally, not exactly a transposed direct form
II integrator, as the 1/2 gain element should have been positioned in the middle. However,
since this is a constant gain, we can shift it without causing the same concerns as in the case
of shifting the potentially varying ωcT gain element.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
228
CHAPTER 6. NONLINEARITIES
Waveshaper followed by a 1-pole lowpass
Let’s now consider the case of the feedback saturator in Fig. 6.6, where the
saturator is not exactly followed by an integrator, but by a complete 1-pole?
Fig. 6.66 depicts this situation explicitly showing the internal structure of the
1-pole.
(cid:82)(cid:47)
•(cid:47)
f (x)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 6.66: Waveshaper followed by a 1-pole lowpass.
Replacing the integrator with its direct form I implementation we obtain the
structure in Fig. 6.67. Following the approach of Fig. 6.64, we reposition the
ωcT element, as shown in Fig. 6.68.
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
ωcT
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
f (x)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
z−1 (cid:111)
•(cid:47)
•(cid:47)
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
Figure 6.67: Waveshaper followed by a 1-pole lowpass built around
a direct form I integrator.
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
f (x)
•(cid:47)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
•(cid:47)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1 (cid:111)
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
Figure 6.68: Structure from Fig. 6.67 with a changed position of
the cutoﬀ gain.
Now we would like to drop the (1+z−1)/2 part of the direct form I integrator,
but only for the signal coming from the waveshaper. The feedback signal of the
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
6.13. ANTIALIASING OF WAVESHAPING
229
1-pole should still come through the full integrator. This can be achieved by
injecting the waveshaped signal into a later point of the feedback loop. The
resulting structure in Fig. 6.69 thereby compensates the latency introduced by
the antialiased waveshaper.
f (x)
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
•(cid:47)
(cid:13)(cid:13)(cid:13)(cid:49)(cid:49)(cid:49)(cid:79)
−1
z−1
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1 (cid:111)
•(cid:47)
•(cid:47)
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
Figure 6.69: Structure from Fig. 6.68 with the waveshaped signal
bypassing the ﬁrst part of the direct form I integrator (thereby
compensating the introduced latency).
The structure in Fig. 6.69 can be further simpliﬁed as shown in Fig. 6.70,
where we “slid” the inverter “−1” all the way through (1 + z−1)/2 to the injec-
tion point of the waveshaped signal. Such change doesn’t cause any noticeable
eﬀects.41 Noticing that the two z−1 elements in Fig. 6.70 are actually sharing
the same input signal, we can combine both into one (Fig. 6.71).
f (x)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
•(cid:47)
•(cid:47)
z−1 (cid:111)
Figure 6.70: Structure from Fig. 6.69 with a changed position of
the inverter.
Fig. 6.71 contains a zero-delay feedback loop, which can be resolved. Let’s
introduce helper variables u, v and s as shown in Fig. 6.71 and let g = ωcT .
Writing the equations implied by the block diagram we have
(cid:18)
v = g ·
u −
(cid:19)
v + s + s
2
41The internal state stored in the first z−1 element is inverted compared to what it used to
be, but this is compensated by the new position of the inverter.
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
230
CHAPTER 6. NONLINEARITIES
x[n]
f (x)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
1/2
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
u
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
v
+
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
s
z−1 (cid:111)
•(cid:47)
•(cid:47)
y[n]
Figure 6.71: Structure from Fig. 6.70 with merged z−1 elements.
from where
v · (1 + g/2) = g · (u − s)
v =
g
1 + g/2
· (u − s)
Considering that y = v + s we obtain the structure in Fig. 6.72.
x[n]
u
f (x)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
ωcT
1+ωcT /2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
y[n]
v
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
z−1 (cid:111)
s
Figure 6.72: Structure from Fig. 6.71 with resolved zero-delay feed-
back loop, implementing Fig. 6.66 with latency compensation.
Notice that the obtained structure in Fig. 6.72 is pretty much identical to
the structure of the naive 1-pole lowpass ﬁlter in Fig. 3.5, except that the cutoﬀ
gain is not ωcT but ωcT /(1+ωcT /2). Thus, in order to implement an antialiased
waveshaper followed by a 1-pole lowpass, we simply use a naive 1-pole lowpass
with adjusted cutoﬀ instead, which eﬀectively “kills” the unwanted latency.
In principle we could have tried to avoid the repositioning of the ωcT gain
element. Attempting to do so, we could have gone from Fig. 6.67 to the structure
in Fig. 6.73. However, this solves only one half of the problem, namely ﬁxing
the issue in the feedback path, while the issue is still there for the waveshaped
signal. The considerations of possibly including the averaging of ωcT into the
antialiased waveshaper apply, where we are having exactly the same situation
as in the case of an integrator following a waveshaper.
1-pole lowpass followed by a waveshaper
In case of a 1-pole lowpass ﬁlter followed by a waveshaper (Fig. 6.74) we can use
the transposed direct form I integrator, as we did in Fig. 6.65. The respective
structure is shown in Fig. 6.75.
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
6.13. ANTIALIASING OF WAVESHAPING
231
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT
•(cid:47)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT
1/2
f (x)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1
•(cid:47)
•(cid:47)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
z−1 (cid:111)
Figure 6.73: Structure from Fig. 6.67 with the waveshaped signal
bypassing the ﬁrst part of the direct form I integrator (thereby
compensating the introduced latency) but without repositioning
of ωcT gain element.
(cid:82)(cid:47)
•(cid:47)
f (x)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 6.74: 1-pole lowpass followed by a waveshaper.
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
ωcT
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
•(cid:47)
z−1 (cid:111)
z−1
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
•(cid:47)
f (x)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
Figure 6.75: 1-pole lowpass built around a transposed direct form
I integrator followed by a waveshaper.
In this case we can simply pick up the waveshaper input signal in the middle
of the integrator, bypassing the second half (Fig. 6.76). Noticing that the two
z−1 elements in Fig. 6.76 are picking up the same signal, we could merge them
into a single z−1 element as shown in Fig. 6.77, thereby producing a direct form
II integrator (compare to Fig. 3.9).
In order to resolve the zero-delay feedback loop in Fig. 6.77 we introduce
helper variables u, v and s as shown in Fig. 6.77 and we let g = ωcT . Then,
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
232
CHAPTER 6. NONLINEARITIES
ωcT
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
f (x)
•(cid:47)
•(cid:47)
•(cid:47)
z−1 (cid:111)
z−1
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
1/2
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 6.76: Structure from Fig. 6.75 with the waveshaper skipping
the second half of the transposed direct form I integrator (thereby
compensating the introduced latency).
(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)(cid:95)
f (x)
y[n]
x[n]
u
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcT
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
1/2
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
v
•(cid:47)
z−1
s
•(cid:15)
(cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95) (cid:95)
Figure 6.77: Structure from Fig. 6.76 with merged z−1 elements.
The dashed line highlights the direct form II integrator.
writing the equations implied by the block diagram, we have
from where
(cid:18)
u = g ·
x −
(cid:19)
u + s + s
2
u · (1 + g/2) = g · (x − s)
u =
g
1 + g/2
· (x − s)
Considering that v = u + s we obtain the structure in Fig. 6.78.
Notice that the obtained structure in Fig. 6.78 is identical to the structure
in Fig. 6.72, except for the the opposite order of the naive 1-pole lowpass and
the waveshaper. Thus, in order to implement a 1-pole lowpass followed by an
antialiased waveshaper we simply use a naive 1-pole lowpass with adjusted cutoﬀ
instead.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
6.13. ANTIALIASING OF WAVESHAPING
233
x[n]
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
ωcT
1+ωcT /2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
u
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:111)
z−1 (cid:111)
s
v
•(cid:47)
f (x)
y[n]
Figure 6.78: Structure from Fig. 6.77 with resolved zero-delay feed-
back loop, implementing Fig. 6.74 with latency compensation.
Other positions of waveshaper
In Fig. 6.66 we had a waveshaper followed by a lowpass, but imagine it was a
highpass instead (Fig. 6.79).42 In this case, even if we use the tricks similar
to the ones we did in the lowpass case, we still won’t be able to eliminate the
latency on the feedforward path between x(t) and y(t).
x(t)
f (x)
•(cid:47)
y(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82) (cid:111)
Figure 6.79: Waveshaper followed by a 1-pole highpass.
If there is e.g. a lowpass further after the the highpass:
f (x)
HP
LP
then we can eliminate the latency by changing the lowpass, exactly as we did
before. The highpass ﬁlter will work on a signal delayed by half a sample, but
this will be compensated in the immediately following lowpass. Similarly, if
there is a preceding lowpass:
LP
f (x)
HP
we could consider compensating the latency by changing that lowpass. The
same of course could be done if instead of a lowpass we ﬁnd an integrator, or a
suitable structure containing one.
However it might happen that there is no lowpass or an integrator or any
other structure suitable for this purpose, neither after the waveshaper nor before
it. In such cases we could artiﬁcially insert a 1-pole lowpass immediately before
or after the waveshaper (Fig. 6.80), setting the cutoﬀ of this lowpass to a very
high value. In this case we could hope that the insertion of the new lowpass
would not signiﬁcantly change the signal, at least not in the audible range, if
its cutoﬀ is lying well above.
42The highpass in Fig. 6.79 might look different from the one in Fig. 2.9, however it’s not
difficult to realize that in fact both structures are identical.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
234
CHAPTER 6. NONLINEARITIES
x(t)
f (x)
LP
•(cid:47)
y(t)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:82) (cid:111)
Figure 6.80: Artiﬁcially inserted 1-pole lowpass.
One still has to be careful, since such lowpass will introduce noticeable
changes into the behavior of the system in the spectral range above the lowpass’s
cutoﬀ and even, to an extent, below its cutoﬀ. Even though a lowpass gener-
ally reduces the amplitude of signals, due to the changes in the phase it could
increase the system’s resonance, causing the system to turn to selfoscillation
earlier than expected.43 In a nonlinear system the inaudible parts of the spec-
trum could become audible through the so-called intermodulation distortion.44
So, it’s a good idea to test for the possible artifacts created by the introduction
of such lowpass.
Zero-delay feedback equation
The appearance of antialiased waveshapers in a zero-delay feedback loop creates
the question of solving the arising zero-delay feedback equations. Fortunately,
this doesn’t create any new problems, as the inistantaneous response of an
antialiased waveshaper can be represented in familiear terms.
Indeed, according to (6.38) the instantaneous response of an antialiased
waveshaper is simply another waveshaper:
˜f (x) =
F (x) − F (a)
x − a
(6.43)
where a = x[n − 1] is the waveshaper function’s parameter, which is having a
ﬁxed value at each given time moment n. Thus we obtain the already familiar
kind of a zero-delay feedback equation with a waveshaper.
Ill-conditioning
If x[n] ≈ x[n − 1], the denominator of (6.38) will be close to zero. Rather
fortunately this also means that the numerator will be close to zero as well,
so that, at least formally, their ratio should produce a ﬁnite value. However
practically this could mean precision losses in the numeric evaluation of the
right-hand side of (6.38) (or division by zero if x[n] = x[n − 1]).
Since x[n] ≈ x[n−1], the value of the interpolated signal x(t) and respectively
the value of y(t) = f (x(t)) shouldn’t change much on [n − 1, n] and thus the
integral in (6.38) can be well approximated by a value of y(t) somewhere on that
43This would be particularly the case in a 4-pole lowpass ladder filter, where the effect is
noticeable at ladder filter’s cutoff settings comparable or higher than the cutoff of the added
lowpass.
44Recall that e.g. in (6.33) the output signal of a waveshaper contained all sums and dif-
ferences of the frequencies of the original signal. Thus if the difference of two frequencies,
both lying well above the audible range, falls into the audible range, these originally inaudible
partials will create an audible one.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
SUMMARY
235
interval.45 In principle we could take any point on that interval, but intuitively
we should expect the midway point to give the best result, and thus we take
y[n] = f
(cid:18) x[n] + x[n − 1]
2
(cid:19)
if x[n] ≈ x[n − 1]
(6.44)
Notice that at f (x) ≈ x (6.44) turns into (6.39).
The fallback formula (6.44) creates no new problems for the solution of the
zero-delay feedback equation, since in instantaneous response terms it looks like
another waveshaper
˜f (x) = f
(cid:19)
(cid:18) x + a
2
(6.45)
where a = x[n − 1]. Note, however, that when using iterative approaches to the
solution of the zero-delay feedback equation, we potentially may need to switch
between (6.43) and (6.45) on each iteration step.
The choice between the normal and the ill-conditioned case formulas should
depend on the comparison of estimated precision losses in (6.38) and the error
In that regard note, that it might be a good idea to choose the
in (6.44).
antiderivative F (x) so that F (0) = 0. This could improve the precision of
numerical computation of (6.38) and (6.43) at low signal levels, as subtraction
of two close numbers is the main source of precision losses here. On the other
hand, the main source of error in (6.44) and (6.45) is nonlinear behavior of f (x)
on the segment lying between x[n − 1] and x[n].46
SUMMARY
Nonlinear ﬁlters can be constructed by introducing waveshapers into block dia-
grams. Two important types of waveshapers are saturators and antisaturators.
Saturators used in resonating feedback loops prevent the signal level from inﬁ-
nite growth. Antisaturators have a similar eﬀect in damping feedback paths.
The discussed types of usage of saturators in ﬁlters included feedback loop
saturation, transistor ladder-style 1-pole saturation and OTA-style 1-pole sat-
uration. The discussed usage of antisaturators included the diode clipper-style
saturation of 1-poles and the usage in the damping path of an SVF.
Waveshapers usually turn zero-delay feedback equations into transcendental
ones, which then need to be solved using approximate or numeric methods,
although in some cases analytic solution is possible.
Discrete-time waveshaping produces aliasing, which might need to be miti-
gated using oversampling and/or some more advanced methods.
45This is more precisely stated by the mean value theorem.
46Note that if f (x) is fully linear on that segment, then (6.44) gives the exact answer. One
could also obtain an estimation of the error of (6.44) by expanding f (x) in Taylor series around
x = (x[n] + x[n − 1])/2 and noticing that applying (6.38) just to the first two terms of this
expansion gives (6.44) (as the contribution of the first-order term of the series turns out to be
zero). Therefore the error of (6.44) is equal to the contribution of the remaining terms of the
Taylor series to (6.38).
236
CHAPTER 6. NONLINEARITIES
Chapter 7
State-space form
Starting with this chapter we begin the discussion of subjects of a more theoret-
ical nature, not in the sense that they are not useful for practical purposes, but
rather that one can already do a lot without the respective knowledge. Simul-
taneously the mathematical level of the presented text is generally higher than
in the previous chapters. Readers who are not too interested in the respective
subjects may consider skipping directly to Chapter 11, where the discussion
returns to the previous “practical” level.
Transfer functions fully describe the behavior of linear time-invariant sys-
tems, but, as we already have seen, once the system parameters start to vary, we
ﬁnd out that some important information about the system topology is lacking.
The state-space form provides a mathematical way to describe a system without
losing the essential information about the system’s topology.1 Practically it’s
not much diﬀerent from block diagrams, just instead of a graphical representa-
tion of a system we represent it by mathematical equations. The state-space
form can help to obtain new insights into the way how diﬀerential and diﬀerence
systems work.
7.1 Differential state-space form
The term state-space form simply means that a diﬀerential system is written in
the form of ordinary diﬀerential equations of the ﬁrst order, where the diﬀeren-
tiation is done with respect to time, and the equations have been algebraically
resolved in respect to derivatives. E.g. suppose we are interested in a 2-pole
allpass based on the state-variable ﬁlter (Fig. 4.1). In principle we already have
the respective equations in (4.1) but for the sake of demonstration let’s reobtain
them from the block diagram in Fig. 4.1.
Let u1 = yBP denote the output of the ﬁrst integrator and u2 = yLP denote
the output of the second integrator. The input of the ﬁrst integrator is x −
2RyBP − yLP, thus
(cid:90)
u1 =
ωc (x − 2Ru1 − u2) dt
1Except in cases where continuous-time block diagrams contain instantaneously unstable
integratorless feedback loops.
237
238
CHAPTER 7. STATE-SPACE FORM
The output of the second integrator is simply yBP:
(cid:90)
u2 =
ωcu1 dt
According to (4.23), the allpass signal can be obtained as y = x − 4RyBP:
Writing all three equations together:
y = x − 4Ru1
(cid:90)
(cid:90)
u1 =
u2 =
ωc (x − 2Ru1 − u2) dt
ωcu1 dt
y = x − 4Ru1
we have obtained the state-space form representation of Fig. 4.1, except that we
are having integral rather than diﬀerential equations. From the mathematical
point of view this is no more than a matter of notation and we can equivalently
rewrite the same equations as
˙u1 = ωc (x − 2Ru1 − u2)
˙u2 = ωcu1
y = x − 4Ru1
It is common to write the state-space equations in the matrix form:
(cid:19)
d
dt
(cid:18)u1
u2
=
ωc
(cid:18)−2Rωc −ωc
0
(cid:18)u1
u2
y = (cid:0)−4R 0(cid:1)
(cid:19)
+ x
(cid:19)
(cid:19) (cid:18)u1
u2
(cid:19)
(cid:18)ωc
0
x
+
(7.1a)
(7.1b)
or, by introducing
A =
(cid:18)−2Rωc −ωc
0
ωc
(cid:19)
b = (cid:0)ωc
0(cid:1)T
cT = (cid:0)−4R 0(cid:1)
d = 1
we rewrite the same in vector notation:
˙u = Au + bx
y = cTu + d · x
This is the general state-space form for a single-input single-output diﬀerential
system. We can promote it further to multiple inputs and multiple outputs by
promoting x and y to vectors and promoting b, cT and d to matrices:
˙u = Au + Bx
(7.2a)
7.2.
INTEGRATORLESS FEEDBACK
y = Cu + Dx
239
(7.2b)
E.g. for a single-input multiple-output LP/BP/HP SVF the equation (7.1b)
turns into
y =


0
1
1
0
−2R −1


(cid:19)
(cid:18)u1
u2

+


0
0
 x
1
The term state-space form originates from the fact that the vector of diﬀer-
ential variables u represents the states of the integrators, or simply the state of
the system. Respectively the linear space of vectors u is referred to as the state
space of the system.
The state-space form encodes the essential information about the system’s
topology, namely, which gains precede the integrators and which follow the
integrators. Speciﬁcally, B is the matrix of gains occuring on the paths from the
inputs to the integrators, C is the matrix of gains occurring on the paths from
the integrators to the outputs, D is the matrix of gains bypassing the integrators
and A is the matrix of gains on the feedback paths, thus they simultaneously
precede and follow the integrators.
Integral form
Equations (7.2) can be rewritten in the integral form, which is merely a nota-
tional switch:
(cid:90)
u =
(Au + Bx) dt = u(0) +
(cid:90) t
0
(Au + Bx) dτ
y = Cu + Dx
(7.3a)
(7.3b)
The integral form also allows to convert the state-space form back to the block
diagram form. Each line of (7.3a) corresponds to an integrator, the respective
right-hand side describing the integrator’s input signal.
Nonlinear state-space form
The right-hand sides of the equations (7.2) actually can be arbitary nonlinear
vector functions of vector arguments, in which case we could write the equations
as
˙u = F (u, x)
˙y = G(u, x)
The discussion of nonlinear systems has been done in Chapter 6. Most of the
ideas discussed in Chapter 6 can be equally applied to the systems expressed as
a state-space form, and we won’t discuss nonlinear state-space forms further.
7.2
Integratorless feedback
Before we can convert a block diagram (or an equation system, for that mat-
ter) into a state-space form we need to resolve integratorless feedback loops,
if there are any. Integratorless feedback is a continuous-time version of zero-
delay feedback. While zero-delay feedback loops in discrete time systems are
240
CHAPTER 7. STATE-SPACE FORM
the loops containing no unit delays, integratorless feedback loops in continuous
time systems are the loops containing no integrators.
The resolution of integratorless feedback is therefore subject to the same
considerations and procedures as the resolution of zero-delay feedback. We are
going to demonstrate this using the TSK allpass from Fig. 5.35 as an example.
k
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
x
•(cid:47)
y0
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
(cid:82)(cid:47)
u1
•(cid:47)
− (cid:15)
+
y1
+
−
•(cid:47)
(cid:82)(cid:47)
u2
•(cid:47)
− (cid:15)
+
y2
•(cid:47)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
k
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
y
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 7.1: Allpass TSK ﬁlter from Fig. 5.35 with expanded 1-pole
allpass structures.
Expanding the internal structures of the 1-pole allpasses in Fig. 5.35 we
obtain the structure in Fig. 7.1. Denoting the 1-pole allpass states as u1 and
u2, their output signals as y1 and y2 and the input of the ﬁrst 1-pole allpass as
y0 (as shown in Fig. 7.1) we obtain the following equations:
˙u1 = y0 − u1
y1 = u1 − (y0 − u1) = 2u1 − y0
˙u2 = y1 − u2
y2 = u2 − (y1 − u2) = 2u2 − y1
y0 = x − ky2
y = y2 + ky0
where this time we have assumed ωc = 1 for simplicity.
Apparently Fig. 7.1 contains an integratorless feedback loop, starting at y0,
going through the highpass path of the ﬁrst allpass to y1, then through the
highpass path of the second allpass to y2 and returning via the global feedback
path to y0. This loop contains three inverters and a gain of k, thus the total
gain of this integratorless feedback loop is −k and it is not instantaneously
unstable provided k > −1. Under this assumption we can resolve it algebraically.
Selecting just the equations for yn we have
y1 = 2u1 − y0
y2 = 2u2 − y1
y0 = x − ky2
We would like to solve for y0, therefore we ﬁrst eliminate y2 in the third equation:
y0 = x − k(2u2 − y1)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:15)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
7.3. TRANSFER MATRIX
241
and then y1 in the just obtained equation:
y0 = x − k(2u2 − (2u1 − y0)) = x − ky0 + 2k(u1 − u2)
and
(1 + k)y0 = x + 2k(u1 − u2)
y0 =
x + 2k(u1 − u2)
1 + k
Notice that the denominator corresponds to the instantaneously unstable case
occuring for k < −1.
Now that we have resolved the integratorless feedback, we need to substi-
tute the resolution result into the remaining equations of the original equation
system:
˙u1 = y0 − u1 =
(cid:18) 2k
1 + k
(cid:19)
− 1
u1 −
2k
1 + k
u2 +
1
1 + k
x =
(k − 1)u1 − 2ku2 + x
1 + k
˙u2 = y1 − u2 = 2u1 − y0 − u2 =
(cid:18)
=
2 −
(cid:19)
2k
1 + k
(cid:18)
u1 −
1 −
(cid:19)
2k
1 + k
u2 −
1
1 + k
x =
=
2u1 + (k − 1)u2 − x
1 + k
y = y2 + ky0 = 2u2 − y1 + ky0 = 2u2 − (2u1 − y0) + ky0 =
= 2(u2 − u1) + (1 + k)y0 = 2(u2 − u1) + x + 2k(u1 − u2) =
= 2(k − 1)u1 − 2(k − 1)u2 + x
Or, in the matrix form
˙u =
1
k + 1
(cid:18)k − 1 −2k
k − 1
(cid:19)
2
y = 2(k − 1) · (cid:0)1 −1(cid:1) u + x
u +
1
k + 1
(cid:19)
(cid:18) 1
−1
x
(7.4a)
(7.4b)
7.3 Transfer matrix
If x(t) = X(s)est, all other signals in the system have the same exponential
form and the system turns into
or
sU(s)est = AU(s)est + BX(s)est
Y(s)est = CU(s)est + DX(s)est
sU(s) = AU(s) + BX(s)
Y(s) = CU(s) + DX(s)
The ﬁrst of the two equations is a linear equation system in a matrix form in
respect to the unknown U(s) and the solution is found from
(s − A)U(s) = BX(s)
242
CHAPTER 7. STATE-SPACE FORM
(where s − A is a short notation for sI − A where I is identify matrix), and
U(s) = (s − A)−1B · X(s)
(7.5)
and thus
Y(s) = CU(s) + DX(s) = C(s − A)−1B · X(s) + D · X(s)
Introducing the matrix
H(s) = C(s − A)−1B + D =
C adj(s − A)B
det(s − A)
+ D
(7.6)
we have
Y(s) = H(s)X(s)
Thus H(s) is the transfer matrix of the system, its elements being the individual
transfer functions corresponding to all possible input-output pairs of the system.
In case of a single-input single-output system H(s) reduces to a 1 × 1 matrix:
H(s) = cT(s − A)−1b + d =
cT adj(s − A)b
det(s − A)
+ d
(7.7)
being simply the familiar transfer function.
From the formula (7.6) or (7.7) we can derive why the transfer functions of
system built on integrators are nonstrictly proper rational functions. Indeed,
the elements of adj(s − A) are polynomials of s of up to (N − 1)-th order (where
N is the dimension of the state space, that is simply the number of integrators).
On the other hand, det(s − A) is a polynomial of s of N -th order. Therefore,
the elements of (s − A)−1 are rational functions of s sharing the same N -th
order denominator det(s − A) and having numerators of up to (N − 1)-th order.
Thus, if D = 0, the elements of H(s) are strictly proper rational functions.
If D (cid:54)= 0, (7.6) turns into
H(s) =
C adj(s − A)B
det(s − A)
+ D =
C adj(s − A)B + D det(s − A)
det(s − A)
and thus the numerators of the elements of H(s) become polynomials of order
N , if the respective element of matrix D is nonzero. Thus, the transfer function
becomes nonstrictly proper only if there is a direct (in the sense that it contains
no integrators) path from the input to the output.
Note that, since the denominator of the transfer function(s) is det(s − A),
it follows that the roots of the det(s − A) polynomial are the system poles. At
the same time the roots of det(s − A) = 0 are the eigenvalues of A. Thus,
eigenvalues of A are the system poles.
7.4 Transposition
Computing the transfer matrix transpose we obtain from (7.6):
H T(s) = (cid:0)C(s − A)−1B + D(cid:1)T
= BT(s − AT)−1C T + DT
7.5. BASIS CHANGES
243
This looks like a transfer function of another system:
˙u(cid:48) = A(cid:48)u(cid:48) + B(cid:48)x(cid:48)
y(cid:48) = C (cid:48)u(cid:48) + D(cid:48)x(cid:48)
where
A(cid:48) = AT B(cid:48) = C T C (cid:48) = BT D(cid:48) = DT
We will refer to this new system as transposed system. The transposition of the
state-space form corresponds to the transposition of block diagrams described
in Section 2.14. Particularly, we swap the input gains B for the output gains C
and vice versa.
So, the transfer function of the transposed system is
H (cid:48)(s) = C (cid:48)(s − A(cid:48))−1B(cid:48) + D(cid:48) = BT(s − AT)−1C T + DT = H T(s)
where
Y(cid:48)(s) = H (cid:48)(s)X(cid:48)(s) = H T(s)X(cid:48)(s)
or, in component form
n(s) = H (cid:48)
Y (cid:48)
nm(s)X (cid:48)
m(s) = Hmn(s)X (cid:48)
m(s)
while for the original system we have
Ym(s) = Hmn(s)Xn(s)
But the input/output pair x(cid:48)
n, is the transposed system corresponds to the
input/output pair xn, ym of the original system and the transfer function for
each pair is Hmn. Thus, transposition preserves the transfer function relation-
ships between the respective input/output pairs.
m, y(cid:48)
7.5 Basis changes
In the process of further analysis of state-space forms it will be highly useful to be
able to change the basis of the state-space. Since a basis change is equivalent to a
linear transformation of the linear space, let u(cid:48) = T u denote such transformation
(where T is some nonsingular matrix). Remember that what we are doing is
changing the basis of the space, the transformation T is just a way to notate the
respective change of coordinates! Then u = T −1u(cid:48) and we can rewrite (7.2a) in
terms of u(cid:48):
(cid:0)T −1u(cid:48)(cid:1) = AT −1u(cid:48) + Bx
(7.8)
d
dt
T −1 ˙u(cid:48) = AT −1u(cid:48) + Bx
˙u(cid:48) = T AT −1u(cid:48) + T Bx
Respectively, (7.2b) in terms of u(cid:48) turns into
y = Cu + Dx = CT −1u(cid:48) + Dx
Introducing
A(cid:48) = T AT −1 B(cid:48) = T B C (cid:48) = CT −1
(7.9)
244
we obtain
CHAPTER 7. STATE-SPACE FORM
˙u(cid:48) = A(cid:48)u(cid:48) + B(cid:48)x
y = C (cid:48)u(cid:48) + Dx
(7.10a)
(7.10b)
which has exactly the same form as (7.2). That is we have obtained a new
state-space representation of the system in the new basis. Note that thereby we
didn’t change the basis of the spaces of the input signals x or the output signals
y, but solely the basis of the state signals u. Thus, the basis change is a purely
internal operation and doesn’t aﬀect the components of the vectors x and y.
Respectively, the transfer matrix is not aﬀected either, which can be explicitly
shown by computing the transfer matrix in the new basis:
H (cid:48)(s) = C (cid:48)(s − A(cid:48))−1B(cid:48) + D = CT −1(s − T AT −1)−1T B + D =
= CT −1(T sT −1 − T AT −1)−1T B + D =
= CT −1(T (s − A)T −1)−1T B + D =
= CT −1T (s − A)−1T −1T B + D =
= C(s − A)−1B + D = H(s)
7.6 Matrix exponential
Another tool which we will need is the concept of the matrix exponential. We
deﬁne the matrix exponential by writing the Taylor series for an ordinary ex-
ponential:
ex = 1 + x +
and replacing x with a matrix:
x2
2!
x3
3!
+
+ . . .
eX = 1 + X +
X 2
2!
+
X 3
3!
+ . . .
(7.11)
The properties of the matrix exponential are similar to the ones of the ordinary
exponential, except that typically the commutativity of the involved matrices
is required. Particularly, the following properties are derived from (7.11) in a
straightforward manner, under the assumption XY = Y X and XX (cid:48) = X (cid:48)X:
eX Y = Y eX
eX+Y = eX eY = eY eX
d
dt
eX(t) = eX X (cid:48) = X (cid:48)eX
The value of eX is particularly easy to compute if X is diagonal:
X =





λ1
0
0
0
0
λ2
0
0
· · ·
· · ·
. . .
· · ·





0
0
0
λN
In this case formula (7.11) turns into N parallel Taylor series for eλn and we
simply have
eX =





eλ1
0
0
0
0
eλ2
0
0
· · ·
· · ·
. . .
· · ·
0
0
0
eλN





7.7. TRANSIENT RESPONSE
245
If X is not diagonal, but diagonalizable by a similarity transformation T XT −1,
the value eX can be computed by noticing that matrix exponential commutes
with similarity trasformation:
eT XT −1
= T eX T −1
(7.12)
which allows to express eX via eT XT −1
. The formula (7.12) is obtainable from
(7.11) in a straightforward manner as well, where we also notice that (7.12)
holds for any T and X, they don’t have to commute.
If X is not diagonalizable, then Jordan normal form can be used instead.
We are going to address this case slightly later.
7.7 Transient response
The diﬀerential state-space equation (7.2a) can be solved in the same fashion as
we solved the diﬀerential equations for the 1-pole in Section 2.15. Indeed, the
diﬀerence between (7.2a) and the Jordan 1-pole (2.21) is that the former has
matrix form. Also in (7.2a) the input signal is additionally multiplied by the
matrix B, but that doesn’t change the picture essentially.
Repeating the same steps as in in Section 2.15, we multiply both sides of
(7.2a) by the matrix exponential e−At:
or
Noticing that
e−At ˙u = e−AtAu + e−AtBx
e−At ˙u − e−AtAu = e−AtBx
(cid:0)e−Atu(cid:1) = e−At ˙u − e−AtAu
d
dt
we rewrite the state-space diﬀerential equation further as
(cid:0)e−Atu(cid:1) = e−AtBx
d
dt
Integrating with respect to time from 0 to t:
e−Atu − u(0) =
(cid:90) t
0
e−Aτ Bx dt
u = eAtu(0) + eAt
(cid:90) t
0
e−Aτ Bx dt = eAtu(0) +
(cid:90) t
0
eA(t−τ )Bx dt
(7.13)
The formula (7.13) is directly analogous to (2.22). Further, assuming complex
exponential x(t) = X(s)est (note that all elements of x share the same expo-
nential est, just with diﬀerent amplitudes) we continue as:
u = eAtu(0) + eAt
= eAtu(0) + eAt
(cid:90) t
0
(cid:90) t
0
e−Aτ BX(s)esτ dt =
e(s−A)τ dt · BX(s) =
246
CHAPTER 7. STATE-SPACE FORM
= eAtu(0) + eAt(s − A)−1e(s−A)τ
= eAtu(0) + eAt(s − A)−1 (cid:16)
= eAt (cid:0)u(0) − (s − A)−1BX(s)(cid:1) + (s − A)−1BX(s)est
τ =0
e(s−A)t − 1
· BX(s) =
BX(s) =
(cid:17)
t
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Comparing to the transfer matrix for u(t) deﬁned by (7.5) we introduce the
steady-state response
us(t) = (s − A)−1B · X(s)est
and therefore
u(t) = eAt (u(0) − us(0)) + us(t) = ut(t) + us(t)
(7.14)
where ut(t) is the transient response.
Note that we have just explicitly obtained the fact (previously shown only for
the system orders N ≤ 2) that, given a complex exponential input X(s)est, the
elements of the steady-state response us will be the same complex exponentials
est, just with diﬀerent amplitudes. An immediately following conclusion is that
the steady-state signals y, being linear combinations of u and x, are also the
same complex exponentials est.
In fact, any other steady-state signal in the
system, being a linear combination of u and x, is the same complex exponential
est.
In a fully analogous to the 1-pole case way we can show that (7.14) also
holds for
in which case
x(t) =
(cid:90) σ+j∞
σ−j∞
X(s)est ds
2πj
us(t) =
(cid:90) σ+j∞
σ−j∞
(s − A)−1BX(s)est ds
2πj
Substituting (7.14) into (7.2b) we obtain
y(t) = CeAt (u(0) − us(0)) + Cus(t) + Dx(t) =
= eAt ((Cu(0) + Dx(0)) − (Cus(0) + Dx(0))) + Cus(t) + Dx(t) =
= eAt (y(0) − ys(0)) + ys(t) = yt(t) + ys(t)
where
and
yt(t) = Cut(t) = eAt (y(0) − ys(0))
ys(t) = Cus(t) + Dx(t) = C
(cid:90) σ+j∞
σ−j∞
(s − A)−1BX(s)est ds
2πj
+ Dx(t) =
=
=
(cid:90) σ+j∞
σ−j∞
(cid:90) σ+j∞
σ−j∞
(cid:0)C(s − A)−1B + D(cid:1) X(s)est ds
2πj
=
H(s)X(s)est ds
2πj
(7.15)
The latter conﬁrms the fact that ys is the steady-state response.
7.8. DIAGONAL FORM
247
7.8 Diagonal form
We have seen that the transient response part of the signals in the system
consists of linear combinations of elements of the matrix eAt. The elements
of eAt can be easily found if A is diagonalized by a similarity transformation.
However, instead of diagonalizing the matrix A taken in isolation, it will be
more instructive to consider this as diagonalization of the state-space system
itself.
According to (7.2a), the matrix A is an operator converting vectors from the
state space into vectors in the same space. This, diagonalization of A can be
achieved by a speciﬁc choice of the state space basis, where the basis vectors
must be the eigenvectors of A. After the change of basis we are having exactly
the same system, just expressed in diﬀerent coordinates. In these coordinates
the matrix A becomes diagonal and its diagonal elements are eigenvalues of A
(which are basis-independent). Now recall that eigenvalues of A are the same
as the system poles. Therefore, a suﬃcient condition for the state-space system
to be diagonalizable is that all of its poles are distinct.2
Thus, in a diagonalizing basis the elements of A are simply the system poles:
A =





p1
0
0
0
0
p2
0
0
· · ·
· · ·
. . .
· · ·





0
0
0
pN
and the system falls apart into a set of parallel Jordan 1-poles:
˙un = pnun + bT
n · x
y = Cu + Dx =
(cid:88)
n
cnun + Dx
(7.16a)
(7.16b)
n are the rows of matrix B (respectively bT
where bT
the Jordan 1-poles), and cn are the columns of matrix C.
n · x are the input signals of
Stability
We already know that the transient response utn(t) of a 1-pole is an exponent
Knepnt (where Kn is the exponent’s amplitude). Respectively, the transient
response part of y in (7.16b) is a linear combination of transient responses of
the Jordan 1-poles:
yt = Cut =
(cid:88)
n
cnutn =
(cid:88)
n
cnKnepnt
That is, the elements of yt are linear combinations of exponents epnt.
Now recall that y is independent on the choice of basis and so must be
its separation into steady-state and transient response parts. Note that this is
in agreement with the fact that according to (7.15) the steady-state response
depends only on the transfer matrix and thus is independent of the basis changes.
This means that the fact that the elements of yt are linear combinations of
2A little bit later we will establish the fact that a system where some poles coincide is most
likely not diagonalizable.
248
CHAPTER 7. STATE-SPACE FORM
exponents epnt is also independent of basis choice. Respectively yt → 0 if and
only if Re pn < 0 ∀n. Thus we have obtained the explanation of the stability
criterion for linear ﬁlters which we introduced in Section 2.9 and have partially
shown for lower-order systems.3
Transfer matrix
Computing the transfer matrix for the diagonal form we notice that
(s − A)−1 =










1
s − p1
0
0
0
0
1
s − p2
0
0
· · ·
· · ·
. . .
· · ·










0
0
0
1
s − pN
(7.17)
that is we have transfer functions of the Jordan 1-poles on the main diagonal.
Respectively the main term of the transfer matrix C(s − A)−1B is just a linear
combination of the Jordan 1-pole transfer functions. Apparently the common
denominator of the terms of this linear combination is
N
(cid:89)
(s − pn)
n=1
which is simultaneously the common denominator of the transfer matrix ele-
ments.
It can be instructive to explicitly write out the elements of the transfer
matrix H(s) in the diagonal case:
Hnm(s) =
N
(cid:88)
k=1
cnk
1
s − pk
bkm + dnm =
N
(cid:88)
k=1
cnkbkm
s − pk
+ dnm
(7.18)
that is, we are having a partial fraction expansion of the rational function
Hnm(s) into fractions of 1st order. Thus, if a transfer matrix is given in advance,
there is not much freedom in respect to the choice of the elements of B and C.
The poles pk are prescribed by the common denominator of the transfer matrix
and the values of the products cnkbkm and of dnm are prescribed by the speciﬁc
functions Hnm(s) occurring in the respective elements of the transfer matrix.
In the single-input single-output case the transfer matrix has 1 × 1 dimen-
sions, while C has 1 × N and B has N × 1 dimensions respectively. Thus there
is only one equation (7.18) and we have N freedom degrees in respect to the
choice of cnk and bkm giving the required values of the products cnkbkm. Each
such degree can be associated with a variable αk, where we replace bkm with
αkbkm and cnk with cnk/αk. Apparently such replacement doesn’t aﬀect the
value of the product cnkbkm. One can also realize that αk simply scale the levels
3Of course, exactly the same results would have been obtained if we simply computed
the explicit form of the matrix exponential eAt for the diagonal matrix At. However then
we would have missed the interpretation of the diagonalizing basis as the basis in which the
system can be seen simply as a set of parallel 1-poles.
7.8. DIAGONAL FORM
249
of the signals uk, which corresponds to diﬀerent choices of the lengths of the
basis vectors.
Now if we add one more output signal, thereby the dimensions of C becoming
2 × N , we can notice that we still have exactly the same N degrees of freedom.
If we attempt to change any of bkm we need to compensate this in both of cnk
for the same k by dividing these cnk by αk, the latter being the ratio of the new
and old values of bkm. Respectively, if we change any of cnk in one of the two
rows of C, this immediately requires the compensating change of bkm, which in
turn requires that the same change occurs not just in one but in both rows of
C. Adding more rows to C and/or more columns to B we see that the available
freedom degrees are still the same and correspond to the freedom of choice of
the basis vector lengths.
Thus, aside from the free choice of the basis vector lengths (and of their
ordering) the transfer matrix uniquely deﬁnes the diagonal form of the state-
space system. Respectively, for a non-diagonal form, if the matrix A is given,
then the transformation T to the diagonal form is uniquely deﬁned (up to the
lengths and the ordering of the basis vectors), and, since the transfer function
uniquely deﬁnes the matrices B(cid:48), C (cid:48) and D(cid:48) of the diagonal form, the matrices
B = T −1B(cid:48), C = C (cid:48)T and D = D(cid:48) are also uniquely deﬁned.
Steady-state response
Apparently, there is the usual freedom in regards to the choice of the steady-state
response arising out of evaluating the inverse Laplace transform of H(s)X(s)
to the left or to the right of the poles of H(s). The change of the steady-
state response (7.15) depending on the choice of the inverse Laplace transform’s
integration path in (7.15) to the left or to the right (or in between) the poles of
H(s) poses no fundamentally new questions compared to the previous discussion
in the analysis of 1- and 2-pole transient responses and results simply in the
changes of the amplitudes of transient response partials.
Diagonalization in case of coinciding poles
Even if two or more poles of the system coincide, it still might be diagonalizable,
if the eigenvectors corresponding to these poles are distinct. It might seem that
this is the most probable situation, after all, what are the changes of two vectors
coinciding, or at least being collinear? Without trying to engage ourselves into
an analysis of the respective probabilities, we are going to look at this fact from
a diﬀerent angle.
Namely, given a diagonal state space form with some of the eigenvalues
coinciding, we are going to have identical entires in the matrix (s − A)−1, as
one can easily see from (7.17). This means that the order of the common
denominator of the elements of (s − A)−1 will be less than N and respectively
the order of the denominator of the transfer matrix H(s) will also be less than
N . This means that the eﬀective order of the system is less than N and the
system is degenerate.
Thus, a non-degenerate system with coinciding poles cannot be diagonalized.
In such cases we will have to use the Jordan normal form, which we discuss a
bit later.
250
CHAPTER 7. STATE-SPACE FORM
7.9 Real diagonal form
Given a state-space system we could decide to implement it in a diagonal form
by ﬁrst performing a diagonalizing change of basis and then implementing the
obtained diagonal state space form. However, if the system has complex poles,
the underlying Jordan 1-poles of the system will become complex too, respec-
tively generating complex signals un. So, while the system has real input and
real output, internally it would need to deal with complex signals. Of course, in
a digital world using complex signals internally in a system shouldn’t be a big
problem. But, for one, this is simply unusual and complicates the implementa-
tion structure. More importantly, operations on complex numbers are at least
twice as expensible as the same operations on real numbers. We therefore wish
to convert a diagonal form containing complex poles to a purely real system,
while retaining as much of the diagonalization as possible.
Since the system itself and the matrix A in the original basis are real, the
complex poles need to come in conjugate pairs. Without loss of generality we
can order the poles in such a way that complex-conjugate pairs come ﬁrst,
followed by purely real poles: p1, p∗
1, p3, p∗
1, p4 = p∗
3,
etc.) We will refer to the complex poles p1, p3, . . . as the odd poles and to p∗
1,
p∗
3, . . . as the even poles. When referring to odd/even poles we will mean only
the essentially complex poles, the purely real poles being excluded. Since the
poles pn are eigenvalues of A, we will be referring to even/odd eigenvalues and
respectively to even/odd eigenvectors.
3, . . ., pN (where p2 = p∗
Let v1 be the eigenvector corresponding to p1, that is Av1 = p1v1. Then,
since A has purely real coeﬃcients, Av1 = Av1 = p1v1 = p∗
1v1, (where v
denotes conjugation of vector’s components). Thus v1 is the eigenvector corre-
sponding to p∗
1. Obviously, the same applies to any other even/odd eigenvector.
Therefore we can choose a set of eigenvectors such that even eigenvectors are
component conjugates of odd eigenvectors: v1, v1, v3, v3, . . ., vN .
If u(cid:48) = T u is the diagonalizing transformation of the system, the new basis
must consist of eigenvectors of A. Respectively, since u = T −1u(cid:48), the columns
of T −1 must consist of the new basis vectors, that is of the eigenvectors of A
(or, more precisely, consist of coordinates of these eigenvectors in the original
basis). We will choose
T −1 = (cid:0)v1 v1 v3 v3
. . . vN
(cid:1)
This means that applying component conjugation to T −1 swaps the even and
the odd columns of T −1, which can be expressed as
T −1 = T −1S
where
S =
1 0
0
0
1

0
1 0


0 0


0 0




0
0 0
0 0 0
0
0
1
0
0
0
· · ·
· · ·
· · ·
· · ·
. . .
· · ·

0
0


0


0



0

1
is the “swapping matrix”. Note that elements of S are purely real and that
S−1 = S.
7.9. REAL DIAGONAL FORM
251
Since
T · T −1 = T T −1 = 1∗ = 1
component conjugation and matrix inversion commute:
−1
T
= T −1 = T −1S
Reciprocating the leftmost and the rightmost expressions we have
T = (cid:0)T −1S(cid:1)−1
= S−1T = ST
That is, component conjugation of T swaps its even and odd rows. Or, put in
a slightly diﬀerent way, the even/odd rows of T are component conjugates of
each other, and so are the even/odd columns of T −1.
Let’s now concentrate on the ﬁrst conjugate pair of poles. Taking the diag-
onalized form equations (7.16) we extract those speciﬁcally concerning the ﬁrst
two poles:
1 = p1u(cid:48)
˙u(cid:48)
1u(cid:48)
2 = p∗
˙u(cid:48)
1 + b(cid:48)T
2 + b(cid:48)T
1 · x
2 · x
y = c(cid:48)
1u(cid:48)
1 + c(cid:48)
2u(cid:48)
2 +
(7.19a)
(7.19b)
(7.19c)
N
(cid:88)
n=3
c(cid:48)
nu(cid:48)
n + Dx
(where we need to employ the prime notation (7.10) for the diagonalized form,
since we explicitly used the diagonalizing transformation u(cid:48) = T u, thus the
non-primed state u referring to the non-diagonalized form).
Using (7.9) and recalling that the ﬁrst two rows of T are component con-
jugates of each other, we must conclude that so are the ﬁrst two rows of B’,
that is b(cid:48)T
1 . Recalling that the ﬁrst two colums of T −1 are component
conjugates of each other, we conclude that c(cid:48)
2 = c(cid:48)
1. On the other hand, writing
out the ﬁrst two rows of (7.13) in the diagonal case we have
2 = b(cid:48)T
u(cid:48)
1(t) = ep1tu(cid:48)
1(0) +
2(t) = ep∗
u(cid:48)
1 tu(cid:48)
2(0) +
(cid:90) t
0
(cid:90) t
0
ep1(t−τ )b(cid:48)T
1 x dt
ep∗
1 (t−τ )b(cid:48)T
1 x dt
Except for the initial state term, the right-hand side of the second equation is a
complex conjugate of the right-hand side of the ﬁrst one. Regarding the initial
state term, practically seen, we would have the following situations
- the initial state would be either zero, in which case u(cid:48)
2(t) = u(cid:48)∗
1 (t),
- or it would be a result of some previous signal processing by the system,
where previously to that processing the initial state would be zero, in
which case u(cid:48)
1 (0) and respectively u(cid:48)
2(0) = u(cid:48)∗
2(t) = u(cid:48)∗
1 (t).
Therefore, we can simply require that u(cid:48)
signals u(cid:48)
Respectively, the contribution of u(cid:48)
to c(cid:48)
1 (0), and thus the output
2(t) of the ﬁrst two Jordan 1-poles are mutually conjugate.
2(t) into y in (7.19c), being equal
2, turns out to be a sum of two conjugate values and is therefore
2(0) = u(cid:48)∗
1(t) and u(cid:48)
1(t) and u(cid:48)
1 + c(cid:48)
1u(cid:48)
2u(cid:48)
252
CHAPTER 7. STATE-SPACE FORM
purely real. Obviously, the same applies to all other complex conjugate pole
pairs.
Thus, even equations of (7.16a) do not contribute any new information about
the system and we could drop them, simply computing even state signals as
conjugates of odd state signals: u(cid:48)
3 , etc. At the same time we
could rewrite the odd equations of (7.16a) explicitly using real and imaginary
parts of the signals u(cid:48):
4 = u(cid:48)∗
2 = u(cid:48)∗
1 , u(cid:48)
d
dt
d
dt
Re u(cid:48)
n = Re pn Re u(cid:48)
n − Im pn Im u(cid:48)
n + (cid:0)Re b(cid:48)T
n
Im u(cid:48)
n = Im pn Re u(cid:48)
n + Re pn Im u(cid:48)
n + (cid:0)Im b(cid:48)T
n
(cid:1) · x
(cid:1) · x
(7.20a)
(7.20b)
Therefore we can introduce the new state variables, taking purely real values:
n = Re u(cid:48)
u(cid:48)(cid:48)
n =
n+1 = Im u(cid:48)
u(cid:48)(cid:48)
n =
n + u(cid:48)
u(cid:48)
2
n − u(cid:48)
u(cid:48)
2j
n+1
n+1



for odd pn
(7.21a)
and
n = u(cid:48)
u(cid:48)(cid:48)
n
for purely real pn
(7.21b)
Then (7.20) turn into
n = Re pn · u(cid:48)(cid:48)
˙u(cid:48)(cid:48)
n+1 = Im pn · u(cid:48)(cid:48)
˙u(cid:48)(cid:48)
n − Im pn · u(cid:48)(cid:48)
n + Re pn · u(cid:48)(cid:48)
n+1 + (cid:0)Re b(cid:48)T
n+1 + (cid:0)Im b(cid:48)T
n
n
(cid:1) · x
(cid:1) · x
(7.22a)
(7.22b)
and the respective terms in (7.19c) turn into
nu(cid:48)
c(cid:48)
n + c(cid:48)
n+1u(cid:48)
n+1 = c(cid:48)
nu(cid:48)
= 2 Re c(cid:48)
n + c(cid:48)∗
n · u(cid:48)(cid:48)
n u(cid:48)∗
nu(cid:48)
n = c(cid:48)
n − 2 Im c(cid:48)
n + (c(cid:48)
n · u(cid:48)(cid:48)
n+1
nu(cid:48)
n)∗ = 2 Re (c(cid:48)
nu(cid:48)
n) =
Thus we have obtained a purely real system









˙u(cid:48)(cid:48) =
Re p1 − Im p1
Re p1
Im p1
0
0
0
0
0
0
0
0
Re p3 − Im p3
Re p3
Im p3
0
0
0
0
0
0
0
0
· · ·
· · ·
· · ·
· · ·
. . .
· · ·









0
0
0
0
0
pN
u(cid:48)(cid:48) +









Re b(cid:48)T
1
Im b(cid:48)T
1
Re b(cid:48)T
3
Im b(cid:48)T
3
...
b(cid:48)T
N
y = (cid:0)2 Re c(cid:48)
1 −2 Im c(cid:48)
n
2 Re c(cid:48)
3 −2 Im c(cid:48)
3
· · ·
(cid:1) u(cid:48)(cid:48) + Dx
c(cid:48)
N









x
(7.23a)
(7.23b)
We will refer to (7.23) as the real diagonal form. It represents the system as a
set of parallel 2-poles (7.22) (and optionally additional parallel 1-poles if some
of the system poles are real).
Note that the substitutions (7.21) are expressible as another linear transfor-
7.9. REAL DIAGONAL FORM
253
mation u(cid:48)(cid:48) = T (cid:48)u(cid:48) where











T (cid:48) =
1
2
1
2
1
2j − 1
2j
0
0
0
0
0
0
0
0
0
0
1
2
1
0
0
1
2
2j
2j − 1
0
0
0
0
· · ·
· · ·
· · ·
· · ·
. . .
· · ·

0

0


0



0



0

1
Therefore the real diagonal form of the system is related to the original form by
a change of basis, where the respective transformation matrix is T (cid:48) T .
Jordan 2-poles
The 2-poles (7.22) in the real diagonal form are fully analogous to the 1-poles
occuring in the diagonal form. They will also occur in the real Jordan normal
form. For that reason we will refer to them as Jordan 2-poles. They are also
sometimes (especially in their discrete-time counterpart form) referred to as
coupled-form resonators.
The key feature of the Jordan 2-pole topology is that in the absence of the
input signal, the system state is spiralling in a circle of an exponentially decaying
(or growing) radius. Indeed, recalling that equations (7.22) are simply separate
equations for the real and imaginary components of a complex signal u(cid:48)
n, we can
return to using the equation (7.19a), which by letting x = 0 and turns into
˙u = pu
where we also dropped the indices and the prime notation for simplicity. Re-
spectively
d
dt
˙u
u
log u =
= p = Re p + j Im p
(7.24)
On the other hand
therefore
log u = ln |u| + j arg u
d
dt
log u =
d
dt
ln |u| + j
d
dt
arg u
(7.25)
Equating the right-hand sides of (7.24) and (7.25), we obtain
d
dt
ln |u| + j
d
dt
arg u = Re p + j Im p
or
from where
d
dt
d
dt
ln |u| = Re p
arg u = Im p
ln |u(t)| = ln |u(0)| + Re p · t
254
or
CHAPTER 7. STATE-SPACE FORM
arg u(t) = arg u(0) + Im p · t
|u(t)| = |u(0)| · et Re p
arg u(t) = arg u(0) + Im p · t
Thus the complex value u(t) is rotating around the origin with the angular
speed Im p, it’s distance from the origin changing as et Re p, thereby moving in
a decaying spiral if Re p < 0, an expanding spiral if Re p > 0, or a circle if
Re p = 0. Recalling that the state components of (7.22) are simply the real
and imaginary parts of u in the above equations, we conclude that the state of
(7.22) in the absence of the input signal is moving in the same spiral trajectory.
Notably, the separation of u into real and imaginary parts works only if the
pole is complex.4
Transfer matrix
In order to obtain the transfer matrix of the real diagonal form we could ﬁrst
obtain the transfer matrices of the individual 2-poles (7.22). Concentrating on
a single 2-pole, we write (7.22) as
˙u1 = Re p · u1 − Im p · u2 + x1
˙u2 = Im p · u1 + Re p · u2 + x2
where we ignored the input mixing coeﬃcients B (in principle we can understand
this form in the sense that the input signals are picked up past the mixing
coeﬃcients B, or as a particular case of B being identity matrix). We could
explicitly compute the matrix (s−A)−1 for the above system, or we could derive
it “manually”, which is what we’re going to do.
Given x1 = X1(s)est, x2 = X2(s)est we have
sU1(s)est = Re p · U1(s)est − Im p · U2(s)est + X1(s)est
sU2(s)est = Im p · U1(s)est + Re p · U2(s)est + X2(s)est
Respectively
(s − Re p)U1(s) + Im p · U2(s)
= X1(s)
− Im p · U1(s) + (s − Re p)U2(s) = X2(s)
Attempting to eliminate U1(s), we multiply each equation by a diﬀerent factor:
(s − Re p) Im p · U1(s) + (Im p)2 · U2(s) = Im p · X1(s)
− (s − Re p) Im p · U1(s) + (s − Re p)2U2(s) = (s − Re p)X2(s)
and add both equations together:
(cid:0)(s − Re p)2 + (Im p)2(cid:1) U2(s) = Im p · X1(s) + (s − Re p)X2(s)
4This is strongly related to the appearance of Jordan normal form at the moment when
two complex conjugate poles coincide on the real axis.
7.9. REAL DIAGONAL FORM
255
Respectively attempting to eliminate U2(s), we multiply each equation by a
diﬀerent factor:
(s − Re p)2U1(s) + (s − Re p) Im p · U2(s) = (s − Re p)X1(s)
− (Im p)2 · U1(s) + (s − Re p) Im p · U2(s) = Im p · X2(s)
and subtract the second equation from the ﬁrst one:
(cid:0)(s − Re p)2 + (Im p)2(cid:1) U1(s) = (s − Re p)X1(s) − Im p · X2(s)
Thus
(cid:18)U1(s)
(cid:19)
U2(s)
=
=
1
(s − Re p)2 + (Im p)2
1
s2 − 2 Re p · s + |p|2
(cid:18)s − Re p − Im p
s − Re p
Im p
(cid:18)s − Re p − Im p
s − Re p
Im p
(cid:19) (cid:18)X1(s)
(cid:19)
X2(s)
(cid:19) (cid:18)X1(s)
(cid:19)
X2(s)
and, since for this system the matrix B is identity matrix,
(s − A)−1 =
1
s2 − 2 Re p · s + |p|2
(cid:18)s − Re p − Im p
s − Re p
Im p
(cid:19)
=
(7.26)
Note that the denominator is the standard 2-pole ﬁlter’s transfer function de-
nominator, written in terms of the pole. Indeed, the complex conjugate poles p
and p∗ of two complex Jordan 1-poles were combined into a Jordan 2-pole by
means of a linear combination. Respectively the Jordan 2-pole has exactly the
same poles.
Generalizing the result obtained in (7.26) to systems of arbitrary order,
containing multiple parallel 2-poles, we conclude that the main diagonal of (s −
A)−1 contains the matrices of the form
G(s) =
1
s2 − 2 Re p · s + |p|2
(cid:18)s − Re p − Im p
s − Re p
Im p
(cid:19)
(7.27)
similarly to how the transfer functions 1/(s − pn) of the Jordan 1-poles are
occurring on the main diagonal of (s − A)−1 in (7.17). Thus (s − A)−1 has the
form
(s − A)−1 =






G1(s)
0
0
G2(s)
0
0
0
0
· · ·
· · ·
. . .
· · ·






0
0
0
1
s − pN
where G(s) have the form (7.27).
Similarly to what we did in the diagonal case, in the real diagonal case we
also would like to explicitly write out the elements of the transfer matrix H(s).
For the sake of notation simplicity we will write them out for the case of a 2 × 2
matrix A. First, let’s notice that
(cid:0)c1
(cid:1)
c2
(cid:18)ρ11
ρ21
ρ12
ρ22
(cid:33)
(cid:19) (cid:32)
bT
1
bT
2
= (cid:0)c1
(cid:1)
c2
(cid:32)
ρ11bT
ρ21bT
1 + ρ12bT
2
1 + ρ22bT
2
(cid:33)
=
256
CHAPTER 7. STATE-SPACE FORM
1 + c1ρ12bT
2 + c2ρ21bT
1 + c2ρ22bT
2
(cid:1) =
= (cid:0)c1ρ11bT
(cid:32) 2
(cid:88)
=
(cid:33)
ρklckbT
l
k,l=1
where ρnm are the elements of (s − A)−1 and where cnbT
m denotes the outer
product of the n-th column of C by the m-th row of B. Then, for a 2 × 2 real
diagonal system we obtain:
2
(cid:88)
Hnm(s) =
ρklcnkblm + dnm =
k,l=1
(cn1b1m + cn2b2m)(s − Re p) + (cn2b1m − cn1b2m) Im p
s2 − 2 Re p · s + |p|2
+ dnm =
αnms + βnm
s2 − 2 Re p · s + |p|2 + dnm
=
=
where αnm and βnm are obtained by summing the respective products of the
elements of b and c. Respectively, for higher-order systems we have
Hnm(s) =
(cid:88)
Im pk>0
αnmks + βnmk
s2 − 2 Re pk · s + |pk|2 +
(cid:88)
Im pk=0
cnkbkm
s − pk
+ dnm
(7.28)
Since real diagonal form is nothing more than a linear transformation of the
diagonal form, there are the same freedom degrees in respect to the choice of the
coeﬃcients of B and C matrices, corresponding to choosing the basis vectors of
diﬀerent lengths.
7.10 Jordan normal form
We have shown that if a non-degenerate system has coinciding poles, it is not
diagonalizable. The generalization of the diagonalization idea, which also works
in this case, is Jordan normal form. The process of diagonalization implies that
there is a similarity transformation of the matrix which brings the matrix into
a diagonal form. Such transformation might not exist. However, there is always
a similarity transformation bringing the matrix into the Jordan normal form.
The building element of a matrix in the Jordan normal form is a Jordan cell.
A Jordan cell is a matrix having the form
Jn =










pn
1
0
0
0
0
0
pn
1
0
0
0
0
0
pn
. . .
0
0
· · ·
· · ·
· · ·
. . .
. . .
· · ·
0
0
0
...
pn
1










0
0
0
0
0
pn
(7.29)
That is it contains one and the same eigenvalue pn all over its main diagonal,
and it contains 1’s on the subdiagonal right below its main diagonal, all other
7.10. JORDAN NORMAL FORM
257
elements being equal to zero.5 Respectively, a matrix in the Jordan normal form
consists of Jordan cells on its main diagonal:
A =





J1
0
0
0
0
J2
0
0
· · ·
· · ·
. . .
· · ·





0
0
0
JM
(where M is the number of diﬀerent Jordan cells), all other entries in the matrix
being equal to zero.
Apparently the sizes of all Jordan cells should sum up to the dimension of
the matrix A. The total number of times an eigenvalue appears on the main
diagonal of A is equal to the multiplicity of the eigenvalue. Typically there
would be a single Jordan cell corresponding to a given eigenvalue. Thus, if an
eigenvalue has a multiplicity of 5, typically there would be a single Jordan cell
of size 5 × 5 containing that eigenvalue. It is also possible that there are several
Jordan cells corresponding to the same eigenvalue, e.g. given an eigenvalue of a
multiplicity of 5, there could be a 2 × 2 and a 3 × 3 Jordan cell containing that
eigenvalue. If there are several Jordan cells for a given eigenvalue, the respective
state-space system is degenerate, fully similar to the case of repeated poles in
the diagonalized case.
It is easy to notice that, compared to the diagonal form, Jordan cells appear
on the main diagonal instead of eigenvalues. A Jordan cell may have a 1×1 size,
in which case it is identical to an eigenvalue appearing on the main diagonal. If
all Jordan cells have 1 × 1 size Jordan normal form turns into diagonal form.
Similarly to diagonal form being unique up to the order of eigenvalues, the
Jordan normal form is unique up to the order of Jordan cells. That is, the
number and the sizes of Jordan cells corresponding to a given pole is a property
of the original matrix A. The process of ﬁnding the similarity transformation
converting a matrix into Jordan normal form is not much diﬀerent from the
diagonalization process: we need to ﬁnd a basis in which the matrix takes
Jordan normal form, which immediately implies a set of equations for such
basis vectors. More details can be found outside of this book.
Jordan chains
It’s not diﬃcult to realize that a Jordan cell corresponds to a series of Jordan
1-poles, which we introduced in Section 2.15 under the name of a Jordan chain.
So, now we should be able to understand the reason for that name.
Indeed, suppose A is in Jordnal normal form and suppose there is a Jordan
cell of size N1 located at the top of the main diagonal of A. Then, writing out
the ﬁrst N1 rows of (7.2) we have
1 · x
˙u1 = p1u1 + bT
˙u2 = p1u2 + (cid:0)u1 + bT
˙u3 = p1u3 + (cid:0)u2 + bT
2 · x(cid:1)
3 · x(cid:1)
· · ·
5Some texts place 1’s above the main diagonal. This is simply a matter of convention. One
can convert from one version to the other by simply reindexing the basis vectors.
258
CHAPTER 7. STATE-SPACE FORM
˙uN1 = p1uN1 + (cid:0)uN1−1 + bT
N1
· x(cid:1)
Note that except for the ﬁrst line, the input signal of the respective 1-pole
contains the output of the previous 1-pole. In Fig. 2.24 we had a single-input
single-output Jordan chain, now we are having a multi-input multi-output one
(Fig. 7.2).
bT
1 · x
bT
2 · x
bT
3 · x
bT
N1
· x
1
s−p
(cid:47) +
•(cid:47)
1
s−p
(cid:47) +
•(cid:47)
· · ·
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
u1
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
u2
1
s−p
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
uN1
Figure 7.2: Multi-input multi-output Jordan chain
Transfer matrix
In the diagonal case the transfer matrix had a diagonal form (7.17) correspond-
ing to the fact that the diagonal form is just a set of parallel Jordan 1-poles.
Now we need to replace these 1-poles with Jordan chains. Thus, instead of
single values 1/(s − pn) on the main diagonal, the transfer matrix will have
submatrices of the size of respective Jordan cells. From Fig. 7.2 it’s not diﬃcult
to realize that a transfer submatrix corresponding to a Jordan cell of the form
(7.29) will have the form


















1
s − pn
1
(s − pn)2
1
(s − pn)3
...
1
(s − pn)N1−1
1
(s − pn)N1
0
1
s − pn
1
(s − pn)2
...
1
(s − pn)N1−2
1
(s − pn)N1−1
0
0
1
s − pn
. . .
1
(s − pn)N1−3
1
(s − pn)N1−2
· · ·
· · ·
· · ·
. . .
. . .
· · ·
0
0
0
...
1
s − pn
1
(s − pn)2


















0
0
0
0
0
1
s − pn
Transient response
According to (7.13), the elements of the matrix eAt are the exponent terms in
u(t) which have the amplitudes un(0). Apparently, being a part of the transient
response, these terms do not explicitly depend on the system input signal and
thus are the same in the single-input single-output and multiple-input multiple-
output cases. Comparing to the explicit expression (2.25) for the output signal
of a single-input single-output Jordan chain, we realize the following.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
7.11.
ILL-CONDITIONING OF DIAGONAL FORM
259
The elements of eAt are tνepnt/ν!. These elements are organized into sub-
matrices of eAt corresponding to Jordan cells of A. Each such submatrix has
the following form:6















1
t
t2
2
...
tN1−2
(N1 − 2)!
tN1−1
(N1 − 1)!
0
1
0
0
t
...
tN1−3
(N1 − 3)!
tN1−2
(N1 − 2)!
1
. . .
tN1−4
(N1 − 4)!
tN1−3
(N1 − 3)!
· · ·
· · ·
· · ·
. . .
. . .
· · ·
0
0
0
...
1
t

0
0



0




0



0




1
· epnt
This conﬁrms that the stability criterion Re pn < 0 ∀n stays the same even if
the system is not diagonalizable.
Real Jordan normal form
If the system has pairs of mutually conjugate poles, the Jordan cells for these
poles will also come in conjugate pairs. Following the same steps as for diagonal
form, we can introduce new state variables for the real and imaginary parts of
complex state signals. Respectively, we each pair of conjugate Jordan cells will
be converted to a purely real cell of double size. We will refer to such cells as
real Jordan cells.
In order to understand how a real Jordan cell looks like, we can recall the
interpretation of Jordan cells as Jordan chains (Fig. 7.2). Let’s imagine that
the signals passing through this chain are complex. This can be equivalently
represented as passing real and imaginary parts of these signals separately. Re-
spectively, an element of a real Jordan chain must simply forward the real and
imaginary parts of its output signal to the real and imaginary inputs of the next
element. E.g. for a pair of conjugate 2nd-order Jordan cells




p 0
1 p
0
0
0
0
0 p∗
1
0




0
0
0
p∗
the corresponding real Jordan cell would be




Re p − Im p
Re p
Im p
0
1
1
0
0
0
0
0
Re p − Im p
Re p
Im p




7.11
Ill-conditioning of diagonal form
Suppose we are having a system where all poles are distinct, which is therefore
diagonalizable. And suppose, as a matter of a thought experiment, we begin
6The explicit form of an exponent of a Jordan normal form matrix can also be obtained
directly from (7.11), but that approach is more involved and we won’t do it here.
260
CHAPTER 7. STATE-SPACE FORM
to modify the system parameters in a continuous way, simultaneously keeping
track of the diagonal form of this system. We also keep track of the similarity
transformation matrix T deﬁned by u(cid:48) = T u, where u is the original state and
u(cid:48) is the “diagonalized” state. Note, that by this experiment we don’t mean
that we are varying the system parameters in respect to time, rather we consider
it as looking at diﬀerent systems with diﬀerent parameter values.
Suppose, we modify the system parameters in such a way, that some poles
of the system get close to each other and ﬁnally coincide. Assuming the system
order doesn’t degenerate, at this point we should switch from a diagonal matrix
A(cid:48) to a Jordan normal form matrix A(cid:48). The diﬀerence between these two matices
is clearly non-zero, thus there is a sudden jump in the components of matrix A(cid:48)
at the moment of the switching. Respectively, there is a jump in the components
of T as well. We wish to analyse more closely, what’s happening in this case.
If two eigenvalues of a matrix become close then the respective eigenvectors
might either also get close to each other or not. If they don’t, the eigenspace
retains the full dimension as the poles coincide, respectively the system is diag-
onalizable and the system order degenerates. Thus, if the order of the system
doesn’t degenerate, the eigenvectors corresponding to closely located eigenval-
ues must get close to each other too. Note that by saying that the eigenvectors
are getting close to each other we mean that they are becoming almost collinear.
Apparently, eigenvectors simply having diﬀerent lengths but the same (or the
opposite) directions don’t count as diﬀerent eigenvectors.
Let’s pick a pair of such eigenvectors which are getting close to each other.
Without loss of generality we may denote these two eigenvectors as v1 and v2.
In order to simplify the discussion, we will ﬁrst assume that both eigenvectors
|v1| = |v2| = 1 (where here and further the lengths will be
are normalized:
deﬁned in terms of the original basis, that is we are treating the original basis
as an orthonormal one). Again, without loss of generality we may assume that
v1 and v2 are pointing in (almost) the same direction.
Suppose we have a state vector u lying fully in the two-dimensional subspace
spanned by v1 and v2. Therefore its coordinate expansion in the diagonalizing
basis is a linear combination of v1 and v2, the other coordinates being zeros:
u = α1v1 + α2v2
We are going to show that α1 and α2 are not well deﬁned.
Let’s introduce two other unit-length vectors into the same two-dimensional
subspace:
v+ =
v− =
v1 + v2
|v1 + v2|
v1 − v2
|v1 − v2|
Apparently, v+ and v− are orthogonal to each other and we could expand u in
terms of v+ and v−:
u = α+v+ + α−v−
such expansion being well-deﬁned, since the basis v+, v− is orthonormal.
Now we wish to express α1 and α2 via α+ and α−:
u = α+v+ + α−v− = α+
v1 + v2
|v1 + v2|
+ α−
v1 − v2
|v1 − v2|
=
7.11.
ILL-CONDITIONING OF DIAGONAL FORM
261
=
(cid:18) α+
|v1 + v2|
+
α−
|v1 − v2|
(cid:19)
v1 +
(cid:18) α+
|v1 + v2|
−
α−
|v1 − v2|
(cid:19)
v2
from where
α1 =
α2 =
α+
|v1 + v2|
α+
|v1 + v2|
+
−
α−
|v1 − v2|
α−
|v1 − v2|
Since α+ and α− are coordinates in an orthonormal basis, both α+ and α− are
taking values of comparable orders of magnitude, bounded by the length of the
vector u. On the other hand, since |v1 − v2| ≈ 0, the values of α1 and α2 will
get extremely large, unless α− is very small.
Now consider a conversion from the basis v1, v2 to a more “decent” basis,
e.g. to v+, v−. Expressing v1, v2 via v+, v−, we have
v1 = β+v+ + β−v−
v2 = β+v+ − β−v−
where β+ ≈ 1 and β− ≈ 0. Therefore
u = α1v1 + α2v2 = α1 (β+v+ + β−v−) + α2 (β+v+ − β−v−) =
= (α1 + α2)β+ · v+ + (α1 − α2)β− · v− = α+v+ + α−v−
As we have noted, usually α1 and α2 are having very large magnitudes, while
+ + α2
α2
− ≤ |u|. This means that usually α1 and α2 are having opposite signs,
in order to have |(α1 + α2)β+| < 1, since β+ ≈ 1. Respectively their diﬀerence
α1 − α2 is usually having a very large magnitude which is being compensated
by the multiplication by β− = 0.
Thus, the problematic equation is
α+ = (α1 + α2)β+ ≈ α1 + α2
where we add two very large numbers of opposite sign in order to obtain a
value of α+ of a reasonable magnitude. Such computations are associated with
large numeric precision losses. Choosing diﬀerent lengths for v1 and v2 will
not change the picture, we still will need to obtain α+ as the sum of the same
opposite values of a much larger magnitude.
A conversion from the basis v1, v2 to a “decent” basis other than v+, v−
can be viewed as converting ﬁrst to v+, v− and then to the desired basis.
Apparently, converting from one “decent” basis to another “decent” one neither
introduces new precision-related issues, nor removes the already existing ones.
Now realize, that essentially we have just been analysing the precision issues
arising in the transformations from the original to the diagonalizing basis and
back. It’s just that we have restricted the analysis to a particular subspace of
the state space, but the transformation which we have been analysing was a
diagonalizing transformation of the entire space. We have therefore determined
that there are range and precision issues arising in the diagonalizing transfor-
mation when two eigenvectors become close to each other. We have also found
out that this situation always occurs in non-degenerate cases of poles getting
262
CHAPTER 7. STATE-SPACE FORM
close to each other. Thus, diagonal form becomes ill-conditioned if the poles are
located close to each other, the eﬀects of ill-conditioning being huge precision
losses and the values possibly going out of range. Jordan cells of size larger than
1 are nothing more than a limiting case of this ill-conditioned situation, where
a diﬀerent choice of basis avoids the precision issues.
The reader may also recall at this point the ill-conditioning in the analysis
of the transient response of the 2-pole ﬁlters, which occurs at R ≈ 1, when both
poles of the system coincide on the real axis. That was exactly the same eﬀect
as the one which we analysed in this section.
7.12 Time-varying case
Until now we have been assuming that the system coeﬃcients are not changing.
If the system coeﬃcients are varying with time, then quite a few of the previously
derived statements do not hold anymore. This also causes problems with some
of the techniques. The fact that the transfer function doesn’t apply in the time-
varying case should be well-known by now, however the other issues arising out
of parameter variation are not that obvious. Let’s look through them one by
one.
Basis change
If the matrix A is varying with time, we might need T to vary with time as well,
e.g. if T is a matrix of the diagonalizing transformation. However, if T is not
constant anymore, the transformations of (7.8) get a more complicated form,
since instead of
(cid:0)T −1u(cid:48)(cid:1) = T −1 ˙u(cid:48)
d
dt
we are having
d
dt
Thus (7.8) transforms as
(cid:0)T −1u(cid:48)(cid:1) = T −1 ˙u(cid:48) +
d
dt
T −1 · u(cid:48)
respectively yielding
and
T −1 ˙u(cid:48) +
d
dt
T −1 · u(cid:48) = AT −1u(cid:48) + Bx
T −1 ˙u(cid:48) =
(cid:18)
AT −1 −
(cid:19)
T −1
d
dt
u(cid:48) + Bx
(cid:18)
˙u(cid:48) =
T AT −1 − T
(cid:19)
T −1
d
dt
u(cid:48) + T Bx
Thus the ﬁrst of the equations (7.9) is changed into
A(cid:48) = T AT −1 − T
d
dt
T −1
(7.30)
The extra term in (7.30) is the main reason why diﬀerent topologies have dif-
If two systems are to share the same transfer
ferent time-varying behavior.
7.12. TIME-VARYING CASE
263
function, they need to share the poles. In this case the matrices A and A(cid:48) have
the same diagonal or Jordan normal form (unless the system order is degenerate)
and are therefore related by a similarity transformation. Given that B, C and
B(cid:48), C (cid:48) are related via the same transformation matrix according to (7.9), the
diﬀerence between the two systems will be purely the one of a diﬀerent state-
space basis, and we would expect a fully identical behavior of both. However,
in order to have identical time-varying behavior, the matrices A and A(cid:48) would
need to be related via (7.30) rather than via a similarity transformation. In fact
(7.30) cannot hold, unless at least one of the matrices A and A(cid:48) depends not
only on some externally controlled parameters (such as cutoﬀ and resonance),
but also on their derivatives, which is a highly untypical control scenario.
Transient response
In the derivation of the transient response in Section 7.7 we have been using the
fact that
(cid:0)e−Atu(cid:1) = e−At ˙u − e−AtAu
d
dt
However if A is not constant then the above needs to be written as
(cid:0)e−Atu(cid:1) = e−At ˙u −
d
dt
(cid:19)
· u
e−At
(cid:18) d
dt
We might want to rewrite the derivative of e−At as
(cid:19)
e−At
(cid:18) d
dt
= e−At d
dt
(−At) = e−At ·
(cid:18)
−A − t
(cid:19)
d
dt
A
but actually we cannot do that, since we don’t know whether the derivative of
−At will commute with At. Thus, our derivation of the transient response stops
right there.7
Diagonal form
Given that we are using a diagonal form as a replacement for another non-
diagonal system, we already know that such replacement changes the time-
varying behavior of the system due to the extra term in (7.30).
A more serious problem occurs in this situation if we want to go through
parameter ranges where the system poles get close or equal to each other. Such
situation is unavoidable if we want a pair of mutually conjugate complex poles
of a real system to smoothly change into real poles, since such poles would need
to become equal on the real axis before they can go further apart. As we have
found out, the diagonal form doesn’t support the case of coinciding poles in
a continuous manner, since switching from poles to Jordan cells on the main
diagonal is a non-continuous transformation of the state space.
7Notably, the same was the case for our transient response derivations for 1- and 2-pole
cases, where we were assuming the fixed values of system parameters. Except for the 1-pole
case, where the only available freedom degree in the 1 × 1 matrix A could be represented as
the cutoff, leading to an equivalent representation of the modulation via time-warping.
264
CHAPTER 7. STATE-SPACE FORM
Cutoff modulation
If all cutoﬀ gains are identical and precede the integrators, it is convenient to
factor them out of matrices A and B:
˙u = ωc · (Au + Bx)
y = Cu + Dx
(7.31a)
(7.31b)
If the cutoﬀ is varying with time, we could explicitly reﬂect this in the ﬁrst
equation, where we can also let B (but not A) vary with time:
d
dt
u(t) = ωc(t) · (Au(t) + B(t)x(t))
Introducing dτ = ωc(t)dt we have
d
dτ
u(t(τ )) = Au(t(τ )) + B(t(τ ))x(t(τ ))
or
where
d
dτ
˜u(τ ) = A˜u(τ ) + ˜x(τ )
(7.32)
˜u(τ ) = u(t(τ ))
˜x(τ ) = B(t(τ ))x(t(τ ))
Thus, as we have already shown in Section 2.16, cutoﬀ modulation is expressible
as a warping of the time axis, provided cutoﬀ is uniformly positive
(cid:90)
τ (t) =
ωc(t)dt
where ωc(t) ≥ ω0 > 0
where the time-warped system deﬁned by (7.32) is time-invariant.
Note that cutoﬀ modulation in (7.31) is a transformation of A which changes
its eigenvalues but not its eigenvectors. Thus, if we diagonalize the system by a
basis change, the new basis can stay unchanged, and there will not be the extra
term in (7.30). Respectively, the diagonalized system will stay fully equivalent
to the original one, even though the cutoﬀ is being modulated. Apparently the
diagonalized system also can be written in the factored-out-cutoﬀ form (7.31).
Equivalence of systems under cutoff modulation
It’s not diﬃcult to realize that the equivalence under the condition of cutoﬀ
modulation in (7.31) holds not only between the original system and its diago-
nalized version, but between any two systems related by a basis change, since
the cutoﬀ modulation is not aﬀecting the transformation between the two sys-
tems. Suppose we are having two systems sharing the same transfer function.
In such case they have an equivalent behavior in the time-invariant case, but
we wish to have it equivalent in the time-varying case too. More speciﬁcally, we
would like make the second system have the time-varying behavior of the ﬁrst
one.
Since the transfer function is the same, both systems share the same diagonal
form up to the ordering and the lengths of the basis vectors. The transformations
between both systems and the shared diagonal form are cutoﬀ-independent and
therefore the systems are equivalent.
7.12. TIME-VARYING CASE
265
Equivalence under other modulations
We have already shown that two systems sharing the same transfer function
are equivalent under the cutoﬀ modulation (7.31). We often would wish to also
analyse for the equivalence under modulation of other parameters. Generally
this will not be the case, but the state-space form techniques may allow us to
ﬁnd out more details about the speciﬁc diﬀerences between the systems.
In
order to demonstrate some of the analysis possibilities, we are going to analyse
the TSK allpass (Fig. 7.1), which we have been converting to the state-space
form in Section 7.2.
Taking (7.4) let’s replace the feedback amount k with damping R. From
(5.17) we are having
= 1 +
= 1 − R
2k
k + 1
1
1 + k
=
k − 1
k + 1
1
1 + 1−R
1+R
1 − R
1 + R
k − 1 =
− 1 =
=
1 + R
1 + R + 1 − R
=
R + 1
2
1 − R − 1 − R
1 + R
= −
2R
1 + R
and thus (7.4) turns into
˙u1 = −Ru1 + (R − 1)u2 +
˙u2 = (R + 1)u1 − Ru2 −
y = −
4R
R + 1
u1 +
4R
R + 1
u2 + x
R + 1
2
R + 1
2
x
x
(7.33a)
(7.33b)
(7.33c)
Looking at the output mixing coeﬃcients we notice a strong similarity to (4.23)
where we subtract the bandpass signal (which, as we should remember, is ob-
tained directly from one of the state variables of an SVF) from the input, the
bandpass signal being multiplied by 4R. On the other hand for the TSK allpass
we have just obtained (7.33c):
y = −
4R
R + 1
u1 +
4R
R + 1
u2 + x = x −
4R
R + 1
(u1 − u2)
This motivates to attempt an introduction of new state variables, where one of
the variables will be a diﬀerence of u1 and u2. We expect this variable to behave
somewhat like an SVF bandpass signal.
Attempting to turn 4R/(R+1)·(u1−u2) into exactly 4Ru(cid:48)
1 (which is what we
would have had for an SVF) might be not the best idea, since the transformation
would be dependent on R, and it would be diﬃcult to assess possible implications
of such dependency. Instead we want something which is proportional to u1−u2,
but the transformation should be independent of R. This is achieved by e.g.
u1 = u(cid:48)
u2 = u(cid:48)
2 + u(cid:48)
1
2 − u(cid:48)
1
which implies u(cid:48)
1 = (u1 − u2)/2. Applying this transformation to (7.33) we have
2 + ˙u(cid:48)
˙u(cid:48)
1 = −R(u(cid:48)
2 + u(cid:48)
1) + (R − 1)(u(cid:48)
2 − u(cid:48)
1) +
R + 1
2
x =
266
CHAPTER 7. STATE-SPACE FORM
2 +
R + 1
x
1) − R(u(cid:48)
R + 1
2
4R
R + 1
1) +
2 −
x
= (1 − 2R)u(cid:48)
1 − u(cid:48)
2 − ˙u(cid:48)
˙u(cid:48)
1 = (R + 1)(u(cid:48)
2 + u(cid:48)
2 − u(cid:48)
1) −
R + 1
2
x =
= (1 + 2R)u(cid:48)
1 + u(cid:48)
y = −
4R
R + 1
from where
(u(cid:48)
2 + u(cid:48)
(u(cid:48)
2 − u(cid:48)
1) + x = −
8R
R + 1
u(cid:48)
1 + x
2 ˙u(cid:48)
2 ˙u(cid:48)
1 = −4Ru(cid:48)
2 = 2u(cid:48)
1
1 − 2u(cid:48)
2 + (R + 1)x
y = −
8R
R + 1
u(cid:48)
1 + x
or
1 − u(cid:48)
2 +
1 = −2Ru(cid:48)
˙u(cid:48)
2 = u(cid:48)
˙u(cid:48)
1
R + 1
2
x
y = −
8R
R + 1
u(cid:48)
1 + x
Now this looks very much like an SVF allpass, except that the input signal has
been multiplied by (R + 1)/2 and the bandpass signal is respectively multiplied
by 8R/(R + 1) instead of multiplying by 4R (Fig. 7.3). Note that the product of
pre- and post-gains is still 4R, exactly what we would normally use to build an
SVF allpass. Thus, the only diﬀerence between the SVF allpass and the TSK
allpass is the distribution of the pre- and post-bandpass gains.
•(cid:47)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
R+1
2
SVF BP
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
8R
R+1
Figure 7.3: An equivalent representation of the allpass TSK ﬁlter
from Fig. 5.35 using an SVF bandpass.
We could also cancel the denominator 2 of the pre-gain with the numerator
of the post-gain (Fig. 7.4). Since 2 is a constant, “sliding” it through the SVF
bandpass system eﬀectively just rescales the internal state of the SVF by a factor
of 2 (without introducing any new time-varying eﬀects), but this rescaling is then
compensated in the post-gain. Thus the system in Fig. 7.4 is fully equivalent to
the one in Fig. 7.3.
7.13 Discrete-time case
Discrete-time block diagrams can be converted to the discrete-time version of
the state-space form, which is also referred to as the difference state-space form.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
7.13. DISCRETE-TIME CASE
267
•(cid:47)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
R+1
SVF BP
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
4R
R+1
Figure 7.4: An equivalent modiﬁcation of Fig. 7.3.
The main principles are the same, except that instead of Au + Bx delivering the
input signals of the integrators, it delivers the input signals of the unit delays.
The same values will occur at the outputs of the unit delays one sample later,
thus the ﬁrst state-space equation takes the form
u[n + 1] = Au[n] + Bx[n]
The second equation is the same as in the continuous-time case:
y[n] = Cu[n] + Dx[n]
Writing both equations together we obtain the discrete-time state-space form:
u[n + 1] = Au[n] + Bx[n]
y[n] = Cu[n] + Dx[n]
(7.34a)
(7.34b)
Transfer matrix
Substituting the complex exponential signal x[n] = X(z)zn into (7.34) we obtain
from where
U(z)zn+1 = AU(z)zn + BX(z)zn
Y(z)zn = CU(z)zn + DX(z)zn
zU(z) = AU(z) + BX(z)
Y(z) = CU(z) + DX(z)
From the ﬁrst of the equations we have
(z − A)U(z) = BX(z)
U(z) = (z − A)−1BX(z)
(7.35)
Substituting this into the second equation we have
Y(z) = C(z − A)−1BX(z) + DX(z)
and thus
where
Y(z) = H(z)X(z)
H(z) = C(z − A)−1B + D =
C adj(z − A)B
det(z − A)
+ D
therefore the eigenvalues of A are the system poles.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
268
CHAPTER 7. STATE-SPACE FORM
Transient response
Substituting the complex exponential input x[n] = X(z)zn into (7.34a) we can
rewrite (7.34a) as
u[n + 1] = Au[n] + BX(z)zn
or as
where
u[n] = Au[n − 1] + BX(z)zn−1 = Au[n − 1] + qzn
(7.36)
q = BX(z)z−1
Recursively substituting (7.36) into itself at progressively decreasing values of
n we obtain
u[n] = Au[n − 1] + qzn =
= A (cid:0)Au[n − 2] + qzn−1(cid:1) + qzn =
= A2u[n − 2] + (cid:0)Az−1 + 1(cid:1) qzn =
= A2 (cid:0)Au[n − 3] + qzn−2(cid:1) + (cid:0)Az−1 + 1(cid:1) qzn =
= A3u[n − 3] +
(cid:16)(cid:0)Az−1(cid:1)2
+ Az−1 + 1
(cid:17)
qzn =
. . .
= Anu[0] +
= Anu[0] +
+ (cid:0)Az−1(cid:1)n−2
(cid:16)(cid:0)Az−1(cid:1)n−1
(cid:16)
1 − (cid:0)Az−1(cid:1)n(cid:17) (cid:0)1 − Az−1(cid:1)−1
qzn =
+ . . . + Az−1 + 1
(cid:17)
qzn =
= Anu[0] + (zn − An) (z − A)−1 qz =
= Anu[0] + (zn − An) (z − A)−1 BX(z) =
= (z − A)−1 BX(z)zn + An (cid:16)
= us[n] + An (u[0] − us[0])
u[0] − (z − A)−1 BX(z)
(cid:17)
=
where
us[n] = (z − A)−1 BX(z)zn = (z − A)−1 Bx[n]
is the steady-state response (compare to the transfer matrix for u in (7.35)),
respectively
ut[n] = An (u[0] − us[0])
(7.37)
The generalization to arbirary signals x[n] is done in the same way as in the
continuous-time case. The steady-state and transient responses for y are triv-
ially obtained from those for u.
Stability
Considering the transient response in (7.37), we could diagonalize the system
by a change of basis. If diagonalization is successful, then it’s obvious than An
decays to zero if and only if |pn| < 1 ∀n and grows to inﬁnity if ∃pn : |pn| > 1.
Since neither the system poles nore the decaying of the transient response to
zero depend on the basis choice, we have thereby established the criterion of
stability of discrete time systems.
7.14. TRAPEZOIDAL INTEGRATION
269
The non-diagonalizable case can be handled by using Jordan normal form,
where the discrete-time Jordan 1-poles of the Jordan chains will be stable if and
only if |pn| < 1 ∀n.
7.14 Trapezoidal integration
Writing (7.31) in an integral form we have
(cid:90)
u =
ωc (Au + Bx) dt
y = Cu + Dx
(7.38a)
(7.38b)
On the other hand, expressing direct form I trapezoidal integration Fig. 3.8 in
equation form we have
y[n] = y[n − 1] +
x[n − 1] + x[n]
2
T
(7.39)
Applying (7.39) to the integral in (7.38a) we obtain
u[n] = u[n − 1] + ωc
A(u[n] + u[n − 1]) + B(x[n] + x[n − 1])
2
T
from where
(cid:18)
1 −
and
ωcT
2
(cid:19)
(cid:18)
A
u[n] =
1 +
(cid:19)
A
u[n − 1] +
ωcT
2
ωcT
2
B(cid:0)x[n] + x[n − 1](cid:1)
(cid:18)
u[n] =
1 −
ωcT
2
A
(cid:19)−1 (cid:18)(cid:18)
1 +
ωcT
2
(cid:19)
A
u[n − 1] +
ωcT
2
B(cid:0)x[n] + x[n − 1](cid:1)
(cid:19)
(7.40)
Equation (7.40) is the resolved zero-delay feedback equation for the state-space
form (7.31) (or, equivalently (7.38)). Since we have used direct form I inter-
gators, it needs additional state variables for the storage of the previous input
values, which we could have spared if direct form II or transposed direct form
II integration was used.
Let’s apply trasposed direct form II integration (3.3) to the integral in
(7.38a). Apparently, we have a notation clash, since in (3.3) the variable u
is an internal variable of the integrator. Notating this internal variable as v and
notating the input signals of the integrators as 2w, and also not forgetting to
introduce a non-unit sampling period T , we obtain from (3.3) a set of equations:
u[n] = v[n − 1] + w[n]
obtained from (3.3a)
v[n] = u[n] + w[n]
ωcT
2
w[n] =
(Au[n] + Bx[n])
obtained from (3.3b)
obtained from (7.38a)
Solving for w[n] we have
w[n] =
ωcT
2
(cid:0)A(w[n] + v[n − 1]) + Bx[n](cid:1)
270
CHAPTER 7. STATE-SPACE FORM
where v[n − 1] are the previous states of the integrators, and respectively
(cid:18)
1 −
ωcT
2
(cid:19)
A
w[n] =
ωcT
2
(cid:0)Av[n − 1] + Bx[n](cid:1)
and
(cid:18)
w[n] =
1 −
ωcT
2
(cid:19)−1 ωcT
2
A
(cid:0)Av[n − 1] + Bx[n](cid:1)
(7.41)
Equation (7.41) is another variant of the resolved zero-delay feedback equa-
tion (7.40), this time written for transposed direct form II form. The beneﬁt,
compared to (7.40), is that we only need to store the previous states of the
integrators v[n − 1].
Since M −1 = adj M/ det M , the denominator of both equations (7.40) and
(7.41) is det(1 − ωcT /2 · A). Since det(λ − M ) = 0 is the eigenvalue equation,
the denominator turns to zero when 1 becomes an eigenvalue of ωcT /2 · A, or
respectively when 2/T becomes an eigenvalue of ωcA. Thus, we have a limitation
ωc · max
pn∈R
{pn} < 2/T
(7.42)
under which the system doesn’t get instantaneously unstable. Apparently ωcpn
are simply the poles of the system, thus (7.42) simply states that the real poles
of the system must be located to the left of 2/T .8
SUMMARY
The state-space form essentially means writing the system as a diﬀerential (or
diﬀerence, in the discrete-time case) equation system in a matrix form. Thereby
we have a compact abstract representation of the system, which, diﬀerently
from to the transfer function, doesn’t lose essential information about the time-
varying behavior. A particularly useful way to approach the state-space form
analysis is by diagonalizing the matrix, which essentially separates the eﬀects
of diﬀerent poles of the system from each other.
8Of course if there are complex poles sufficiently close to the real semiaxis [2/T, +∞), the
performance of trapezoidal integration is also questionable.
Chapter 8
Raising the filter order
As the order of the ﬁlter grows, there are more and more diﬀerent choices of
the transfer function. Particularly, there is more than one way to introduce the
resonance into a transfer function of order higher than 2. Some of the most
interesting options were already discussed in the previous chapters.
We have also introduced the state-space form as a general representation
for diﬀerential systems. However, being so general, the state-space form leaves
lots of open questions in regards to the choice of topology and the user-facing
parameters.
In this chapter we are going to discuss a number of standard topologies which
can be used to construct a system of any given order and also a number of ways
to map commonly used user-facing parameters, such as cutoﬀ and resonsance,
to the internal parameters of such systems. Note, however, that these structures
and techniques are useful only occasionally, for rather speciﬁc purposes.
8.1 Generalized SVF
We have seen that the idea of the ladder ﬁlter can be generalized from a 4-pole
to other numbers of poles, even though there are problems arising at pole counts
other than 4. Could we somehow attempt to generalize the SVF?
The most natural way to generalize the SVF is probably to treat it as the so-
called controllable canonical form, (Fig. 8.1) which is the analog counterpart of
direct form II (Fig. 3.33). Apparently, the main diﬀerence between Fig. 3.33 and
Fig. 8.1 is simply that all unit delays are replaced by integrators. The other
diﬀererence, namely the inverted feedback is merely a matter of convention,
resulting in opposite signs of the coeﬃcients an compared to what they would
have been in the absence of the feedback inversion. We chose the convention
with the inverted feedback mainly because it’s more similar to the 2-pole SVF
structure in Fig. 4.1.
The controllable canonical form allows to implement an arbitrary transfer
function of N -th order (the requirement that the transfer function is a non-
strictly proper rational function being implicitly understood). Indeed, it’s not
271
272
CHAPTER 8. RAISING THE FILTER ORDER
y(t)
+
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
b0
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
x(t)
+
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
b1
+
. . .
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
b2
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
bN
(cid:82)(cid:47)
•(cid:47)
(cid:82)(cid:47)
•(cid:47)
. . .
(cid:82)
•(cid:47)
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
a1
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
a2
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
aN
+
+
. . .
Figure 8.1: Generalized SVF (controllable canonical form).
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
diﬃcult to ﬁgure out that the tranfer function of the system in Fig. 8.1 is
N
(cid:88)
bns−n
N
(cid:88)
bnsN −n
H(s) =
n=0
N
(cid:88)
1 +
n=1
ans−n
=
n=0
N
(cid:88)
1 +
n=1
=
ansN −n
sN +
N
(cid:88)
n=0
bN −nsn
N −1
(cid:88)
n=0
aN −nsn
Thus an and bn are simply the denominator and numerator coeﬃcients of the
transfer function. Notice that bn are essentially modal pickups and we can share
the feedback part of the structure (consisting of integrators and an gains) among
several diﬀerent sets of pickup coeﬃcients bn to simultaneously implement a
number of ﬁlters sharing a common denominator.
Normally Fig. 8.1 assumes unit-cutoﬀ integrators, because the an and bn
coeﬃcients provide enough freedom to implement any transfer function of the
given order. However, in music DSP applications cutoﬀ control is a common
feature, therefore we could also allow the integrators to take identical non-unit
cutoﬀs. Further, letting N = 2, a2 = 1 and a1 = 2R we obtain an SVF with bn
serving as modal mixing coeﬃcients for HP, BP and LP outputs. On the other
hand, at N = 1, a1 = 1 we obtain the 1-pole ﬁlter we discussed in the beginning
of this book.
Generally, letting aN have a ﬁxed value is a good way to remove the re-
dundancy introduced into the system control by the embedded cutoﬀs of the
integrators. It is not diﬃcult to realize that
aN =
(cid:89)
(−pn)
where pn are the positions of the system poles when ωc = 1. Notably, although
it is mostly academic, this also can support the case of real poles of opposite
signs, which cannot be implemented by a classical 2-pole SVF due to aN being
ﬁxed to 1.
Unfortunately, there is no clear answer to what the coeﬃcients an should
be for N > 2. The simplicity of the 2-pole case was due to the fact that
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
8.2. SERIAL CASCADE REPRESENTATION
273
the denominator of a 2-pole transfer function essentially has only 2 degrees of
freedom (corresponding to a1 and a2), one degree being taken by the cutoﬀ, and
we are being left with the remaining degree which just happens to correspond to
the resonance. With the 1-pole there was only one freedom degree, being taken
by the cutoﬀ. At N > 2 there are too many diﬀerent options of how to map the
freedom degrees to ﬁlter control parameters and there is no deﬁnite answer to
that, although some of the options will be discussed later in this chapter.
With the numerator coeﬃcients bn there is a bit more clarity, as there are
certain general considerations applying more or less for any choice of an. E.g.
if the numerator is equal to aN , we get some kind of an N -th order lowpass,
since H(0) = 1 and H(s) ∼ aN /sN for s → ∞. For the sN numerator we
have H(∞) = 1 and H(s) ∼ sN /aN for s → 0, corresponding to some kind
of an N -th order highpass. For an even N and an a1/2
N sN/2 numerator we get
H(s) ∼ sN/2/a1/2
N /sN/2 for s → ∞, corresponding
to some kind of a bandpass. This however deﬁnes only the asymptotic behavior
at 0 and ∞, the amplitude response shape in the middle can be pretty much
arbitrary, being deﬁned by the denominator.
N for s → 0 and H(s) ∼ a1/2
By transposing the controllable canonical form one obtains the so-called
observable canonical form. We are not going to address it in detail, as most of
the discussion of the controllable canonical form above applies to the observable
canonical form as well.
8.2 Serial cascade representation
Another structure which allows implementing arbitrary transfer functions is the
serial cascade. It is probably the one most commonly used. Compared to the
generalized SVF, in the serial cascade representation we are using only 1- and
2-pole ﬁlters and we can choose commonly known and well-studied structures
to implement those.1 The beneﬁt compared to the parallel implementation
(discussed later in this chapter) is that the serial cascade form doesn’t get ill-
conditioned when system poles get close to each other.
Cascade decomposition
Given an arbitrary N -th order real transfer function, let’s write it in the multi-
plicative form:
H(s) = g ·
Nz(cid:89)
(s − zn)
n=1
Np
(cid:89)
(s − pn)
n=1
(8.1)
where Nz ≤ Np, since H(s) must be nonstrictly proper. Since H(s) has real
coeﬃcients, all complex poles of H(s) will come in conjugate pairs, and the
same can be said about the zeros.
1Serial cascade implementation is especially popular in classical DSP, since direct forms
commonly used there are reportedly starting to have more issues as the filter order grows,
although the author didn’t verify that by his own experiments.
274
CHAPTER 8. RAISING THE FILTER ORDER
Now we are going to write each pair of conjugate poles as a purely real
2nd-order factor in the denominator:
(s − p)(s − p∗) = s2 − s · 2 Re p + |p|2
and we are going to write each pair of conjugate zeros as a purely real 2nd-order
factor in the numerator:
(s − z)(s − z∗) = s2 − s · 2 Re z + |z|2
Further, if necessary, we can combine any two real poles into a 2nd-order factor
in the denominator:
(s − p1)(s − p2) = s2 − (p1 + p2) · s + p1p2
and we can combine any two real zeros into a 2nd-order factor in the numerator:
(s − z1)(s − z2) = s2 − (z1 + z2) · s + z1z2
Thus we can distribute all conjugate pair of poles and zeros into 2nd-order real
rational factors of the form
s2 + as + b
s2 + cs + d
unless we do not have enough zeros, in which case there will be one or more
2nd-order real rational factors of the form
s + b
s2 + cs + d
and/or
1
s2 + cs + d
The remaining pairs of real poles and zeros can be combined into 1st-order real
rational factors of the form
s + a
s + b
and/or
1
s + b
or they can be also combined into 2nd-order real rational factors, e.g.:
s + a1
s + b1
·
s + a2
s + b2
=
s2 + (a1 + a2)s + a1a2
s2 + (b1 + b2)s + b1b2
Thus the entire transfer function is represented as a product of purely real 2nd-
and 1st-order factors:
H(s) = g ·
N2(cid:89)
n=1
H2n(s) ·
N1(cid:89)
n=1
H1n(s)
(8.2)
where H2n(s) and H1n(s) are the 2nd- and 1st-order factors respectively. The
gain coeﬃcient g, if desired, can be factored into the numerator of one or several
of the factors H2n(s) and H1n(s), so that the product expression gets a simpler
form:
N2(cid:89)
N1(cid:89)
H(s) =
H2n(s) ·
H1n(s)
(8.3)
Now recall that 1-pole multimode can implement any stable real 1st-order trans-
fer function and SVF can implement any stable real 2nd-order transfer function.
n=1
n=1
8.2. SERIAL CASCADE REPRESENTATION
275
This means that we can implement pretty much any H(s) as a serial chain of
SVFs2 and 1st-order multimodes.3 We will refer to the process of representing
H(s) is a cascade form as cascade decomposition of H(s).
Cutoff control
The denominator 1 + s/ωc of a 1-pole ﬁlter is controlled by a single parameter,
which is the ﬁlter cutoﬀ. The denominator 1+2Rs/ωc +(s/ωc)2 of a 2-pole ﬁlter
is controlled by cutoﬀ and damping. Thus each of the 2- and 1-poles in (8.3)
has a cutoﬀ, deﬁned by the positions of the respective poles. Writing explicitly
these cutoﬀ parameters in (8.3) we obtain
H(s) =
N2(cid:89)
n=1
¯H2n(s/ω2n) ·
N1(cid:89)
n=1
¯H1n(s/ω1n)
where ¯H2n and ¯H1n are unit-cutoﬀ versions of the same 2- and 1-poles and ω2n
and ω1n are the respective cutoﬀs.
Suppose the above H(s) deﬁnes a unit-cutoﬀ ﬁlter. Then non-unit cutoﬀ for
H(s) is achieved by
H(s/ωc) =
N2(cid:89)
n=1
¯H2n(s/ωcω2n) ·
N1(cid:89)
n=1
¯H1n(s/ωcω1n)
(8.4)
which means that the cutoﬀs of the underlying 2- and 1-poles are simply mul-
tiplied by ωc and we have ωcω2n and ωcω1n as the 2- and 1-pole cutoﬀs.
One should remember, that it is important to apply one and the same pre-
warping for all ﬁlters in the cascade, as discussed in Section 3.8. E.g. we could
choose to prewarp (8.4) at ω = ωc, which means that we prewarp only ωc
(rather than individually prewarping the 2- and 1-pole cutoﬀs ωcω2n and ωcω1n),
thereby obtaining its prewarped version ˜ωc, and then simply substitute ˜ωc for
ωc in (8.4):
H(s/˜ωc) =
N2(cid:89)
n=1
¯H2n(s/˜ωcω2n) ·
N1(cid:89)
n=1
¯H1n(s/˜ωcω1n)
Thus, the 2- and 1-pole cutoﬀs become ˜ωcω2n and ˜ωcω1n respectively.
Cascaded model of a ladder filter
As an example of the just introduced technique we are going to implement the
transfer function of a 4-pole lowpass ladder ﬁlter by a serial chain of two SVFs.
A 4-pole lowpass ladder ﬁlter has no zeros and two conjugate pairs of poles
for k > 0. By considering two coinciding poles on a real axis also as mutually
conjugate, we can assume k ≥ 0.
2Of course a multimode TSK, a multimode SKF, or any other 2nd-order filter with sufficient
freedom in transfer function parameters would do instead of an SVF.
3Apparently H(s) can be implemented by 1-poles and SVFs if its factors can be imple-
mented by 1-poles and SVFs. Those which can not, can be implemented by generalized SVFs.
276
CHAPTER 8. RAISING THE FILTER ORDER
Since there are no zeros, we simply need a 2-pole lowpass SVF for each
1, p2, p∗
2 be the poles of the ladder ﬁlter.
conjugate pair of poles. Let p1, p∗
According to (5.2)
p1,2 = −1 +
±1 + j
√
2
k1/4
(8.5)
By (4.13), the cutoﬀs of the 2-pole lowpasses ω1,2 = |p1,2| and R = − Re p1,2/|p1,2|.
Respectively the transfer function of the ladder ﬁlter can be represented as
H(s) = g
1
(cid:19)2
(cid:18) s
ω1
+ 2R1
s
ω1
+ 1
·
(cid:19)2
(cid:18) s
ω2
1
+ 2R2
s
ω2
+ 1
(8.6)
The unknown gain coeﬃcient g can be found by evaluating (5.1) at s = 0,
obtaining the condition H(0) = 1/(1 + k). Evaluating (8.6) at s = 0 yields
H(0) = g. Therefore
g =
1
1 + k
This gives us a cascade of 2-poles implementing a unit-cutoﬀ ladder ﬁlter. Ex-
tending (8.6) to arbitrary cutoﬀs is respectively done by
H(s) =
1
1 + k
·
1
(cid:18) s
(cid:19)2
ωcω1
+ 2R1
s
ωcω1
+ 1
·
(cid:18) s
(cid:19)2
ωcω2
1
+ 2R2
s
ωcω2
+ 1
Cascaded multimode
The cascade decomposition can be also used to provide modal outputs, sharing
the same transfer function denominator. In order to demonstrate this we will
consider a serial connection of two SVFs.4
The transfer function of such structure can have almost any desired 4th or-
der stable denominator.5 We would like to construct modal outputs for such
connection, so that by mixing those modal signals we should be able to ob-
tain arbitrary numerators. This should allow us to share this chain of SVFs
for generation of two or more signals which share the same transfer function’s
denominator.
We have several options of connecting two SVFs in series, depending on
which of the modal outputs of the ﬁrst SVF is connected to the second SVF’s
input. The most symmetric option seems to be picking up the bandpass output
(Fig. 8.2).
Now let
D1(s) = s2 + 2R1ω1s + ω2
1
D2(s) = s2 + 2R2ω2s + ω2
2
be the denominators of the transfer functions of the two SVFs and let D(s) =
D1(s)D2(s) be their product. Writing out the transfer functions for the signals
4The idea to specifically address this is the book arose from a discussion with Andrew
Simper.
5Denominators not achievable by classical SVFs can be achieved by using generalized 2nd-
order SVFs.
8.2. SERIAL CASCADE REPRESENTATION
277
x(t)
SVF1
HP1
BP1
LP1
SVF2
HP2
BP2
LP2
Figure 8.2: A multimode cascade of two SVFs.
at the SVF outputs (in respect to the input signal x(t) in Fig. 8.2) we obtain
HLP1(s) =
HBP1(s) =
HHP1(s) =
HLP2(s) =
HBP2(s) =
HHP2(s) =
ω2
1
D1(s)
ω1s
D1(s)
s2
D1(s)
ω2
2
D2(s)
ω2s
D2(s)
s2
D2(s)
=
=
=
ω2
1D2(s)
D(s)
ω1sD2(s)
D(s)
s2D2(s)
D(s)
· HBP1(s) =
· HBP1(s) =
· HBP1(s) =
ω2
2ω1s
D(s)
ω2ω1s2
D(s)
ω1s3
D(s)
Or, since we have the common denominator D(s) everywhere, we could concen-
trate just on the numerators:
NLP1(s) = ω2
1D2(s)
NBP1(s) = ω1sD2(s)
NHP1(s) = s2D2(s)
NLP2(s) = ω2
2ω1s
NBP2(s) = ω2ω1s2
NHP2(s) = ω1s3
Noticing from Fig. 8.2 that BP1 can be obtained as LP2 + 2R2BP2 + HP2
anyway, we can drop the respective numerator from the list and try to arrange
the remaining ones in the order of the descending polynomial order:
2s2
NHP1(s) = s2D2(s) = s4 + 2R2ω2s3 + ω2
NHP2(s) = ω1s3
NBP2(s) = ω2ω1s2
NLP2(s) = ω2
2ω1s
1D2(s) = ω2
NLP1(s) = ω2
1s2 + 2R2ω2
1ω2s + ω2
1ω2
2
The last line doesn’t really ﬁt, and the ﬁrst one looks more complicated than
the next three, but we can ﬁx that by replacing the ﬁrst and the last lines by
linear combinations:
NHP1(s) − 2R2
ω2
ω1
NHP2(s) −
ω2
ω1
NBP2(s) = s4
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
278
CHAPTER 8. RAISING THE FILTER ORDER
NHP2(s) = ω1s3
NBP2(s) = ω2ω1s2
NLP2(s) = ω2
2ω1s
1ω2
NLP2(s) = ω2
2
NLP1(s) −
ω1
ω2
NBP2(s) − 2R2
ω1
ω2
Thus we can obtain all powers of s from linear combinations of LP1, HP1, LP2,
BP2 and HP2, thereby being able to construct arbitrary polynomials of orders
up to 4 for the numerator.
Notably, instead of connecting the bandpass output of the ﬁrst SVF to the in-
put of the second SVF, as it has been shown in Fig. 8.2, we could have connected
the lowpass or the highpass output. This would have resulted in somewhat dif-
ferent math, but essentially gives the same modal mixture options.
8.3 Parallel representation
Real poles
Given a transfer function which has only real poles which are all distinct, we
could expand it into a sum of 1st-order partial fractions. Each such 1st-order
fraction corresponds to a 1-pole and we could implement the transfer function
as a sum of 1-poles. Essentially this is identical to the diagonal state-space
form, which, provided all system poles are real and suﬃciently distinct (so that
no ill-conditioning occurs), is just a set of parallel Jordan 1-poles.
In the case of a single-input single-output system, which we are currently
considering, the transfer function of such diagonal system, given by (7.18), has
the form
H(s) =
N
(cid:88)
n=1
cnbn
s − pn
+ d
(8.7)
where bn and cn are the input and output gains respectively. Given a particular
nonstrictly rational H(s), the partial fraction expansion (8.7) uniquely deﬁnes
d and the products cnbn. The respective freedom of choice of cn and bn can be
resolved by letting bn = 1 ∀n and thus we control the numerator of the transfer
function by the output mixing coeﬃcients cn (Fig. 8.3).6
We could also replace Jordan 1-poles by ordinary 1-pole lowpasses, where we
need to divide the mixing coeﬃcients by the respective cutoﬀs ωcn (Fig. 8.4).
The global cutoﬀ control of the entire ﬁlter in Fig. 8.3 or Fig. 8.4 is achieved
in the same way as with serial cascades. Obviously, the usual consideration of
common prewarping of the 1-pole components applies here as well.
Complex poles
If system poles are complex we need to use the real diagonal form, which replaces
the complex Jordan 1-poles with Jordan 2-poles. For a single-input single-
6Of course, we could instead let cn = 1 and control the transfer function numerator by the
input gains bn, or distribute the control between bn and cn.
8.3. PARALLEL REPRESENTATION
279
x(t)
d
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
1
s−p1
1
s−p2
•(cid:47)
•(cid:15)
•(cid:15)
...
c1
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
c2
y(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
...(cid:79)
1
s−pN
cN
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
Figure 8.3: Implementation by parallel Jordan 1-poles.
d
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
c1/ωc1
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
c2/ωc2
x(t)
•(cid:47)
•(cid:15)
•(cid:15)
...
ωc1
s+ωc1
ωc2
s+ωc2
y(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
...(cid:79)
cN /ωcN
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
ωcN
s+ωcN
Figure 8.4: Implementation by parallel 1-pole lowpasses.
output system, equation (7.28) takes the form
H(s) =
(cid:88)
Im pn>0
αns + βn
s2 − 2 Re pn · s + |pn|2 +
(cid:88)
Im pn=0
cnbn
s − pn
+ d
(8.8)
We could obtain the explicit expressions for αn and βn from the derivation of
(7.28), but it would be more practical to simply obtain their values from the
partial fraction expansion of H(s). That is, given H(s), we ﬁnd αn and βn
(as well as, of course, cnbn and d) from (8.8). We also should remember that,
according to the freedom of choice of the state space basis vectors lengths, we
could choose any non-zero input gains vector, e.g. (cid:0)1
which means that we
0(cid:1)T
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
280
CHAPTER 8. RAISING THE FILTER ORDER
are using only the “real part” input of the Jordan 2-pole.7 According to (7.26),
the contribution of such Jordan 2-pole to H(s) will be
1
s2 − 2 Re pn · s + |pn|2
(cid:0)cn
(cid:1)
cn+1
cn(s − Re pn) + cn+1 Im pn
s2 − 2 Re pn · s + |pn|2 =
(cid:19)
(cid:18)s − Re pn − Im pn
s − Re pn
Im pn
cns + (cn+1 Im pn − cn Re pn)
s2 − 2 Re pn · s + |pn|2
(cid:19) (cid:18)1
0
=
=
Thus
from where
αn = cn
βn = cn+1 Im pn − cn Re pn
cn = αn
cn+1 =
βn + αn Re pn
Im pn
Thus, having found αn and βn, we can ﬁnd cn and cn+1. The respective struc-
ture is shown in Fig. 8.5. Notice that as Im pn becomes smaller, cn+1 becomes
larger. This is the ill-conditioning eﬀect of the diagonal form discussed in Sec-
tion 7.11.
d
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
H1
H3
(cid:47)Re
Im
(cid:47)Re
Im
Re
Im
Re
Im
x(t)
•(cid:47)
•
•
...
c1
c2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
c4
c3
y(t)
+
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
...(cid:79)
HN
cN
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
Figure 8.5: Implementation by parallel Jordan 2- and 1-poles. Dis-
connected imaginary part inputs are receiving zero signals.
Similarly to how we could replace Jordan 1-poles with ordinary 1-pole low-
passes, we could replace Jordan 2-poles by some other 2-poles, e.g. by SVFs.
Finding the output mixing coeﬃcients becomes simpler, since, apparently, the
coeﬃcients αn and βn in (8.8) now simply correspond to SVF bandpass and
lowpass output gains (properly scaled by the cutoﬀ). Fig. 8.6 illustrates.
7The dual approach would be to let the output mixing vector (cid:0)1
0(cid:1), in which case we
control the transfer function’s numerator by the input gains.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
8.4. CASCADING OF IDENTICAL FILTERS
281
x(t)
d
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
BP
LP
BP
LP
H1
H3
•(cid:47)
•(cid:15)
•(cid:15)
...
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
α1/ωc1
β1/ω2
c1
α3/ωc3
β3/ω2
c3
y(t)
+
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
...(cid:79)
HN
cN /ωcN
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
Figure 8.6: Implementation by parallel SVFs and 1-pole lowpasses.
Another beneﬁt of an SVF is that it doesn’t have a problem at the point
where its poles coincide and also can support the case of real poles, meaning
that we could convert arbitrary pairs of parallel 1-poles into an SVF. The same
apparently could be done by an SKF/TSK. There would still be a problem
though, if poles of diﬀerent parallel 2-poles coincide, resulting in the already
known ill-conditioning eﬀect.
Regarding the cutoﬀ control of the entire system, there is no diﬀerence from
the parallel 1-poles case.
Coinciding poles
Generally, anything with repeated or close to each other poles cannot be imple-
mented in a parallel form and needs some non-parallel implementation (SVF,
a chain of SVFs, Jordan chain, etc.) However the implementation could still
be partially parallel, where the poles may be repeated within each block, but
diﬀerent parallel blocks shouldn’t have poles at the same locations.
8.4 Cascading of identical filters
So we have learned a number of diﬀerent ways to implement higher-order trans-
fer functions, of which cascaded form is said to be usually the best option,
however, how do we construct these transfer functions in the ﬁrst place? E.g.
how do we generalize a resonating 2-pole transfer function to a 4-th or 8-th
order? Or how do we generalize a 1-st order lowpass to a 5-th or 8-th order?
One possible way which could immediately occur to us is to stack several
identical ﬁlters together. Note that, given a ﬁlter with the transfer function
G(s) and another one with the transfer function H(s) = GN (s) and looking at
their decibel-scale amplitude responses, we notice that the latter is simply the
former multiplied by N , that is the amplitude response becomes scaled N times
vertically (obviously, the same scaling is happening to the phase response).
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
282
CHAPTER 8. RAISING THE FILTER ORDER
Particularly this means that the rolloﬀ slope of the ﬁlter becomes N times
steeper.
Therefore in order to generalize a 1-st order lowpass 1/(1 + s) to the N -th
order we could simply connect N such lowpasses in series:
H(s) =
(cid:18) 1
(cid:19)N
1 + s
resulting in the amplitude response curve in Fig. 8.7. It looks as if the cutoﬀ of
H(s) = GN (s) is is too low. In principle we could address this by shifting the
ﬁlter cutoﬀ, so that |GN (j)| = 1/
2. In order to do so we solve the equation
√
1
|1 + jω|N =
1
√
2
obtaining the frequency which should be treated as the cutoﬀ point of each of
the chain’s elements:
so the transfer function becomes
ω =
(cid:112)
21/N − 1
H(s) =
(cid:18)
√
1 + s
1
21/N − 1
(cid:19)N
(8.9)
This looks a bit better (Fig. 8.8) and can be taken as a possible option.
|H(jω)|, dB
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 8.7: Amplitude response of a 1-pole lowpass ﬁlter (dashed)
vs. amplitude response of a serial chain of 4 identical 1-pole lowpass
ﬁlters (solid).
.
In the same way we could generalize a resonating 2-nd order lowpass 1/(1 +
2Rs + s2) to the 2N -th order by connecting N of such lowpasses together
H(s) =
(cid:18)
1
1 + 2Rs + s2
(cid:19)N
However in this case the situation is somewhat worse than with 1-poles. First
we notice that the resonance peak becomes much higher at the same damping
8.5. BUTTERWORTH TRANSFORMATION
283
|H(jω)|, dB
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 8.8: Amplitude response of a 1-pole lowpass ﬁlter (dashed)
vs. amplitude response of a serial chain of 4 identical 1-pole lowpass
ﬁlters with adjusted cutoﬀ (solid).
.
(Fig. 8.9). At ﬁrst sight it doesn’t look like a big problem, we could simply use
smaller values of the damping. However if we compare the ampiltude response
curves of a 2-pole vs. N stacked 2-poles with the damping adjusted to produce
the same peak height,8 we notice that due to the now smaller damping value
the resonance peak of the 2-pole chain is much wider than the peak of a single
2-pole (Fig. 8.10), all in all not a very desirable scenario.
8.5 Butterworth transformation
We have seen that cascading N identical ﬁlters is one possible way to obtain
higher-order ﬁlters, which eﬀectively scales the decibel-scale amplitude response
and the phase response of the ﬁlter N times vertically, respectively making the
ﬁlter rolloﬀ N times steeper.
Another way to make the rolloﬀ N times steeper would be ﬁnding a trans-
formation which shrinks the amplitude response in the logarithmic frequency
scale N times:
log ω ← N log ω
ω ≥ 0
N = 2, 3, 4, . . .
(where we don’t care about ω < 0 because for real ﬁlters |H(jω)| = |H(−jω)|,
and where log 0 = −∞). Or equivalently
ω ← ωN
ω ≥ 0
(8.10)
The readers may recall the LP to HP transformation s ← 1/s which ﬂips the
responses in the logarithmic frequency axis. One could try to draw an analogy
and attempt substitutions of the form s ← sN or s ← asN (a ∈ C, |a| = 1),
however it’s not diﬃcult to convince oneself that such substitutions do not work.
Nevertheless, the basic direction is mostly right. Just instead of of performing
8We can do this using formulas (4.7) and (4.8).
284
CHAPTER 8. RAISING THE FILTER ORDER
|H(jω)|, dB
+18
+12
+6
0
-6
-12
1/8
1
8
ω
Figure 8.9: Amplitude response of a 2-pole ﬁlter (dashed) vs. am-
plitude response of a serial chain of 4 identical 2-pole ﬁlters (solid).
.
|H(jω)|, dB
+12
+6
0
-6
-12
-18
1/8
1
8
ω
Figure 8.10: Amplitude response of a 2-pole ﬁlter (dashed) vs.
amplitude response of a serial chain of 4 identical 2-pole ﬁlters
with adjusted damping (solid).
.
an argument substitution on the transfer function, we will directly apply (8.10)
8.5. BUTTERWORTH TRANSFORMATION
285
to the amplitude response |H(jω)|. That is we will be looking for such H (cid:48)(s)
that
|H (cid:48)(jω)| = |H(jωN )|
ω ≥ 0
(8.11)
We will refer to the transformation of H(s) into H (cid:48)(s) deﬁned by (8.11) as
Butterworth transformation.9 The integer N will be respectively referred to
as the order of the Butterworth transformation. We will denote Butterworth
transformation as
H (cid:48)(s) = B [H(s)]
or, if we want to explicitly specify the order
H (cid:48)(s) = BN [H(s)]
where H (cid:48)(s) denotes the new transfer function obtained as the result of the
transformation.10
Without having developed the transformation details yet, we can already
establish several properties of this transformation, which follow from (8.11):
- the transformation doesn’t change a constant function:
B [a] = a
(8.12a)
- a constant gain can be simply factored out of the transformation:
B [g · H(s)] = g · B [H(s)]
(8.12b)
- a change of the cutoﬀ is shrunk N times in the logarithmic scale after the
transformation:
BN [H(s/a)] = BN [H(s)]
(cid:12)
(cid:12)
(cid:12)
(cid:12)s←s/a1/N
- the transformation commutes with LP to HP substitution
BN [H(1/s)] = BN [H(s)]
(cid:12)
(cid:12)
(cid:12)
(cid:12)s←1/s
- the transformation distributes over multiplication:
(8.12c)
(8.12d)
B [H1(s)H2(s)] = B [H1(s)] · B [H2(s)]
(8.12e)
- the transformation distributes over division:
B [H1(s)/H2(s)] = B [H1(s)] /B [H2(s)]
(8.12f)
9The term Butterworth transformation has been coined by the author and is originating
from the fact that this transformation, when applied to 1-pole filters, generates Butterworth
filters. At the time of the writing the author is not aware of this concept being described
elsewhere in the literature and would be thankful for any pointers to the commonly used
terminology, if any exists.
10Of course, (8.11) doesn’t uniquely define H (cid:48)(s). E.g. if H (cid:48)(s) satisfies (8.11), then so does
−H (cid:48)(s). In that sense Butterworth transformation is not uniquely defined. However during
the development of the Butterworth transformation we will suggest some default choices which
will work most of the time. Assuming these default choices, the Butterworth transformation
becomes uniquely defined.
286
CHAPTER 8. RAISING THE FILTER ORDER
- Butterworth transformations can be chained:
BN [BM [H(s)]] = BN ·M [H(s)]
(8.12g)
Since (8.11) doesn’t uniquely deﬁne the transformation result, the above
properties have to be understood in the sense that the right-hand side can be
taken as one possible result of the transformation in the left-hand side. However
the amplitude responses of the transformation results are uniquely deﬁned and
in those terms the above properties can be understood as usual equalities. E.g.
the property (8.12g) can be understood as
|BN [BM [H(s)]]| = |BN ·M [H(s)]|
∀s = jω, ω ∈ R
Instead of developing Butterworth transformation immediately for arbitrary
order ﬁlters we are going to ﬁrst ﬁnd a way to apply it to 1-pole ﬁlters and then
to 2-pole ﬁlters. At that point we will be able to simply use the property (8.12e)
to apply Butterworth transformation to arbitrary-order ﬁlters by representing
these arbitrary order ﬁlters as cascades of 1-st and 2-nd order ﬁlters.
8.6 Butterworth filters of the 1st kind
As we just mentioned, ﬁrst we will develop a way to apply Butterworth transfor-
mation to 1-pole ﬁlters, in which case we will more speciﬁcally refer to this trans-
formation as Butterworth transformation of the 1st kind. The results of But-
terworth transformation of the 1st kind coincide with ﬁlters commonly known
as Butterworth filters. However in this book later we will generalize the idea
of Butterworth ﬁlters to include the results of Butterworth transformation of
In order to be able to tell between diﬀerent
ﬁlters of orders higher than 1.
kinds of Butterworth ﬁlters, we are going to more speciﬁcally refer to the ﬁlters
obtained by Butterworth transformation of 1-pole ﬁlters as Butterworth filters
of the 1st kind.
Considering that a 1-pole transfer function is essentially a ratio of two 1st-
order polynomials
H(s) =
P1(s)
P2(s)
and that the amplitude response of H(s) can be written as a ratio of formal
amplitude responses of these polynomials:
|H(jω)| =
|P1(jω)|
|P2(jω)|
it is suﬃcient to develop the transformation for 1st-order polynomials. The
transformation of H(s) can be then trivially obtained as:
H (cid:48)(s) = B [H(s)] =
B [P1(s)]
B [P2(s)]
=
P (cid:48)
P (cid:48)
1(s)
2(s)
where P (cid:48)
1(s) and P (cid:48)
2(s) are transformed polynomials P1(s) and P2(s).
8.6. BUTTERWORTH FILTERS OF THE 1ST KIND
287
Transformation of polynomial P (s) = s + 1
We begin by obtaining the Butterworth transformation of the polynomial P (s) =
s + 1. Its formal amplitude response is
|P (jω)| =
(cid:112)
1 + ω2
and we wish to ﬁnd P (cid:48)(s) = B [P (s)] such that
|P (cid:48)(jω)| = |P (jωN )| =
(cid:112)
1 + ω2N
In order to get rid of the square root we can deal with squared amplitude
response instead
|P (jω)|2 = 1 + ω2
|P (cid:48)(jω)|2 = 1 + ω2N
Now we would like to somehow obtain P (cid:48)(s) from the latter equation.
In order to do so, let’s notice that
|P (jω)|2 = 1 + ω2 = 1 − (jω)2 = (1 + jω)(1 − jω) = P (jω)P (−jω) = Q(jω)
where Q(s) = P (s)P (−s), so the roots of Q(s) consist of the root of P (s) at
s = −1 and of its origin-symmetric image at s = 1, the latter being the root of
P (−s). This motivates to introduce Q(cid:48)(s) such that
Q(cid:48)(jω) = 1 + ω2N
and then try to factor it into P (cid:48)(s)P (cid:48)(−s) in such a way that
|P (cid:48)(jω)|2 = P (cid:48)(jω)P (cid:48)(−jω) = Q(cid:48)(jω)
In order to ﬁnd the possible ways to factor Q(s) into P (cid:48)(s)P (cid:48)(−s) let us ﬁnd
the roots of Q(s). Instead of solving Q(cid:48)(s) = 0 for s let’s solve Q(cid:48)(jω) = 0 for
ω, where we formally let ω take complex values. The solutions in terms of s are
related to the solutions in terms of ω through s = jω.
Solving Q(cid:48)(jω) = 1 + ω2N = 0 for ω we obtain
ω = (−1)1/2N = ejα
α = π
2n + 1
2N
= π
1
2 + n
N
n = 0, . . . , 2N − 1 (8.13)
The solutions are illustrated in Figs. 8.11 and 8.12 where the complex plane can
be alternatively interpreted in terms of s or in terms of ω (note the labelling of
the axes), thus these ﬁgures simultaneously illustrate the solutions in terms of
ω or in terms of s. Thus the 2N roots of Q(cid:48)(s) are equally spaced on a unit
circle with an angular step of π/N . If N is odd there will be roots at s = ±1
otherwise there are no real roots.
Another possible way to look at the solutions of Q(cid:48)(jω) = 0 is to rewrite the
equation 1 + ω2N = 0 as
1 + ω2N = ω2N − j2 = (ωN + j)(ωN − j) = 0
In this case the roots obtained from the equation ωN − j = 0 will be interleaved
with the roots obtained from the equation ωN + j = 0 (Figs. 8.11 and 8.12
288
CHAPTER 8. RAISING THE FILTER ORDER
Im s
(Re ω)
j
0
−j
−1
1
Re s
(−Im ω)
Figure 8.11: Roots of Q(cid:48)(s) for the Butterworth transformation
of the 1st kind of an even order (N = 6). White and black dots
correspond to even and odd roots.
Im s
(Re ω)
j
0
−j
−1
1
Re s
(−Im ω)
Figure 8.12: Roots of Q(cid:48)(s) for the Butterworth transformation
of the 1st kind of an odd order (N = 5). White and black dots
correspond to even and odd roots.
illustrate). Sometimes therefore such roots are referred to as even and odd
roots respectively, since they occur respectively at even and odd n in (8.13).
This distinction usually can be ignored, but occasionally becomes important.
Having found the roots of Q(cid:48)(s) how do we split them into the roots of P (cid:48)(s)
and the roots of P (cid:48)(−s)? Obviously we cannot do this splitting in an arbitrary
8.6. BUTTERWORTH FILTERS OF THE 1ST KIND
289
way, since there are several special properties which need to be satisﬁed.
- For any possible polynomial P (cid:48)(s) its roots are origin-symmetric to the
roots of P (cid:48)(−s), so our root splitting must respect this property.
- P (cid:48)(s) must be a real polynomial. This requires its roots to be either real
or coming in complex conjugate pairs.
- If P (cid:48)(s) is the denominator of the ﬁlter’s transfer function, then its roots
must be located in the left complex semiplane (in order for the ﬁlter to be
stable).
- The requrement |P (cid:48)(jω)|2 = P (cid:48)(jω)P (cid:48)(−jω) implies |P (cid:48)(jω)| = |P (cid:48)(−jω)|.
In order to satisfy the latter, the roots of P (cid:48)(s) must be symmetric to the
roots of P (cid:48)(−s) with respect to the imaginary axis (essentially it is the
same reasoning which we had in the discussion of minimum phase and
maximum phase zero positioning).
Looking at Figs. 8.11 and 8.12 it’s not diﬃcult to notice that all of the
above requirements will be satisﬁed if we choose the roots in the left complex
semiplane to be the roots of P (cid:48)(s) and the roots in the right complex semiplane
as the roots of P (cid:48)(−s) respectively.11
Having found the roots p(cid:48)
n of P (cid:48)(s) we still need to ﬁnd the leading coeﬃcient
g(cid:48) of P (cid:48)(s):
P (cid:48)(s) = g(cid:48) ·
(cid:89)
(s − p(cid:48)
n)
In order to do so, notice that (8.11) implies |P (cid:48)(0)| = |P (0)|. Since P (0) = 1
and |P (0)| = 1 we should have |P (cid:48)(0)| = 1. Actually, if we let g(cid:48) = 1 we will
obtain P (cid:48)(0) = 1. Indeed,
n
N
(cid:89)
P (cid:48)(0) =
(0 − p(cid:48)
n) =
n=1
N
(cid:89)
(−p(cid:48)
n)
n=1
That is P (cid:48)(0) is equal to the product of all roots of P (cid:48)(−s). Looking at Figs. 8.11
and 8.12 we notice that the product of all roots of P (cid:48)(−s) is equal to 1 and thus
P (cid:48)(0) = 1.12
Thus, by ﬁnding the roots and the leading coeﬃcient of P (cid:48)(s) we have ob-
tained a real polynomial P (cid:48)(s) = B [P (s)] in the multiplicative form. In practical
ﬁlter implementations the complex conjugate pairs of factors of P (cid:48)(s) will be
represented by 2nd-order ﬁlter sections, the purely real factor of P (cid:48)(s) appearing
for odd N will be represented by a 1st-order ﬁlter section:
P (cid:48)(s) = (s + 1)N ∧1 ·
(cid:89)
(s2 + 2Rns + 1)
n
11If P (s) is the numerator of a transfer function, then the roots all being in the left semiplane
imply the minimum phase implementation. However in this case we could instead pick up
the right semiplane roots as the roots of P (s), thereby obtaining a maximum phase transfer
function. Or one could take the minimum phase implementation and exchange one conjugate
pair of roots of P (s) against the matching conjugate pair of roots of P (−s). Or one could
exchange several of such pairs. Or one could exhange the real roots of P (s) and P (−s) if the
transformation order is odd. Still, the default choice will be to take the roots from the left
semiplane.
12Obviously g(cid:48) = −1 would also ensure |P (cid:48)(0)| = 1. However, the default choice will be
g(cid:48) = 1.
290
where
CHAPTER 8. RAISING THE FILTER ORDER
N ∧ 1 =
(cid:40)
1
0
if N is odd
if N is even
stands for bitwise conjunction.
Arbitrary 1st-order polynomials
Considering P (s) of a more generic form P (s) = s + a (a > 0) we notice that
essentially the procedure is the same as for P (s) = s + a except that instead of
the equation ω2N + 1 = 0 we obtain the equation
ω2N + a2 = 1
This means that the roots of Q(cid:48)(s) are no longer located on the unit circle but
on a circle of radius a1/N . It is not diﬃcult to see that the leading coeﬃcient
of P (cid:48)(s) is still equal to 1.
The above result also could have been obtained by rewriting P (s) as P (s) =
a · (s/a + 1) and applying properties (8.12b) and (8.12c), which on one hand
gives a more intuitive understanding of why the circle of roots is scaled by
a1/N , on the other hand can serve as an explicit proof of (8.12c) for the case of
Butterworth transformation of the 1st kind.
The case of a = 0 (P (s) = s) can be obtained as a limiting case13 a → +0
resulting in P (cid:48)(s) = sN .
If a < 0 then, noticing that the amplitude responses of P (s) = s + a and
P (s) = s − a are identical (for a ∈ R), we could obtain P (cid:48)(s) as Butterworth
transformation of P (s) = s − a. However, since the root of P (s) is in the right
semiplane, it would be logical to also pick the right semiplane roots of Q(cid:48)(s) as
the roots of P (cid:48)(s). Particularly, if P (s) is the numerator of a maximum phase
ﬁlter, the transformation result will retain the maximum phase property.
The 1st-order polynomials of the most general form P (s) = a1s + a0 can
be treated by rewriting them as P (s) = a1 · (s + a0/a1), if a1 (cid:54)= 0. The case
of a1 = 0 can be simply treated as a limiting case a1 → 0, where we drop the
vanishing higher-order terms of P (cid:48)(s), resulting in P (cid:48)(s) = a0.
Lowpass Butterworth filter of the 1st kind
Given
H(s) =
1
s + 1
(8.14)
we transform the denominator P (s) = s + 1 according to the previous discussion
of the Butterworth transformation of a 1st order polynomial. The roots of the
transformed polynomial (located on the unit circle) become the poles of H (cid:48)(s).
The numerator of H (cid:48)(s) is obviously unchanged by the transformation. Thus
we obtain
H (cid:48)(s) =
(cid:18) 1
(cid:19)N ∧1
s + 1
(cid:89)
·
n
1
s2 + 2Rns + 1
13Treating as a limiting case (here and later in the text) is important because it ensures the
continuity of the result at the limiting point.
8.6. BUTTERWORTH FILTERS OF THE 1ST KIND
291
where the 1/(s + 1) term occurs in case of an odd N (“N ∧ 1” standing for
bitwise conjunction). Therefore H (cid:48)(s) can be implemented as a series of 1-pole
and 2-pole lowpass ﬁlters, where the 1-pole appears in case of an odd N .
Fig. 8.13 compares the amplitude response of a Butterworth lowpass ﬁlter
of the 1st kind (N = 2) against the prototype 1-pole lowpass ﬁlter. One can
observe the increased steepness of the cutoﬀ slope resulting from the shrinking
along the logarithmic frequency axis. Fig. 8.14 compares the same Butterworth
lowpass ﬁlter against cascading of identical 1st order lowpasses, that is compar-
ing the shrinking along the logarithmic frequency axis vs. stretching along the
logarithmic amplitude axis. One can see that the Butterworth lowpass ﬁlter has
the sharpest cutoﬀ corner among diﬀerent ﬁlters in Figs. 8.13 and 8.14.
|H(jω)|, dB
0
-6
-12
-18
1/8
1
8
ω
Figure 8.13: 2nd order lowpass Butterworth ﬁlter of the 1st kind
(solid line) vs. 1st order lowpass ﬁlter (dashed line).
|H(jω)|, dB
0
-6
-12
-18
1/8
1
8
ω
Figure 8.14: 2nd order lowpass Butterworth ﬁlter of the 1st kind
(solid line) vs. duplicated 1st order lowpass ﬁlter without and with
cutoﬀ adjustment (dashed lines).
It is useful to know and recognize the expression for the squared amplitude
292
CHAPTER 8. RAISING THE FILTER ORDER
response of a Butterworth lowpass ﬁlter of the 1st kind. Since the squared
amplitude response of (8.14) is
|H(jω)|2 =
1
1 + ω2
after the substitution ω ← ωN we obtain
|H (cid:48)(jω)|2 =
1
1 + ω2N
(8.15)
This expression is used in traditional derivation of Butterworth ﬁlters. Essen-
tially the N -th order lowpass Butterworth ﬁlter is traditionally deﬁned as a
ﬁlter whose the amplitude response satisﬁes (8.15). Note that by (8.15) the
1-pole lowpass is the Butterworth ﬁlter of order 1. We can formally treat it as
a 1st-order Butterworth transformation of itself
1
1 + s
= B1
(cid:21)
(cid:20) 1
1 + s
It is also useful to explicitly know the transfer function of the Butterworth
lowpass ﬁlter of the 1st kind of order N = 2. It’s not diﬃcult to realize that
for P (s) = s + 1 the roots of P (cid:48)(s) are located 45◦ away from the negative real
semiaxis. Thus the respective damping is R = arccos 45◦ = 1/
2 and
√
H (cid:48)(s) =
1
√
2s + 1
s2 +
√
This damping value and the 2nd-order term s2 +
2s + 1 appears in all But-
terworth ﬁlters of the 1st kind of order N = 2 (highpass, bandpass, etc.) The
2 in the
readers may also recall the appearance of the damping value R = 1/
discussion of 2-pole ﬁlters, where it was mentioned that at R = 1/
2 the 2-
pole ﬁlter turns into a Butterworth ﬁlter. This also corresponds to the fact that
among all non-resonating (in the sense of the missing resonance peak) 2nd-order
ﬁlters the Butterworth ﬁlter is the one with the sharpest possible cutoﬀ corner
in the amplitude response.
√
√
Highpass Butterworth filter of the 1st kind
For
H(s) =
s
1 + s
we have the same denominator as for the respective lowpass. Thus the result of
the denominator transformation is the same as for the lowpass. The result of
the numerator transformation is sN and thus
H (cid:48)(s) =
(cid:18) s
(cid:19)N ∧1
s + 1
(cid:89)
·
n
s2
s2 + 2Rns + 1
That is we obtain the same result as for the 1-pole lowpass, except that instead
of a series of lowpasses we should take a series of highpasses. Fig. 8.15 illustrates
the respective amplitude response.
It is not diﬃcult to verify that the highpass Butterworth ﬁlter obtained in the
described above way is identical to the result of LP to HP substitution applied
to the lowpass Butterworth ﬁlter of the same order, which is in agreement with
(8.12d).
8.6. BUTTERWORTH FILTERS OF THE 1ST KIND
293
|H(jω)|, dB
0
-6
-12
-18
1/8
1
8
ω
Figure 8.15: 2nd order highpass Butterworth ﬁlter of the 1st kind
vs. 1st order highpass ﬁlter (dashed line).
Bandpass Butterworth filter of the 1st kind
For an even N , by formally putting a numerator sN/2 over the Butterworth
transformation of a polynomial P (s) = 1 + s we obtain a kind of a bandpass
ﬁlter:
H (cid:48)(s) =
(cid:89)
n
sN/2
s2 + 2Rns + 1
(Fig. 8.16), which can be also formally seen as a Butterworth transformation of
H(s) = s1/2/(s + 1).
|H(jω)|, dB
0
-6
-12
-18
1/8
1
8
ω
Figure 8.16: 2nd order bandpass Butterworth ﬁlter of the 1st kind.
Note that thereby this bandpass ﬁlter doesn’t have any parameters to con-
trol, except the cutoﬀ. As we will see a bit later in the discussion of Butter-
worth ﬁlters of the 2nd kind, this ﬁlter also can be obtained by an order N/2
Butterworth transformation of the 2-pole bandpass H(s) = s/(s2 +
2s + 1).
Therefore there is not much point in speciﬁcally using Butterworth bandpass
√
294
CHAPTER 8. RAISING THE FILTER ORDER
ﬁlters of the 1st kind, one can simply use Butterworth bandpass ﬁlters of the
2nd kind instead, achieving exactly the same response at a particular resonance
setting.
A bandpass ﬁlter which has controllable bandwidth can be obtained by ap-
plying the LP to BP substitution to a Butterworth lowpass ﬁlter of the 1st kind.
Apparently this produces a normalized bandpass (Fig. 8.17). This ﬁlter does
not coincide with the result of the Butterworth transformation of the normal-
2s + 1). The reason is that in the
ized 2-pole bandpass H(s) =
ﬁrst case we have a Butterworth transformation of a 1-pole lowpass 1/(1 + s)
followed by the LP to BP substitution, while in the second case we ﬁrst have
the LP to BP substitution (with an appropriately chosen bandwidth) applied
2s + 1), which is then followed by the
to 1/(1 + s) yielding H(s) =
Butterworth transformation. So it’s the opposite order of the application of LP
to BP substitution and the Butterworth transformation.
2s/(s2 +
2s/(s2 +
√
√
√
√
|H(jω)|, dB
0
-12
-24
-36
1/8
1
8
ω
Figure 8.17: A bandwidth-tuned LP to BP substitution of a low-
pass Butterworth ﬁlter of the 1st kind vs. Butterworth transfor-
mation of H(s) =
2s + 1) (dashed line).
2s/(s2 +
√
√
The LP to BP substitution can be performed algebraically on the transfer
function of the Butterworth lowpass. In order to simplify things, the substi-
tution can be applied in turn to the poles of each of the underlying 1- and
2-pole ﬁlters of the cascaded implementation of the Butterworth lowpass. After
organizing the transformed poles into mutually conjugate pairs, we can simply
construct the result as a series of normalized 2nd order bandpasses, deﬁned by
those pole pairs. Alternatively the LP to BP substitution can be implemented
using the integrator substitution technique (Fig. 4.19).
8.7 Butterworth filters of the 2nd kind
Now we are going to apply the Butterworth transformation to 2nd order poly-
nomials and respectively 2nd order ﬁlters. Such transformation will be referred
to as Butterworth transformation of the 2nd kind and the ﬁlters obtained as the
results of the tranformation will be referred to Butterworth filters of the 2nd
kind.
8.7. BUTTERWORTH FILTERS OF THE 2ND KIND
295
Transformation of polynomial P (s) = s2 + 2Rs + 1
We will ﬁrst consider the following 2nd order polynomial
P (s) = s2 + 2Rs + 1
corresponding to the denominator of a unit-cutoﬀ 2-pole ﬁlter.
It will be most illustrative to obtain the Butterworth transformation of the
2nd kind as a combination of two opposite perturbations of two Butterworth
transformations of the 1st kind. Factoring P (s) we obtain
P (s) = (s + a1)(s + a2) = P1(s)P2(s)
At R = 1 we have a1 = a2 = 1 and P (s) is a product of two 1st-order polynomi-
als P1(s) = P2(s) = s + 1. Applying the Butterworth transformation of the 1st
kind to each of the polynomials P (cid:48)
1(s) and P (cid:48)
2(s) we obtain two identical sets of
the roots of P (cid:48)
1(s) and P (cid:48)
2(s) respectively. We can also consider the respective
(also identical) extended polynomials
Q1(s) = P1(s)P1(−s)
Q2(s) = P2(s)P2(−s)
Q(s) = P (s)P (−s) = Q1(s)Q2(s)
1(s)P (cid:48)
1(s) = P (cid:48)
Q(cid:48)
1(−s)
Q(cid:48)
2(s)P (cid:48)
2(s) = P (cid:48)
2(−s)
Q(cid:48)(s) = P (cid:48)(s)P (−s) = Q(cid:48)
1(s)Q(cid:48)
2(s)
which additionally contain the right-semiplane roots. As we should remember
from the discussion of the Butterworth transformation of the 1st kind, the roots
in each of the two sets corresponding to Q(cid:48)
2(s) are equally spaced on
the unit circle.14
1(s) and Q(cid:48)
Now suppose we initially have R = 1 and then increase R to a value R > 1,
resulting in a1 growing and a2 decreasing, staying reciprocal to each other:
a1 = R +
(cid:112)
R2 − 1
a2 = R −
(cid:112)
R2 − 1
(a1a2 = 1)
(Fig. 8.18). Since a1 and a2 are the “cutoﬀs” of the 1st-order polynomials s + a1
and s + a2, from the properties of the Butterworth transformation of the 1st
kind we obtain that the radii of the circles, on which the roots of Q(cid:48)
1(s) and
Q(cid:48)
2(s) are located, become equal to
r(cid:48)
1 = (R +
(cid:112)
R2 − 1)1/N
r(cid:48)
2 = (R −
(cid:112)
R2 − 1)1/N
(r(cid:48)
1r(cid:48)
2 = 1)
Thus, one circle grows and the other circle shrinks, while their radii are staying
reciprocal to each other (Fig. 8.19).
Now let’s decrease R from 1 to a value 0 < R < 1. This makes a1 and a2
complex:
a1 = ejα
a2 = e−jα
(cos α = R,
a1a2 = 1)
14With Butterworth transformation of the 2nd kind we won’t be making a distinction be-
tween even and odd roots. Instead we will be paying attention to which roots originate from
Q1(s) and which from Q2(s).
296
CHAPTER 8. RAISING THE FILTER ORDER
Im s
j
×k
×k−1
−a1
−1
−a2
0
×k−1
a2
×k
1
a1
Re s
−j
k = a1 = 1/a2
Figure 8.18: Roots of Q(s) for R > 1 (black dots are roots of
Q1(s), white dots are roots of Q2(s)) and their positions at R = 1
(indicated by circled dots, where each such dot denotes a root of
Q1(s) coinciding with a root of Q2(s)).
Im s
j
×k
×k
×k−1
×k−1
−1
×k
×k−1
0
1
×k−1
Re s
×k
−j
k = a1/N
1 = a−1/N
2
1(s), white dots are roots of Q(cid:48)
Figure 8.19: Roots of Q(cid:48)(s) for R > 1 (black dots are roots of
Q(cid:48)
2(s)) and their positions at R = 1
(indicated by circled dots, where each such dot denotes a root of
Q(cid:48)
2(s)). Butterworth transforma-
tion order N = 2.
1(s) coinciding with a root of Q(cid:48)
Writing out the “amplitude response” we have
|P (jω)|2 = P (jω)P (−jω) = P1(jω)P2(jω) · P1(−jω)P2(−jω) =
8.7. BUTTERWORTH FILTERS OF THE 2ND KIND
297
= P1(jω)P1(−jω) · P2(jω)P2(−jω) =
= Q1(jω)Q2(jω) = (ω2 + a2
1) · (ω2 + a2
2)
Respectively, our goal is to have
|P (cid:48)(jω)|2 = Q(cid:48)
1(jω)Q(cid:48)
2(jω) = Q1(jωN )Q2(jωN ) = (ω2N + a2
1) · (ω2N + a2
2)
1(s) and Q(cid:48)
So how do we ﬁnd the roots of Q(cid:48)
2(s)? If a1 = 1 (α = 0, R = 1)
then, as we just discussed, Q(cid:48)
1(s) simply generates a set of the Butterworth
roots of the 1st kind on the unit circle. Now if we replace α = 0 with α > 0
(corresponding to replacing R = 1 with R < 1) this means a rotation of a1 by
the angle α (Fig. 8.20). This rotates all roots of Q(cid:48)
1 by α/N
(Fig. 8.21). At the same time a2 will be rotated by −α and respectively all
roots of Q(cid:48)
1(jω) = ω2N + a2
2(jω) = ω2N + a2
2 by −α/N .
Im s
j
−a2
α
α
−1
0
−a1
−j
a1
1
a2
α
α
Re s
α = arccos R
Figure 8.20: Roots of Q(s) for 0 < R < 1 (black dots are roots of
Q1(s), white dots are roots of Q2(s)) and their positions at R = 1
(indicated by circled dots, where each such dot denotes a root of
Q1(s) coinciding with a root of Q2(s)).
Even though generally for α > 0 the set of roots of ω2N +a2
1 is not symmetric
relatively to the imaginary axis and neither is the set of roots of ω2N + a2
2, the
combination of the two sets is symmetric (as one can observe from Fig. 8.21).
Thus we can simply drop the roots in the right semiplane, the same way as we
did for R ≥ 1. Note that this also means that we do not need to rotate the
full set of roots of Q(cid:48)
2(s). Since at the end we are interested just in
the left-semiplane roots, it suﬃces to rotate only the left-semiplane halves of
2(s) (that is, the roots of P (cid:48)
the roots of Q(cid:48)
2(s)), as long as
the roots do not cross the imaginary axis. It is not diﬃcult to realize that the
said crossing of the imaginary axis happens at α = π/2 corresponding to R = 0,
where one of the roots on the imaginary axis will be from P (cid:48)
1(s) and the other
from P (cid:48)
1(s) and Q(cid:48)
1(s) and Q(cid:48)
1(s) and P (cid:48)
2(s).
298
CHAPTER 8. RAISING THE FILTER ORDER
Im s
j
α/N
α/N
α/N
α/N
−1
α/N
α/N
−j
0
1
Re s
α/N
α/N
α = arccos R
1(s), white dots are roots of Q(cid:48)
Figure 8.21: Roots of Q(cid:48)(s) for 0 < R < 1 (black dots are roots
of Q(cid:48)
2(s)) and their positions at
R = 1 (indicated by circled dots, where each such dot denotes
a root of Q(cid:48)
2(s)). Butterworth
transformation order N = 2.
1(s) coinciding with a root of Q(cid:48)
So, let’s reiterate. At R = 1 (α = 0) the roots of P (cid:48)(s) consist of two
identical sets, each set being just the (left-semiplane) roots of a Butterworth
transformation of a 1st-order polynomial s+1, all roots in such set being located
on the unit cicle. For R > 1 we need to change the radii of both sets in a
reciprocal manner:
r(cid:48) = (R +
R2 − 1)±1/N
(cid:112)
(Fig. 8.19). For R < 1 we need to rotate both sets by opposite angles
∆α(cid:48) = ±α/N
α = arccos R
(Fig. 8.21).
√
2).
We have mentioned that at R = 0 (α = π/2) two of the rotated roots of P (cid:48)(s)
reach the imaginary axis. Another special case occurs when the roots of P (s)
are halfway from the “neutral position” (α = 0) to selfoscillation (α = π/2),
In this case the four roots of Q(s) are
that is when α = π/4 (R = 1/
In the process
equally spaced on the unit circle with the angular step π/2.
of the Butterworth transformation we rotate the roots of Q(cid:48)
2(s) by
±α/N = ±π/4N , resulting in the set of roots of Q(cid:48)(s) being equally spaced
on the unit circle by the angular step π/2N . But this is the set of roots of
the Butterworth transformation of the 1st kind of order 2N (which produces
the same polynomial order 2N as the order N Butterworth transformation of
2
the 2nd kind). This result becomes obvious if we notice that at R = 1/
and α = π/4 the polynomial s2 + 2Rs + 1 is the result of the Butterworth
transformation of the 1st kind of order 2 of the polynomial s+1. It is therefore no
wonder that a Butterworth transformation of order 2 followed by a Butterworth
1(s) and Q(cid:48)
√
8.7. BUTTERWORTH FILTERS OF THE 2ND KIND
299
transformation of order N is equivalent to the Butterworth transformation of
order 2N (in other words, shrinking along the frequency axis by the factor 2N
is equivalent to shrinking ﬁrst by the factor of 2 and then by the factor of N ).
Seamless transition at R = 1
In the derivation of the Butterworth transformation of the 2nd kind we have
been treating the cases R > 1 and R < 1 separately.
In practice however
we would like to be able to smoothly change R from R > 1 to R < 1 and
back in a seamless way (without clicks or other artifacts arising from an abrupt
reconﬁguration of a ﬁlter chain). This means that we need to ﬁnd a way to
distribute the roots of P (cid:48)(s) among 2nd-order factors in a continuous way, where
there are no jumps in the values of the coeﬃcients of these factors if R is varied
in a continuous way. Formally saying, the coeﬃcients of the 2nd-order factors
must be continuous functions of R everywhere. The continuity for R (cid:54)= 1 should
occur for granted, thus we are speciﬁcally concerned about continuity at R = 1.
First, let’s assume the order of the transformation is even.
Let R ≥ 1. There is an even count of the roots of P (cid:48)
1(s) and these roots come
in complex-conjugate pairs (Fig. 8.19). Therefore each conjugate pair of roots
of P (cid:48)
1(s) can be grouped into a single 2nd-order factor. The same can be done
2(s) and this half of our second-order factors corresponds to P (cid:48)
for P (cid:48)
1(s) and the
other half to P (cid:48)
2(s).
At R = 1 both sets of 2nd-order factors become identical, since P (cid:48)
1(s) be-
comes identical to P (cid:48)
2(s).
At R < 1 the roots of P (cid:48)
1(s) are rotated counterclockwise and the roots
of P (cid:48)
2(s) are rotated clockwise (Fig. 8.21), therefore the roots of each of the
polynomials won’t combine into conjugate pairs and thus the polynomials won’t
be real anymore (Fig. 8.22).
Im s
j
−1
0
1
Re s
−j
Figure 8.22: Movement of roots of P (cid:48)
(white dots) as R smoothly varies around R = 1.
1(s) (black dots) and P (cid:48)
2(s)
300
CHAPTER 8. RAISING THE FILTER ORDER
However, since we started the rotation from two identical sets of roots with
conjugate pairwise symmetry within each set, for each root of P (cid:48)
1(s) there is now
a conjugate root in P (cid:48)
2(s) and vice versa. We can therefore formally redistribute
the roots between P (cid:48)
2(s) in such a way, that the roots P (cid:48)
1(s) and P (cid:48)
1(s) will be
rotated by α/N towards the negative real semiaxis (compared to R = 1) and
the roots P (cid:48)
2(s) will be rotated by α/N away from the negative real semiaxis
(Fig. 8.23).
Im s
j
−1
0
1
Re s
−j
Figure 8.23: Movement of redistributed roots of P (cid:48)
and P (cid:48)
2(s) (white dots) as R smoothly varies around R = 1.
1(s) (black dots)
Thus, at R = 1 we have two identical sets of roots. At R > 1 the roots of
1(s) move outwards from the unit circle, at R < 1 the roots of P (cid:48)
P (cid:48)
1(s) move
towards the negative real semiaxis. The roots of P (cid:48)
2(s) move inwards from the
unit circle (R > 1) and away from the negative real semiaxis (R < 1). This way
we can keep the same assignment of the roots to the 2nd-order factors.15
If the order of the transformation is odd, then besides the conjugate pairs
that we just discussed, we get two “special” roots, corresponding to the purely
real root of the Butterworth transformation of the 1st kind of s + 1 (Fig. 8.24).
These two roots are real for R ≥ 1 and complex conjugate for R < 1, where
at R = 1 both roots are at −1. Thus, they can simply be assigned to one and
the same 2nd-order factor of the form s2 + 2R(cid:48)s + 1 (which cannot be formally
assigned to P (cid:48)
1(s)
and P (cid:48)
2(s), but can be thought of as being “shared” among P (cid:48)
2(s)), where R(cid:48) depends on R.
1(s) or P (cid:48)
Arbitrary 2nd-order polynomials
The non-unit-cutoﬀ polynomials P (s) = s2 + 2Ras + a2 can be simply treated
using (8.12c).
15Of course we could have done the opposite redistribution of roots among P (cid:48)
2(s),
1(s) move outwards from the unit circle and away from the negative real
1(s) move inwards from the unit circle and towards the negative
1(s) and P (cid:48)
where the roots of P (cid:48)
semiaxis, while the roots of P (cid:48)
real semiaxis.
8.7. BUTTERWORTH FILTERS OF THE 2ND KIND
301
Im s
j
−1
1
0
Re s
−j
Figure 8.24: Movement of the “special” root of P (cid:48)
and the “special” root of P (cid:48)
around R = 1.
1(s) (black dot)
2(s) (white dot) as R smoothly varies
The case a = 0 can be taken in the limiting sense a → 0 giving P (cid:48)(s) = s2N .
The case R = 0 also can be taken in the limiting sense R → +0.
The case R < 0 can be treated by noticing that the amplitude responses
of P (s) = s2 + 2Ras + a2 and P (s) = s2 − 2Ras + a2 are identical. Thus, we
can apply the Butterworth transformation to the positive-damping polynomial
P (s) = s2 − 2Ras + a2. Since the roots of P (s) in case of R < 0 lie in the
right complex semiplane, we might as well pick the right semiplane roots for
P (cid:48)(s). Particularly, if P (s) is the numerator of a maximum phase ﬁlter, the
transformation result will retain the maximum phase property.
The polynomial of the most general form P (s) = a2s2 + a1s + a0 can be
treated by rewriting it as P (s) = a2 · (s2 + (a1/a2)s + a0/a2) where usually
a2 (cid:54)= 0, a0/a2 > 0. If a0/a2 < 0 then P (s) has two real roots of opposite sign
and can be handled as a product of two 1st-order polynomials, to which we can
apply Butterworth transformation of the 1st kind. If a2 = 0, we can treat this
as a limiting case a2 → 0. Noticing that at a2 → 0 the damping a1/2a2 → ∞
we rewrite P (s) as a product of real 1st-order terms:
(cid:32)
P (s) = a2 ·
s +
a1 + (cid:112)a2
1 − 4a2a0
2a2
(cid:33)
(cid:32)
·
s +
a1 − (cid:112)a2
1 − 4a2a0
2a2
(cid:33)
∼
∼ (a2s + a1) · (s + a0/a1)
(for a2 → 0)
and as a2 vanishes we discard the inﬁnitely large root of the polynomial a2s + a1
(and the associated roots of P (cid:48)(s)), formally replacing the polynomial a2s + a1
with the constant factor a1.
302
CHAPTER 8. RAISING THE FILTER ORDER
Lowpass Butterworth filter of the 2nd kind
Given
H(s) =
1
1 + 2Rs + s2
and transforming its denominator according to the previous discussion of the
Butterworth transformation of a 2nd order polynomial we obtain
H (cid:48)(s) =
(cid:89)
n
1
s2 + 2Rns + 1
Therefore H (cid:48)(s) can be implemented as a series of 2-pole lowpass ﬁlters.
Fig. 8.25 compares the amplitude response of a Butterworth lowpass ﬁlter
of the 2nd kind (N = 2) against the prototype resonating 2-pole lowpass ﬁlter.
Note the increased steepness of the cutoﬀ slope and the fact that the resonance
peak height is preserved by the transformation.
|H(jω)|, dB
+12
+6
0
-6
-12
-18
1/8
1
8
ω
Figure 8.25: 4th order lowpass Butterworth ﬁlter of the 2nd kind
vs. 2nd order lowpass ﬁlter (dashed line).
Fig. 8.26 compares the same Butterworth lowpass ﬁlter against cascading
of identical 2nd order lowpasses, where the resonance has been adjusted to
maintain the same resonance peak height. Note the much larger width of the
resonance peak of the latter.
√
As mentioned before in the discussion of the Butterworth transformation
of the 2nd kind, at R = 1/
2 we get the same set of poles as for order N
Butterworth transformation of the 1st kind. Thus, at this resonance setting our
lowpass Butterworth ﬁlter of the 2nd kind (the ﬁlter order of which is 2N ) is
equal to the lowpass Butterworth ﬁlter of the 1st kind of the same ﬁlter order
2N .
8.7. BUTTERWORTH FILTERS OF THE 2ND KIND
303
|H(jω)|, dB
+12
+6
0
-6
-12
-18
1/8
1
8
ω
Figure 8.26: 4th order lowpass Butterworth ﬁlter of the 2nd kind
vs. duplicated 2nd order lowpass ﬁlter of the same resonance peak
height (dashed line).
Highpass Butterworth filter of the 2nd kind
The highpass Butterworth ﬁlter of the 2nd kind is obtained from
resulting in
H(s) =
s2
1 + 2Rs + s2
H (cid:48)(s) =
s2
s2 + 2Rns + 1
(cid:89)
n
Therefore H (cid:48)(s) can be implemented as a series of 2-pole highpass ﬁlters. Fig. 8.27
shows the respective amplitude response.
As with lowpass Butterworth ﬁlter of the 1st kind, the highpass Butterworth
ﬁlter of the 2nd kind can be equvalently obtained by applying the LP to HP
substitution to a lowpass Butterworth ﬁlter of the 2nd kind.
As with lowpass Butterworth ﬁlter of the 2nd kind, at R = 1/
2 we get a
√
highpass Butterworth ﬁlter of the 1st kind.
Bandpass Butterworth filter of the 2nd kind
The bandpass Butterworth ﬁlter of the 2nd kind is obtained from
resulting in
H(s) =
s
1 + 2Rs + s2
H (cid:48)(s) =
(cid:89)
n
s
s2 + 2Rns + 1
304
CHAPTER 8. RAISING THE FILTER ORDER
|H(jω)|, dB
+12
+6
0
-6
-12
-18
18
1
8
ω
Figure 8.27: 4th order highpass Butterworth ﬁlter of the 2nd kind
vs. 2nd order highpass ﬁlter (dashed line).
Therefore H (cid:48)(s) can be implemented as a series of 2-pole bandpass ﬁlters. Dif-
ferently from the bandpass Butterworth ﬁlter of the 1st kind, this one allows
to control the amount of resonance. Fig. 8.28 shows the respective amplitude
response.
|H(jω)|, dB
+12
+6
0
-6
-12
-18
ωc/8
ωc
8ωc
ω
Figure 8.28: 4th order bandpass Butterworth ﬁlter of the 2nd kind
vs. 2nd order bandpass ﬁlter (dashed line).
8.8. GENERALIZED LADDER FILTERS
305
√
As with lowpass and highpass, at R = 1/
2 we get a bandpass Butterworth
ﬁlter of the 1st kind. Since bandpass Butterworth ﬁlter of the 1st kind occurs
only for an even transformation order 2N , any bandpass Butterworth ﬁlter of
the 1st kind is simply a bandpass Butterworth ﬁlter of the 2nd kind (of the
same ﬁlter order 2N ) at a particular resonance setting.
By replacing the underlying 2-pole bandpasses with their normalized versions
one can obtain the normalized bandpass Butterworth ﬁlter of the 2nd kind:
H (cid:48)(s) =
(cid:89)
n
2Rns
s2 + 2Rns + 1
One can of course also apply the LP to BP substitution to a Butterworth
lowpass of the 2nd kind. Note, however, that if the lowpass has a resonance
peak, then the resulting bandpass will have two of those, so this would be a
rather special kind of a bandpass.
8.8 Generalized ladder filters
Now that we have learned to construct generic higher-order ﬁlters with resonance
(by means of Butterworth transformation) we might consider going into the
selfoscillation range by letting the 2nd kind Butterworth poles in Fig. 8.21 to
rotate past the imaginary axis (that is by letting α > π/2). Clearly we would
need a nonlinear implementation structure then, to prevent the selfoscillating
ﬁlter from explosion.
So far we have discussed 3 diﬀerent implementations of generic high-order
ﬁlters: generalized SVF and serial and parallel cascades. The SVF structure
doesn’t accomodate nonlinearities easily, even though there can be ways. With
serial and parallel cascades we of course could use various nonlinear 2-poles (and
possibly nonlinear 1-poles) in the implementation, however this doesn’t feel very
natural, compared to the nonlinearities appearing in the ladder ﬁlters. As we
should remember, in ladder ﬁlters the resonance is created by the feedback, the
more feedback, the more resonance, so that a saturator in the feedback loop was
an eﬃcient way to build a nonlinear ladder ﬁlter. In such ﬁlter the nonlinearity
is a part of a feedback loop going through the entire ﬁlter, whereas in cascade
implementations diﬀerent nonlinearities would be independent. Of course, both
approaches work in a way, but the approach where we have independent nonlin-
earities feels somewhat more artiﬁcial than the one with a “global” nonlinearity
aﬀecting the entire ﬁlter. For that reason we will make another attempt at
generalizing the 4-pole ladder ﬁlter to abitrary pole counts.
We have seen that one can apply the general idea of a ladder ﬁlter to pole
counts other than 4 by simply increasing the number of underlying 1-poles, but
it hardly looks as a smooth generalization, as, except for bandpass16 ladders,
the resonance behavior of the resulting ﬁlters is obtaining “special features”,
like e.g. an oﬀset of the resonant peak position for the 8-pole lowpass. We will
start now with a diﬀerent approach, namely with the generalized SVF (Fig. 8.1)
where we replace all integrators with 1-pole lowpass ﬁlters (which, as it’s not
diﬃult to realize, corresponds to the substitution s ← s+1). Fig. 8.29 illustrates
(compare to Fig. 8.1).
16And allpass, as covered in Chapter 11.
306
CHAPTER 8. RAISING THE FILTER ORDER
y(t)
+
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
b0
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
x(t)
+
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
b1
+
. . .
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
b2
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
bN
LP1
•(cid:47)
LP1
•(cid:47)
. . .
LP1
•(cid:47)
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
a1
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
a2
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
aN
+
+
. . .
Figure 8.29: Generalized ladder (generalized TSK) ﬁlter.
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Comparing to Fig. 5.9 we notice that the diﬀerence is that we are taking
the feedback signal as a linear combination of all modal outputs, rather than
simply from the last modal output. Comparing to Fig. 5.26 we notice that the
latter essentially implements the same idea: taking a mixture of modal outputs
as feedback signal, where both structures become equivalent at a1 = −k and
a2 = k. Thus, one could see Fig. 8.29 also as a generalization of the TSK ﬁlter.
The transfer function of Fig. 8.29 can be obtained from the transfer function
of Fig. 8.1 by s ← s + 1 substitution:
H(s) =
N
(cid:88)
n=0
bN −n(s + 1)n
(s + 1)N +
N −1
(cid:88)
n=0
aN −n(s + 1)n
Given a prescribed transfer function in the usual form
H(s) =
N
(cid:88)
n=0
βnsn
sN +
N −1
(cid:88)
n=0
αnsn
we could obtain the an and bn coeﬃcients from αn and βn by equating both
transfer function forms
N
(cid:88)
n=0
bN −n(s + 1)n
(s + 1)N +
N −1
(cid:88)
n=0
=
aN −n(s + 1)n
sN +
N
(cid:88)
n=0
βnsn
N −1
(cid:88)
n=0
αnsn
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
8.8. GENERALIZED LADDER FILTERS
307
and then performing the s + 1 ← s (or equivalently s ← s − 1) subtitution on
the entire equation:
N
(cid:88)
n=0
bN −nsn
=
aN −nsn
sN +
sN +
N −1
(cid:88)
n=0
N
(cid:88)
n=0
βn(s − 1)n
N −1
(cid:88)
n=0
αn(s − 1)n
Now we simply expand all (s − 1)n in the right-hand side and equate the coef-
ﬁcients at the same powers of s to obtain the expressions for an and bn via αn
and βn.
Nonlinearities
Letting all an = 0 in Fig. 8.29 we obtain an N -th order lowpass of the form
1/(s + 1)N . This consideration shows that the resonance in Fig. 8.29, if any,
would be created through non-zero an coeﬃcients and one could prevent the
ﬁlter from exploding selfoscillation by inserting a saturator into the feedback
path, which would eﬀectively reduce the values of an, bringing them almost to
zero at excessive signal levels. Fig. 8.30 illustrates.
y(t)
+
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
b0
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
x(t)
+
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
b1
+
. . .
(cid:49)(cid:49)(cid:49)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)
b2
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
bN
LP1
•(cid:47)
LP1
•(cid:47)
. . .
LP1
•(cid:47)
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
a1
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
a2
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
aN
tanh
+
+
. . .
Figure 8.30: Nonlinear generalized ladder.
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
The feedback modal mixing coeﬃcients an serve the role of the feedback gain
coeﬃcient k from Fig. 5.9. That is we don’t have anymore a single gain coeﬃ-
cient to control the feedback amount. Therefore we can’t place the saturator in
Fig. 8.30 before the gains. If a reverse placement of the saturator relatively to
the gains is desired, we can use the transposed version of Fig. 8.30.
In order to compare Fig. 8.31 to Fig. 5.9, or, even better, this time to Fig. 5.1
we should assume in Fig. 8.31 all bn = 0 except bN = 1, which corresponds to
modal gains for the pure N -th order lowpass mode. Then we can see that
Fig. 8.31 diﬀers from Fig. 5.1 by the fact that the feedback is going not only
It is also
to the input of the ﬁrst lowpass stage, but to all lowpass stages.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
308
CHAPTER 8. RAISING THE FILTER ORDER
. . .
•(cid:111)
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
bN
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)(cid:49)(cid:49)(cid:49)(cid:79)
aN
LP1
. . .
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
b2
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)(cid:49)(cid:49)(cid:49)(cid:79)
a2
LP1
•(cid:111)
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
b1
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:13)(cid:13)(cid:13)(cid:49)(cid:49)(cid:49)(cid:79)
a1
LP1
•(cid:111)
x(t)
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
b0
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
y(t)
. . .
•(cid:111)
•(cid:111)
tanh
Figure 8.31: Transposed nonlinear generalized ladder.
instructive to compare Fig. 8.31 to Fig. 5.31, where we should be able to see
that Fig. 8.31 is a generalized version of Fig. 5.31 with a1 = −k and a2 = k.
Non-lowpass generalizations
In principle we could perform other substitutions in the generalized SVF struc-
ture. E.g. we could replace integrators with highpass ﬁlters, which corresponds
to the substitution 1/s ← s/(1 + s) or, equivalently s ← 1 + 1/s. This would
correspond to generalized highpass ladder ﬁlters. More complicated substitu-
tions could also be done, e.g. replacing some integrators with highpasses and
some with lowpasses. One also doesn’t have to be limited by using 1-poles as
substitutions. Having outlined the basic idea, we won’t go into further detail.
SUMMARY
We have introduced four general topology classes: the generalized SVF, the
serial cascade form, the parallel form and the generalized ladder ﬁlter. These
topologies can be used to implement almost any transfer function (with the
most prominent restriction being that the parallel form can’t deal with repeated
poles).
We also introduced two essentially diﬀerent ways to obtain a higher-order
ﬁlter from a given ﬁlter of a lower order:
identical ﬁlter cascading and But-
terworth transformation. The former is stretching the amplitude and phase
responses vertically (which may cause a number of unwanted eﬀects), while the
latter is shrinking the amplitude response horizontally.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:111)
(cid:111)
(cid:111)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
(cid:111)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
Chapter 9
Classical signal processing
filters
In Chapter 8 we have introduced, among other ideas, Butterworth ﬁlters of
the 1st kind.
In this chapter we are going to construct further similar ﬁlter
types by allowing the amplitude response to have ripples of equal amplitude
(a.k.a. equiripples) in the pass- or stop-band, or in both. These ﬁlters as well
as Butterworth ﬁlters of the 1st kind are the ﬁlter types used in classical signal
processing. They have somewhat less prominent role in music DSP, therefore
we ﬁrst concentrated on other ﬁlter types. Still, they are occasionally useful.
9.1 Riemann sphere
Before we begin discussing equiripple ﬁlters we need to go into some detail of
complex algebra, concerning the Riemann sphere and some derived concepts.
The key feature of the Riemann sphere is that inﬁnity is treated like any other
point, and this (among with some other possibilities arising out of using the
Riemann sphere) will be quite helpful in our discussions. It seems there are a
number of slightly diﬀerent conventions regarding the Riemann sphere. We are
going to introduce now one particular convention which will be most useful for
our purposes.1
Given a complex plane w = u+jv we introduce the third dimension, thereby
embedding the plane into the 3-dimensional space (x, y, z). The x and y axes
coincide with u and v axes, the z axis is directed upwards. The Riemann sphere
will be the sphere of a unit radius x2 + y2 + z2 = 1 (Fig. 9.1). Thus, the
intersection of the Riemann sphere with the complex plane is at the “equator”
which in terms of w is simply the complex unit circle |w| = 1. The center of
projection will be at the “north pole” (0, 0, 1) of the Riemann sphere, which
thereby is the image of w = ∞. Respectively, the complex unit circle |w| = 1
coincides with its own projection image on the Riemann sphere.
We will denote and refer to the points on the Riemann sphere by the complex
values that they represent. E.g. the “north pole” will be simply denoted as ∞,
1The general discussion of the idea of the Riemann sphere is not a part of this book.
Readers unfamiliar with this concept are advised to consult the respective literature.
309
310
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
z
∞
1
−j
y
w=u+jv
x
Figure 9.1: Riemann sphere.
the “south pole” as 0, the point on the “zero meridian” as 1, the point on the
90◦ meridian as j etc. Some of these points are shown on Fig. 9.1.
Real Riemann circle
The 2-dimensional subspace (x, z) of the (x, y, z) space in Fig. 9.1 contains just
the real axis u of the complex plane and the real axis’s image on the Riemann
sphere which is a circle of unit radius x2 + z2 = 1 (Fig. 9.2). It will be intuitive
to refer to this circle as the real Riemann circle.
z
∞
ϕ
2
ϕ
0
u
1
x
(x, z)
−1
Figure 9.2: Real Riemann circle. The 0, 1, ∞, −1 labels denote
the points on the circle which correspond to these values.
9.1. RIEMANN SPHERE
311
We can use the polar angle ϕ (deﬁned as shown in Fig. 9.2) as the coordinate
on the Riemann circle. One of the reasons for this choice of deﬁnition of ϕ are
the following convenient mappings between u and ϕ:
u = 0 ⇐⇒ ϕ = 2πn
u = 1 ⇐⇒ ϕ =
π
2
u = −1 ⇐⇒ ϕ = −
+ 2πn
π
2
+ 2πn
u = ∞ ⇐⇒ ϕ = π + 2πn
Also, if we restrict ϕ to (−π, π), then
u = 0 ⇐⇒ ϕ = 0
u > 0 ⇐⇒ ϕ > 0
u < 0 ⇐⇒ ϕ < 0
From Fig. 9.2, using some basic geometry it’s not diﬃcult to ﬁnd that u and ϕ
are related as
u = tan
ϕ
2
and that
u =
x
1 − z
(9.1)
(9.2a)
By introducing the “homogeneous” coordinate ¯z = 1 − z the equation (9.2a)
can be rewritten in a more intuitive form:
Conversely, using Fig. 9.2 and equation (9.1) we have
u = x/¯z
x = sin ϕ =
2u
u2 + 1
u2 − 1
u2 + 1
z = − cos ϕ =
¯z =
2
u2 + 1
(9.2b)
(9.3a)
(9.3b)
(9.3c)
Symmetries on the real Riemann circle
Certain symmetries between a pair of points on the real axis correspond to
symmetries on the real Riemann circle. Speciﬁcally, from (9.1) we obtain:
u1 + u2 = 0 ⇐⇒ ϕ1 + ϕ2 = 2πn
u1u2 = 1 ⇐⇒ ϕ1 + ϕ2 = π + 2πn
u1u2 = −1 ⇐⇒ ϕ1 − ϕ2 = π + 2πn
or, by restricting ϕ1 and ϕ2 to [−π, π]
u1 + u2 = 0 ⇐⇒ ϕ1 + ϕ2 = 0
u1u2 = 1 ⇐⇒
π
2
u1u2 = −1 ⇐⇒ ϕ1 − ϕ2 = ±π
ϕ1 + ϕ2
2
= ±
Fig. 9.3 illustrates.
(9.4a)
(9.4b)
(9.4c)
(9.5a)
(9.5b)
(9.5c)
312
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
−1/u
∞
1/u
−1
1
−u
u
0
Figure 9.3: Symmetries of the values on the real Riemann circle.
Coordinate relationships for the Riemann sphere
The equations (9.2) and (9.3) generalize to the 3-dimensional space (x, y, z)
containing the complex plane w = u+jv and the Riemann sphere x2+y2+z2 = 1
in an obvious way as:
and
u =
v =
x
1 − z
y
1 − z
= x/¯z
= y/¯z
w = u + jv =
x + jy
¯z
x =
y =
z =
¯z =
2u
|w|2 + 1
2v
|w|2 + 1
|w|2 − 1
|w|2 + 1
2
|w|2 + 1
(9.6a)
(9.6b)
(9.6c)
(9.7a)
(9.7b)
(9.7c)
(9.7d)
(9.7e)
The equation (9.1) can be generalized if we restrict ϕ to [0, π], in which case
|w| = tan
ϕ
2
(9.8)
In principle we could also introduce the spherical azimuth angle, which is simply
equal to arg w, but we won’t do it in this book.
Imaginary Riemann circle
The (y, z) subspace of the (x, y, z) space in Fig. 9.1 contains just the imaginary
axis v of the complex plane and the imaginary axis’s image on the Riemann
sphere which is a circle of unit radius x2 + y2 = 1. We will refer to this circle
as the imaginary Riemann circle.
9.2. ARCTANGENT SCALE
313
There are no essential diﬀerences to the real Riemann circle. The same
illustrations and formulas hold, except that we should use v in place of u. Fig. 9.4
provides a simple illustration of the circla and its symmetries. Note that the
reciprocal symmetries change sign if expressed in terms of the complex variable
w, since 1/jv = −j/v.
1/w
∞
−1/w
−j
j
−w
w
0
Figure 9.4: Symmetries of the values on the imaginary Riemann
circle, where w = jv, v = Im w.
9.2 Arctangent scale
From the Riemann circle one can derive a special scale which will be useful
for plotting function graphs with interesting behavior around inﬁnity. One
commonly known special scale is a logarithmic scale x(cid:48) = log x which maps
the logical values x to the geometric positions x(cid:48) on the plot.
In a similar
fashion, we introduce the arctangent scale
x(cid:48) = 2 arctan x
(9.9)
which is using the polar angle ϕ of the real Riemann circle as the geometric
position x(cid:48). It is easy to notice that (9.9) is equivalent to (9.1), where we have
x in place of u and x(cid:48) in place of ϕ.
The actangent scale warps the entire real axis (−∞, +∞) into the range
(−π, π). Due to the periodic nature of the Riemann circle’s polar angle it is not
unreasonable to require the scale x(cid:48) to be periodic as well, in which case we can
also support the value x = ∞ which will map to π + 2πn.
Treating the inﬁnity like any other point, the arctangent scale provides a
convenient means for plotting the functions where the range of the values of
interest includes inﬁnity. E.g. we could plot the graph of the cosecant function
y = csc x = 1/ sin x using the arctangent scale for the function’s value axis, as
illustrated by Fig. 9.5
Symmetries in the arctangent scale
The symmetries of a graph plotted in the arctangent scale are occurring in agree-
ment with Riemann circle symmetries (9.4) and (9.5) (illustrated in Fig. 9.3).
Speciﬁcally:
1. Mutually opposite values are symmetric with respect to points 0 and ∞.
314
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
∞
y
−1
1
0
−1
− 3π
2
−π
− π
2
π
2
π
x
3π
2
Figure 9.5: Graphs of y = sin x (dashed) and y = csc x = 1/ sin x
(solid) using arctangent scale for the ordinate.
2. Mutually reciprocal values are symmetric with respect to points 1 and −1.
3. Values whose product is −1 map are spaced by the distance equal to the
half of the arctangent scale’s period, that is e.g. to the distance between
−1 and 1 or between 0 and ∞.
One can observe all of these properties if Fig. 9.5, where the third property
can be observed between the csc x and the half-period-shifted sin x.
9.3 Rotations of Riemann sphere
We are going to introduce two special transformations of the complex plane:
ρ+1(w) =
ρ−1(w) =
1 + w
1 − w
w − 1
w + 1
(9.10a)
(9.10b)
where w ∈ C ∪ ∞. It is easy to check by direct substitution that ρ−1(ρ+1(w)) =
ρ+1(ρ−1(w)) = w, that is the transformations ρ1 and ρ−1 are each other’s
inverses.2 As we shall see, ρ±1 are simply rotations of the Riemann sphere by
90◦ in two opposite directions.
Letting w = u + jv where u and v are the real and imaginary parts of w, we
have
w(cid:48) = u(cid:48) + jv(cid:48) = ρ+1(w) =
1 + w
1 − w
=
1 + (u + jv)
1 − (u + jv)
=
(1 + u) + jv)
(1 − u) + jv
=
(cid:0)(1 + u) + jv(cid:1)(cid:0)(1 − u) + jv(cid:1)
(cid:0)(1 − u)2 + v2(cid:1)
=
=
(1 − u2 − v2) + 2jv
1 + u2 + v2 − 2u
=
(1 − |w|2) + 2jv
1 + |w|2 − 2u
That is
u(cid:48) =
1 − |w|2
1 + |w|2 − 2u
2One could also notice that (9.10) are very similar to the bilinear transform and its inverse,
where the latter two have an additional scaling by T /2.
9.3. ROTATIONS OF RIEMANN SPHERE
315
v(cid:48) =
2v
1 + |w|2 − 2u
On the other hand,
|w(cid:48)|2 =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1 + w
1 − w
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
|1 + w|2
|1 − w|2 =
(1 + u)2 + v2
(1 − u)2 + v2 =
1 + |w|2 + 2u
1 + |w|2 − 2u
and
|w(cid:48)|2 + 1 =
(1 + |w|2 + 2u) + (1 + |w|2 − 2u)
1 + |w|2 − 2u
= 2
1 + |w|2
1 + |w|2 − 2u
from where by (9.7d)
¯z(cid:48) =
2
|w(cid:48)|2 + 1
=
1 + |w|2 − 2u
1 + |w|2
Then, using (9.7) we obtain
x(cid:48) = ¯z(cid:48)u(cid:48) =
y(cid:48) = ¯z(cid:48)v(cid:48) =
1 − |w|2
1 + |w|2 = −z
2v
1 + |w|2 = y
z(cid:48) = 1 − ¯z(cid:48) =
(1 + |w|2) − (1 + |w|2 − 2u)
1 + |w|2
=
2u
1 + |w|2 = x
That is x(cid:48) = −z, y(cid:48) = y, z(cid:48) = x which is simply a rotation by 90◦ around the
y axis in the direction from the (positive) x axis towards the (positive) z axis.
Thus ρ+1 simply rotates the Riemann sphere around the imaginary axis of the
complex plane w by 90◦ in the direction from 1 to ∞, or, which is the same,
in the direction from 0 to 1 (where by 0, 1 and ∞ we mean the points on the
Riemann sphere which are the projection images of w = 0, w = 1 and w = ∞
respectively). The points ±j are thereby untouched, which can be also seen by
directly evaluating ρ+1(±j) = ±j. The transformation ρ−1 being the inverse of
ρ+1 simply rotates in the opposite direction.
In terms of the real Riemann circle ρ±1 clearly correspond to a counterclock-
wise (for ρ+1) or clockwise (for ρ−1) rotation by 90◦ (Fig. 9.6).3 Respectively,
in the arctangent scale they correspond to shifts by a quarter of the arctangent
scale’s period.
The imaginary Riemann circle is rotated into the unit circle |w(cid:48)| = 1, which is
its own image on the Riemann sphere. Therefore for ρ+1 the polar angle ϕ from
Fig. 9.2 becomes the polar angle in the complex plane (since ϕ = 0 ⇐⇒ w = 0
and since arg ρ+1(0) = arg 1 = 0), therefore ϕ = arg ρ+1(w) and by (9.1) we
have
(cid:16)
ρ+1
j tan
(cid:17)
ϕ
2
= ejϕ
This can also be veriﬁed by direct substitution, where it’s easier to use the
inverse transformation:
ρ−1(ejϕ) = =
ejϕ − 1
ejϕ + 1
=
ejϕ/2 − e−jϕ/2
ejϕ/2 + e−jϕ/2
=
3This is also the reason for the notation ρ+1: the subscript simply indicates the result of
the transformation of w = 0.
316
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
−1
∞
0
1
Figure 9.6: Transformation of the real Riemann circle by ρ+1.
= j
ejϕ/2 − e−jϕ/2
2j
·
2
ejϕ/2 + e−jϕ/2
= j tan
ϕ
2
For ρ−1 the polar angle ϕ gets mapped into arg ρ−1(w) = π − ϕ. Conversely a
polar angle equal to π − ϕ will be mapped into arg ρ−1(w) = ϕ, thus
ejϕ = ρ−1
(cid:18)
j tan
(cid:19)
π − ϕ
2
= ρ−1
(cid:16)
j tan
(cid:17)(cid:17)
(cid:16) π
2
−
ϕ
2
= ρ−1
(cid:18) j
(cid:19)
tan ϕ
2
Summing up:
ρ+1
ϕ
2
(cid:16)
j tan
(cid:18) j
(cid:19)
tan ϕ
2
(cid:17)
= ejϕ
= ejϕ
ρ−1
ρ−1
(cid:0)ejϕ(cid:1) = j tan
ϕ
2
ρ+1
(cid:0)ejϕ(cid:1) =
j
tan ϕ
2
(9.11a)
(9.11b)
(9.12a)
(9.12b)
Note that (9.12) do not simply give the inverse versions of (9.11), but also reﬂect
the fact the complex unit circle gets rotated into the imaginary Riemann circle.
Symmetries
The transformations of symmetries by ρ±1 can be derived in an intuitive way
from the symmetries on the real Riemann circle and the arctangent scale and
from the fact that these transformations are ±90◦ rotations of the real Riemann
circle or (equivalently) shifts of the arctangent scale by the scale’s quarter period,
if we assume w ∈ R ∪ ∞.
Particularly, a ±90◦ rotation of the real Riemann circle maps 0 and ∞ to ±1
and vice versa. Respectively, points on the Riemann circle which are symmetric
relatively to 0 and ∞ map to points symmetric relatively to ±1 and vice versa.
Thus, mutually opposite values map to mutually reciprocal values and vice
versa:
ρ+1(w)ρ+1(−w) = 1
(9.13a)
9.3. ROTATIONS OF RIEMANN SPHERE
ρ−1(w)ρ−1(−w) = 1
ρ+1(w) + ρ+1(1/w) = 0
ρ−1(w) + ρ−1(1/w) = 0
317
(9.13b)
(9.13c)
(9.13d)
Fig. 9.7 illustrates, where more properties are immediately visible. E.g. one
could notice that ρ±1(w) are obtained by rotating w by ±90◦, therefore the
results are 180◦ apart, which means:
ρ+1(w)ρ−1(w) = −1
(9.14)
or that rotating the opposite points ±w by 90◦ in opposite directions produces
opposite points:
ρ±1(−w) = −ρ∓1(w)
(9.15)
etc.
−1/w
∞
1/w
ρ+1(1/w) = ρ−1(−w)
ρ+1(w) = ρ−1(1/w)
−1
1
ρ+1(−1/w) = ρ−1(w)
ρ+1(−w) = ρ−1(−1/w)
−w
w
0
Figure 9.7: Symmetries of the transformations ρ±1 on the real
Riemann circle, where they represent 90◦ rotations.
The formulas (9.13), (9.14) and other similarly obtained symmetry-related
properties of ρ±1 work not only for w ∈ R ∪ ∞ but actually for any w ∈
C ∪ ∞ which can be veriﬁed algebraically. However, the interpretation of these
symmetries in terms of the real Riemann circle obviously works only for w ∈
R ∪ ∞.
The imaginary Riemann circle gets rotated by ρ±1 into the complex unit
circle, where the results of the two transformations are thereby symmetric rela-
tively to the imaginary axis. Fig. 9.8 illustrates some of the symmetries arising
out of this rotation. More illustrations of this kind can be created, particularly
for the transformation of the values lying on the complex unit circle, but the
ones we are already having should be suﬃcient for our purposes in this book.
Unit circle rotations
The Riemann sphere rotations ρ±1 can be described as rotations of the Riemann
sphere around the imaginary axis or as rotations of the real Riemann circle.
Similarly the Riemann sphere rotations around the vertical axis z can be thought
of as rotations of the unit circle.
Apparently, such rotations are simply achived by a multiplications of com-
plex values by a unit-magnitude complex constant, where we are not restricted
to rotations by multiples of 90◦. We won’t need a special notation for this
318
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
ρ+1
(cid:16) j
(cid:17)
tan ϕ
= −e−jϕ
j
ρ+1(j tan ϕ) = ejϕ
−1
0
ρ+1
(cid:17)
(cid:16) −j
tan ϕ
= −ejϕ
ρ+1(−j tan ϕ) = e−jϕ
−j
Figure 9.8: Symmetries of the results of the transformation ρ+1 of
the imaginary axis (lying on the complex unit circle).
transformation and will simply write
Obviously
w(cid:48) = aw
(|a| = 1)
|w(cid:48)| = |w|
arg w(cid:48) = arg w + arg a
The rotations by +90◦ and −90◦ are simply multiplications by j and −j respec-
tively.
There is not much more to say in this respect as this rotation is pretty trivial.
Imaginary rotations
We could also wish to rotate the imaginary Riemann circle. We will denote the
respective transformations as ρ±j, where the subscript, as with ρ±1, denotes the
image of the zero after the transformation.
We could rotate the imaginary circle by doing the three steps in succession:
1. First, we rotate the Riemann sphere by 90◦ around its “vertical” axis,
thereby turning the imaginary Riemann circle into the real Riemann circle,
where w = ±j is transformed into w = ±1 respectively, while 0 and ∞
stay in place. Such rotation is simply achieved by multiplying w by −j.
2. Now we apply ρ±1 to rotate the real circle.
3. We convert the real circle back to the imaginary one by multiplying the
rotation result by j.
Therefore we simply have
ρ±j(w) = jρ±1(−jw) = −jρ∓1(jw)
(9.16)
(where the second expression is obtained in the same way as the ﬁrst one, except
that we rotate the Riemann sphere in the other direction, both vertically and
horizontally).
In order to distinguish between ρ±1 and ρ±j we could refer to the former as
real rotations of the Riemann sphere and to the latter as imaginary rotations
of the Riemann sphere.
9.4. BUTTERWORTH FILTER REVISITED
319
9.4 Butterworth filter revisited
In Chapter 8 we have developed the lowpass Butterworth ﬁlters of the 1st kind
(also simply known as Butterworth ﬁlters) as a Butterworth transformation of
the 1-pole lowpass ﬁlter, where we also mentioned that the tranditional deﬁni-
tion of the Butterworth ﬁlter simply deﬁnes the (lowpass) Butterworth ﬁlter as
a (stable) ﬁlter whose amplitude response is
|H(jω)|2 =
1
1 + ω2N
(ω ∈ R)
(9.17)
Apparently both deﬁnitions are equivalent.
We could generalize this idea by replacing ωN in (9.17) by some other poly-
nomial function f (ω):
|H(jω)|2 =
1
1 + f 2(ω)
(ω ∈ R)
(9.18)
The practical application of (9.18) essentially follows the steps of the But-
terworth transformation of the 1st kind, which includes solving 1 + f 2 = 0
to obtain the poles of |H(s)|2 and then discarding the right-semiplane poles,
thereby eﬀectievely obtaining the desired transfer function H(s) expressed in
the cascade form. Note that there are two implicit conditions which need to be
fulﬁlled in order for this procedure to work:
- There is the symmetry of the poles of |H(s)|2 with respect to the real axis:
if s is a pole then so is s∗ (where s∗ may be the same pole as s if s is real).
This is necessary in order for H(s) to be a real function of s.
- There is the pairwise symmetry of the poles |H(s)|2 with respect to the
imaginary axis: if s is a pole then −s∗ is another pole (it must be another
pole even if s = −s∗, in which case it simply means that the pole is
duplicated). This guarantees that we can split the poles into the left-
and right-semiplane halves with identical contributions to the amplitude
response. Therefore by discarding the right-semiplane half of the poles,
we eﬀectively go from |H(jω)|2 to |H(jω)|.
We could expect that these properties will not be fulﬁlled for an arbitrary f (ω).
However, let’s require that
- The function f (ω) is a real function of ω.
- The function f (ω) is odd or even.
The readers can convince themselves that under these restrictions the poles of
|H(s)|2 deﬁned by (9.18) will have the necessary symmetries with respect to the
real and imaginary axes.
Rational f (ω)
We could allow f (ω) to be not just a polynomial but a rational function. In this
case H(s) has not only poles, but also zeros at locations where f (ω) has poles
and respectively the denominator of |H(jω)|2 turns to ∞, which means that
both |H(jω)|2 and |H(jω)| turn to zero. In order to see that the multiplicities
320
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
of the zeros of H(s) are equal to the multiplicities of the respective poles of of
f (ω) simply consider
|H(jω)|2 =
1
P 2
1 (ω)
P 2
2 (ω)
1 +
=
P 2
2 (ω)
1 (ω) + P 2
P 2
2 (ω)
Thus the set of zeros of |H(jω)|2 is the duplicated set of zeros of P2(ω), how-
ever after switching to H(jω) we should drop the duplicates, being left only
with a single set of zeros of P2(ω), which are the poles of f (ω). Note that
H(s) shouldn’t have more zeros than poles, which means that the order of the
denominator of f (ω) should not exceed the order of the numerator of f (ω).
In order for H(s) to be real, its zeros must be conjugate symmetric, which
will be ensured if f (ω) is real odd or even function. Indeed, in this case the
poles of f (ω) are both conjugate symmetric and symmetric with respect to the
origin, which implies that they are also symmetric with respect to the imaginary
ω axis, which is the same as the symmetry with respect to the real s axis.
Representations of linear scaling
We are now going to develop another way of looking at the Butterworth ﬁlter
generating function f (x) = xN . It will be highly useful with other functions
f (x) occurring in place of f (x) = xN in (9.18).
Let f (x) = xN . Since xN = exp(N ln x), we can write
f (x) = exp(N ln x)
(9.19)
Introducing auxiliary variables u and v we can rewrite (9.19) as a set of equa-
tions:
x = exp u
v = N u
f (x) = exp v
which also allows to deﬁne f (x) implicitly as a function satisfying the equation
f (exp u) = exp(N u)
(9.20)
We could consider x and f (x) as representations of u and v, where the connection
between the preimages u and v and their respective representations x and f (x) is
achieved via the exponential mapping x = exp u (Fig. 9.9). In terms of preimage
domain the function f (x) = xN is simply a multiplication by N .
Now consider that the function exp u is periodic in the imaginary direction:
exp u = exp(u + 2πj)
Therefore preimages are 2πj-periodic, that is if u is a representation of x, then
so is u + 2πjn ∀n ∈ Z (Fig. 9.10).
A multiplication by an integer in the preimage domain v = N u expands one
period Im u ∈ [0, 2π] to N periods Im v ∈ [0, 2πN ]. Respectively the function
f (x) takes each value N times as x goes across all possible values in C. Con-
versely, a division by an integer u = v/N shrinks N periods to one period and
9.4. BUTTERWORTH FILTER REVISITED
321
Representation
domain
Preimage
domain
x
u
p
x
e
=
x
u
f (x) = xN
v = N u
xN
u
N
p
x
e
=
N
x
N u
Figure 9.9: The preimage and representations domains.
Im u
6πj
4πj
2πj
0
−2πj
−4πj
−6πj
Re u
Figure 9.10: Periods of the preimage of the representation x =
exp u. Each strip (where there is no diﬀerence between gray and
white strips) denotes a preimage of the entire complex plane with
the exception of zero. The preimages denoted by the dots are
preimages of one and the same value.
a previously single value of f (x) turns into N diﬀerent values of x. This is an-
other possible way to explain the fact that the equation xN = a has N diﬀerent
solutions. Fig. 9.11 demonstrates the result of transformation of all preimages
in Fig. 9.10 by a division by 2 corresponding to the equation u = v/2. Notice
that the preimages in Fig. 9.11 correspond to two diﬀerent representation val-
ues, while the peimages in Fig. 9.10 were corresponding to one and the same
representation value.
The preimage of the real axis x ∈ R consists of two “horizontal” lines Im u =
0 and Im u = π, or, more precisely, of their periodic repetitions Im u = 2πn
and Im u = π + 2πn, where n ∈ Z. The line Im u = 0 (and its repetitions)
322
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Im u
6πj
4πj
2πj
0
−2πj
−4πj
−6πj
Re u
Figure 9.11: Transformation of the preimage points in Fig. 9.10 by
division by 2.
corresponds to the positive real numbers, the line Im u = π (and its repetitions)
corresponds to the negative real numbers. The preimage of the zero x = 0 exists
only in the limiting sense Re u → −∞. Thus the multiplication of u by N is
mapping the preimage of the real axis onto itself, therefore this multiplication’s
representation xN maps the real axis onto itself.
The “vertical” lines Re u = a are preimages of circles of radius ea, where
moving upwards along such lines corresponds to the counterclockwise movement
along the respective circle (Fig. 9.12), Particularly the imaginary axis is the
preimage of the unit circle (note that a section of such line extending over a
single imaginary period Im u ∈ [b, b + 2π) is suﬃcient to generate all possible
values on such circle). Multiplication by N maps the imaginary axis onto itself,
thereby its representation xN maps the unit circle onto itself. A single preimage
period [0j, 2πj] is thereby mapped to N periods [0, 2πjN ], which corresponds
to the unit circle being mapped to itself “N times”. We will shortly see that
the mapping of the unit circle onto itself is the reason for the Butterworth poles
being located on the unit circle.
Even/odd poles
The poles of (9.18) are given by 1 + f 2 = 0, which can be rewritten as f = ±j.
In the Butterworth case the solutions of f = ±j were interleaved on the unit
circle and also corresponded to even and odd values of n in the solution formula
for 1 + f 2 = 0, where f = j was deﬁning the even poles and f = −j was deﬁning
the odd poles.
We will take eﬀort to keep the same correspondence between the equations
f = ±j and the even/odd values of n for other functions f (ω). In that regard
it is instructive to ﬁrst review the Butterworth case, but now using the just
introduced linear scaling representation form, as it will then nicely generalize to
9.4. BUTTERWORTH FILTER REVISITED
323
Im x
Re u
Re x
0
Im u
4πj
2πj
0
−2πj
−4πj
Figure 9.12: A circular trajectory and its preimage.
other f (ω) that we are going to use.
Let ω move along the unit circle in the counterclockwise direction. Its preim-
age u deﬁned by ω = exp u will respectively move upwards along the imaginary
axis and so will v = N u. Respectively f (ω) = exp v moves along the unit circle
in the counterclockwise direction, just N times faster, so while f (ω) completes
one circle, ω will complete only 1/N -th of a circle. The value of f (ω) will be
passing through the points j and −j, since they are lying on the unit circle. At
these moments the value of ω will be the solution of the equations f (ω) = j
and f (ω) = −j respectively (Figs. 9.13 and 9.14). There will be no other solu-
tions since if ω moves in a circle of any other radius, this circle will map to a
circle of a non-unit radius and f (ω) will not go through the points ±j. Thus
Fig. 9.14 contains the full set of Butterworth poles, where the interleaving of
the white/black dots on the circle in Fig. 9.14 arises from the interleaving of the
white/black dots on the circular tajectory in Fig. 9.13.
Im v
2πj
0
−2πj
Im f (ω)
Re v
Re f (ω)
0
Figure 9.13: f (ω) moving in a unit-radius circular trajectory, the
points f (ω) = ±j and their preimages.
324
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Im ω
Re u
Re ω
0
Im u
2πj
0
−2πj
Figure 9.14: Transformation of Fig. 9.13 by u = v/N (for N = 2).
The white and black dots on the circle are even/odd Butterworth
poles in terms of ω.
Apparently, the passing of f (ω) through ±j corresponds to
v = jπ
(cid:19)
+ n
(cid:18) 1
2
where f (ω) = j occurs at even n and f (ω) = −j occurs at odd n. The value of
u at these moments is
u = jπ
1
2 + n
N
and the value of ω is
(cid:18)
ω = exp
jπ
(cid:19)
1
2 + n
N
which is pretty much the same as the expression (8.13) we have developed before.
The even/odd values of n still correspond to the solutions of f = j and f = −j
respectively and thus we have a constency in referring to the solutions of f = j
as even poles and to the solutions of f = −j as odd poles.
Lowpass, bandpass and highpass filters
If we want H(s) in (9.18) to be a (unit cutoﬀ) lowpass ﬁlter, then we should
impose some additional requirements on f (ω):
f (ω) ≈ 0
for ω (cid:28) 1
f (ω) ≈ ∞ for ω (cid:29) 1
(9.21)
where around ω = 1 the absolute magnitude of f (ω) should smoothly grow from
0 to ∞. Apparently the Butterworth ﬁlter’s function f (ω) = ωN satisﬁes these
requirements.
Similarly to Butterworth ﬁlter, with other ﬁlter types arising from (9.18)
we will not be constructing f (x) which give a highpass or bandpass response.
Instead, highpass and bandpass ﬁlters can be simply obtained by LP to HP and
LP to BP substitutions respectively.
9.5. TRIGONOMETRIC FUNCTIONS ON COMPLEX PLANE
325
9.5 Trigonometric functions on complex plane
The trigonometric functions, such as sin x, cos x, tan x and so on can be evalu-
ated for complex argument values. In that regard they occur to be closely related
to the hyperbolic functions sinh x, cosh x, tanh x and so on. The extention to
x ∈ C can be obtained by simply evaluating the formulas
cosh x =
sinh x =
tanh x =
cos x =
sin x =
ex + e−x
2
ex − e−x
2
sinh x
cosh x
ejx + e−jx
2
ejx − e−jx
2j
= cosh jx
= −j sinh jx
tan x =
sin x
cos x
=
1
j
·
ejx − e−jx
ejx + e−jx = −j tanh jx
(9.22)
(9.23)
(9.24)
(9.25)
(9.26)
(9.27)
etc., where the function ex is allowed to take complex argument values. Notice
that thereby we immediately obtain the “imaginary argument properties”:
sin jx = j sinh x
cos jx = cosh x
sinh jx = j sin x
cosh jx = cos x
(9.28a)
(9.28b)
(9.28c)
(9.28d)
etc., where intuitively we assume x ∈ R, but the formulas also work for x ∈ C.
By direct evaluation one could verify that all basic properties and fundamen-
tal trigonometric and hyperbolic identities continue to hold in complex domain.
Particularly
sin(−x) = − sin x
cos(−x) = cos x
cos(x + 2π) = cos x
cos(x − π/2) = sin x
sin2 x + cos2 x = 1
sinh(−x) = − sinh x
cosh(−x) = cosh x
cosh2 x − sinh2 x = 1
etc. Also, apparently, conjugation commutes with the respective functions:
sin x∗ = (sin x)∗
cos x∗ = (cos x)∗
sinh x∗ = (sinh x)∗
cosh x∗ = (cosh x)∗
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
326
etc.
A direct corollary of (9.28) and the trigonometric formulas for the sum
of arguments are the formulas allowing to express a trigonometric function a
complex argument via the real and imaginary parts of the argument. E.g.
cos(u + jv) = cos u cos jv − sin u sin jv = cosh v cos u − j sinh v sin u (9.29a)
sin(u + jv) = sin u cos jv + cos u sin jv = cosh v sin u + j sinh v cos u (9.29b)
etc.
Periodicity
Since the periodicity property is retained in the complex domain, the former real
periods of the trigonometric functions turn into strips on the complex plane. E.g.
the 2π periods of cos x are shown in Fig. 9.15.
Im x
−4π
−2π
0
2π
4π
Re x
Figure 9.15: Periods of cos x in the complex plane. All dots are
preimages of one and the same value.
Due to the even symmetry of the cosine, almost every value occurs twice on
a period (as illustrated by the dots in Fig. 9.15). That is if the value y occurs
at x (that is y = cos x), then y also occurs at −x. The exceptions are being
cos x = 1 and cos x = −1, which are mapped to themselves by x ← −x if the
periodicity of cos x is taken into account.
Inverse functions
Inverting (9.22) and (9.23) we obtain
cosh−1 x = ln
sinh−1 x = ln
(cid:16)
x ±
(cid:16)
x ±
(cid:112)
(cid:112)
x2 − 1
x2 + 1
(cid:17)
(cid:17)
The principal value of the complex square root is deﬁned as
√
x = exp
ln x
2
= (cid:112)|x| · exp j
arg x
2
(9.30)
9.5. TRIGONOMETRIC FUNCTIONS ON COMPLEX PLANE
327
x ≥ 0 ∀x ∈ C thereby (9.30) is a
where arg x ∈ [−π, π],4 in which case Re
generalization of arithmetric square root of real argument to complex domain.
Notice that (9.30) gives the values on the upper imaginary semiaxis for real
x < 0 (provided arg x = π for x < 0).
√
The principal value of ln x is deﬁned in the usual way:
Respectively we can introduce the principal values of cosh−1 and sinh−1 as:
ln x = ln |x| + j arg x
cosh−1 x = ln
sinh−1 x = ln
(cid:16)
(cid:16)
x +
x +
(cid:112)
(cid:112)
(cid:17)
x2 − 1
(cid:17)
x2 + 1
(9.31a)
(9.31b)
where we chose the signs in front of the square roots in such as way as to ensure
that cosh−1 x ≥ 0 ∀x ≥ 1 and sinh−1 x ∈ R ∀x ∈ R.
The formulas (9.31a), (9.31b) raise concerns of numerical robustness in cases
where the two terms under the logarithm sign are nearly opposite. By recipro-
cating the values under the logarithm signs we can rewrite them equivalently
as
cosh−1 x = − ln
(cid:16)
x −
(cid:112)
x2 − 1
(cid:17)
sinh−1 x = − ln
(cid:16)(cid:112)
x2 + 1 − x
(cid:17)
(9.31c)
(9.31d)
where the choice between (9.31a), (9.31b) and (9.31c), (9.31d) should be made
based on comparing the complex arguments of the two terms under the loga-
rithm sign. We should choose the formulas where we are adding two numbers
whose complex arguments are not further than 90◦ apart. Particularly, for real
x we may write
sinh−1 x = sgn x · ln
(cid:16)
|x| +
(cid:112)
x2 + 1
(cid:17)
(x ∈ R)
(9.31e)
whereas the formula (9.31a) already works well for real x ≥ 1.
Using (9.28) we can construct the principal values:
arccos x = −j cosh−1 x =
(cid:112)
(cid:16)
= −j ln
x +
x2 − 1
(cid:17)
= j ln
(cid:16)
x −
(cid:112)
x2 − 1
(cid:17)
(9.32a)
arcsin x = −j sinh−1 jx =
(cid:112)
(cid:16)
= −j ln
jx +
1 − x2
(cid:17)
= j ln
(cid:16)(cid:112)
1 − x2 − jx
(cid:17)
(9.32b)
However besides the precision issues there are issues related to the principal
values of arg x switching leaf on the negative real axis. Technically this means
that there is a discontinuity in the principal values of
and ln on the negative
real axis. With (9.31) this was generally tolerable, as the discontinuities weren’t
√
4We specifically leave it undefined, whether arg x = π or −π for negative real numbers,
as this is anyway a line on which the principal value of arg x has a discontinuity and thus,
considering the usual computation precision losses, one often can’t rely on the exact value of
arg x being returned for x < 0.
328
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
arising for the “usual” values of the argument, which are x ≥ 1 for cosh−1 x
and x ∈ R for sinh−1 x, since neither the argument of the square root nor the
argument of the logarithm become real negative in such cases. With (9.32a), on
the other hand, we do have a negative expression under the square root for real
x ∈ (−1, 1), which is the most important argument range.
We could therefore adjust (9.32a) to
(cid:17)
(cid:16)
(cid:112)
arccos x = −j ln
x + j
1 − x2
= j ln
(cid:16)
x − j
(cid:112)
1 − x2
(cid:17)
(9.32c)
The formulas (9.32b) and (9.32c) work well for x ∈ [−1, 1], particularly precision-
wise it doesn’t matter which of the two options in (9.32b) and (9.32c) are taken
for x ∈ [−1, 1], however they exibit a discontinuity for real x : |x| > 1. On the
other hand, for purely imaginary argument values the formula (9.32b) can be
rewritten essentially as (9.31e) to automatically choose the best option precision-
wise:
arcsin jx = j sinh−1 x = j sgn x · ln
(cid:16)
|x| +
(cid:112)
x2 + 1
(cid:17)
(x ∈ R)
(9.32d)
Preimages of the real line with respect to cos x
By (9.29a) cos x attains purely real values iﬀ x ∈ R or Re x = πn where n ∈ Z.
However, due to periodicity and evenness properties, each value is attained
inﬁnitely many times. We would like to choose a principal preimage of the real
line with respect to cos x. That is, we are interested in a (preferably continuous)
minimal set of points, whose image under transformation y = cos x is R.
Note, that we do not really have to choose this principal preimage, as the
discussions where we are going to refer to it should lead to exactly the same
results no matter which of the preimages of the real line is taken. However,
for the sake of clarity of discussion it is convenient to have an unambiguous
reference preimage.
Apparently there are inﬁnitely many choice possibilities, among which there
are at least several “reasonable” ones. For the purposes of this text we will
choose the principal preimage as shown in Fig. 9.16. The same ﬁgure also
shows the periodic repetitions of the principal preimage.5
This principal preimage of the real axis thereby consists of three parts:
x ∈ [0, π]
⇐⇒ y ∈ [−1, 1]
x ∈ [0, +j∞) ⇐⇒ y ∈ [1, +∞)
x ∈ [π, π + j∞) ⇐⇒ y ∈ (−∞, −1]
Apparently the principal preimage alone doesn’t cover all preimage points of
the real line. Neither does it if we add its periodic repetitions in Fig. 9.16, since
the lower-semiplane points of lines Re x = πn are still not included. We can
include them by simply rotating all preimages in Fig. 9.16 around the origin,
which corresponds to multiplication of all points x by −1. Notice that by adding
periodic repetitions we addressed the periodicity of cos x, while by adding the
preimages multiplied by −1 we addressed the evenness property of cos x.
5Notice that the principal preimage in Fig. 9.16 doesn’t necessarily coincide with the set
of values returned by the formulas (9.32), particularly since the arccos formulas in (9.32)
exhibit discontinuities either for x ∈ [−1, 1] or for real x : |x| > 1. The main reason to choose
this specific preimage is that its generalization to the case of Jacobian elliptic cosine will be
convenient for our purposes.
9.5. TRIGONOMETRIC FUNCTIONS ON COMPLEX PLANE
329
Im x
−2π
−π
0
π
2π
Re x
Figure 9.16: The principal preimage (solid line) of the real axis,
with respect to y = cos x, and its periodic repetitions (dashed
lines).
Representations of horizontal preimage lines by cos x
Equation (9.29a) implies that if the imaginary part v is ﬁxed and the real part
u is varying, that is the argument of the cosine is moving in a line parallel to the
real axis, then the value of cos(u + jv) is moving along an ellipse in the complex
plane, the real semiaxis of the ellipse being equal to cosh v and the imaginary
semiaxis being equal to sinh v. Fig. 9.17 illustrates. At v = 0 the real semiaxis
is 1 and the imaginary semiaxis is zero. As |v| grows both semiaxes grow, the
imaginary semiaxis staying smaller than the real one in absolute magnitude
(Fig. 9.18). Both semiaxes become equal in the limit v → ∞ where the ellipse
turns into a circle.
v
Im y
y = cos(u + jv)
−4π
−2π
0
2π
4π
0
u
Re y
Figure 9.17: An elliptic trajectory and two its preimages. The
picture is qualitative. In reality, for this kind of ellipse proportions
the preimages would need to be located much closer to the real
line.
Given v > 0 and increasing u, the movement of the point cos(u + jv) along
the ellipse will be in the negative (clockwise) direction, due to the − sign in front
of the imaginary part in (9.29a). Respectively the positive (counterclockwise)
330
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Im y
y = cos(u + jv)
−1
j
0
−j
1
Re y
Figure 9.18: A family of elliptic trajectories generated from hori-
zontal preimages v = const < 0.
direction movement will occur either for a decreasing u or for a negative v,
where the latter is illustrated in Fig. 9.17.
The even symmetry of the cosine (cos(−x) = cos x) implies that for each
horizontal trajectory u + jv of the cosine’s argument, there is a symmetric
trajectory −(u + jv) which produces exactly the same cosine trajectory. This
other trajectory is shown in Fig. 9.17 by the dashed line. Notice how this is
related to the fact that ﬂipping the sign of v ﬂips the direction of the movement
along the ellipse: ﬂipping the sign of v is the same as ﬂipping the sign of the
entire cosine’s argument (that is ﬂipping the signs of both u and v), which leaves
the elliptic trajectory unaﬀected, and then ﬂipping the sign of u, which reverts
the direction of movement of both u + jv and cos(u + jv).
From the fact that the semiaxes of the ellipse are cosh v and sinh v and
therefore their absolute magnitudes are monotonically growing with |v| (as one
can see in Fig. 9.18) we can deduce that ellipses corresponding to diﬀerent v
do not overlap, except for a switch from v to −v, which simply changes the
direction of the movement along the ellipse. That is for a given ellipse with
cosh v and sinh v semiaxes there are only two preimages, as shown in Fig. 9.17.
An exception occurs when the imaginary semiaxis of the ellipse is zero, in which
case there is only one preimage, which is the real line.
Representations of horizontal preimage lines by sec x
The secant function sec x = 1/ cos x is obviously 2π-periodic. In fact it bears
quite a few further similarities to cos x, which are easier to see if we write it in
the polar form:
|sec x| =
1
|cos x|
arg sec x = − arg cos x
(9.33a)
(9.33b)
9.5. TRIGONOMETRIC FUNCTIONS ON COMPLEX PLANE
331
Consider a horizontal line u + jv (where u is varying and v = const) and its
respective representation y = sec(u + jv). According to (9.33), y moves in an
ellipse-like curve around the origin, as shown in Fig. 9.19. At smaller magnitudes
of v the curve begins to look more like a ﬁgure of 8 (Fig. 9.20). The curve is not
an ellipse anymore due to the reciprocation in (9.33a). On the other hand, by
(9.33b) the “angular velocity” is the same as with y = cos x, except that is has
the opposite sign, therefore the rotation is happening in the opposite direction.
Therefore for an increasing u we get counterclockwise rotation iﬀ v > 0 rather
than iﬀ v < 0.
v
Im y
y = sec(u + jv)
−4π
−2π
0
2π
4π
0
u
Re y
Figure 9.19: A quasielliptic trajectory and two its preimages (qual-
itatively).
Im y
y = sec(u + jv)
1
Re y
−1
j
0
−j
Figure 9.20: A family of quasielliptic trajectories generated from
horizontal preimages v = const > 0.
332
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
9.6 Chebyshev polynomials
An N -th order Chebyshev polynomial is deﬁned as:
TN (x) = cos (N arccos x)
(9.34)
Fig. 9.21 illustrates. Notice the bipolar oscillations of equal amplitude (referred
to as equiripples) on the range [−1, 1]. As one can see in Fig. 9.21, the equiripple
amplitude is unity.
Somewhat surprisingly, (9.34) can be equivalently written as an N -th order
real polynomial of x at any N ∈ N, e.g. for N = 4 we have T4(x) = 8x4 −8x2 +1,
which is why they are called polynomials.
TN (x)
1
0
-1
−1
1
x
Figure 9.21: Chebyshev polynomials of even (solid) and odd
(dashed) orders.
Note that arccos x takes complex values for x > 1 and x < −1. We will
also often assume x taking complex values, therefore arccos x and TN (x) will be
complex as well. For |x| > 1 even though arccos x becomes complex, the value
cos(N arccos x) is still real, and so is the polynomial itself.
A proper discussion of Chebyshev polynomials falls outside the scope of the
book, as the respective information can be easily found elsewhere. Here we shall
concentrate on the details which will be important for our purposes.
Chebyshev polynomials as representations of linear scaling
Introducing auxiliary variables u and v we rewrite (9.34) as
x = cos u
v = N u
TN (x) = cos v
or, in the implicit form:
TN (cos u) = cos(N u)
(9.35)
Thus the function TN (x) is a representation of the linear scaling v = N u, the
mapping function being x = cos u. Note that by multiplying the whole preimage
domain by j we obtain a diﬀerent (but equivalent) representation:
x = cosh u
9.6. CHEBYSHEV POLYNOMIALS
333
v = N u
TN (x) = cosh v
which gives us another equivalent expression for (9.34)
TN (x) = cosh (cid:0)N cosh−1 x(cid:1)
(9.36)
and its respective implicit form
TN (cosh u) = cosh(N u)
In our discussion we will stick to using the cosine-based representation. The
readers may draw parallels to the hyperbolic cosine-based representation if they
wish.
In case of Butterworth ﬁlter-generating functions xN represented via exp u
mapping, the preimages were 2πj-periodic. This time they are 2π-periodic.
Additionally there is an even symmetry: u and −u are preimages of the same
x.
Considering the eﬀect the linear scaling v = N u on the principal preimage
of the real line shown in Fig. 9.16, we obtain the following:
- The principal real half-period [0, π] is expanded to to [0, πN ], which is re-
sponsible for the occurrence of the equiripples on the segment x ∈ [−1, 1].
Since N is integer, other real half-periods expand similarly, without gener-
ating yet more representation values of f (x). Thus f (x) is a single-valued
function.
- The principal preimage [0, +j∞) of x ∈ [1, +∞) maps onto itself. The
non-principal preimages [2πn + 0j, 2πn + j∞) of x ∈ [1, +∞) map onto
some other preimages [2πN n + 0j, 2πN n + j∞) of x ∈ [1, +∞). Similar
mappings occur for the preimages of x ∈ [1, +∞) located in the lower
complex semiplane. Thus x ∈ [1, +∞) is mapped by TN (x) onto itself,
corresponding to the monotonically growing behavior of TN (x) for x ≥ 1.
- The principal preimage [π + 0j, π + j∞) of x ∈ (−∞, −1] is mapped
onto [πN + 0j, πN + j∞), which is a preimage of x ∈ [1, +∞) is N is
even and of x ∈ (−∞, −1] if N is odd. The non-principal preimages of
x ∈ (−∞, −1] (both those in the upper complex semiplane and in the
lower complex semiplane) are mapped similarly. Thus x ∈ (−∞, −1] is
mapped by TN (x) onto itself if N is odd, or onto x ∈ [1, +∞) if N is even,
corresponding to the monotonic behavior of TN (x) for x ≤ −1.
Notice that these results correspond to the graphs in Fig. 9.21.
Now consider a line Im u = β (where β is some real constant value) parallel
to the real axis in the preimage domain. Such lines are are, as we know from
the previous discussion of the cosine of complex argument, the preimages of
ellipses of various sizes, where the size grows with |β| (Fig. 9.18). These ellipses
are also not overlapping each other, except that the ellipses corresponding to β
and −β are identical (but have opposite orientations). Therefore TN (x) maps
any ellipse from this family onto another ellipse from this family and vice versa,
similarly to how xN mapped the unit circle onto itself and mapped circles onto
334
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
other circles. This time, however, the ellipse which is mapped onto itself, is the
one with a zero imaginary semiaxis.
Notice that since the line Im u = β is mapped to Im v = N β, the line stays
in the same (upper or lower) semiplane after such mapping and goes in the same
(to the right or to the left) direction. Thus, if x is moving in an ellipse in a
counterclockwise or respectively clockwise direction, then TN (x) moves in the
same direction.
Even/odd property
Since cos(u ± π) = − cos u, a negation of x corresponds to a shift of its preimage
u by π. Respectively v is shifted by N π, which will result in a negation of
TN (x) if N is odd and will not change TN (x) is N is even. Therefore TN (x) is
even/odd if N is even/odd:
TN (−x) = (−1)N TN (x)
(9.37)
Values at special points
The principal preimage of x = 1 is u = 0. Therefore v = 0 and TN (x) = 1.
Therefore
By (9.37)
TN (1) = 1
TN (−1) = (−1)N
The principal preimage of x = 0 is u = π/2. Respectively v = N π/2 and
(cid:40)
0
(−1)N/2
if N is odd
if N is even
TN (0) = Re jN =
where Re jN is a way of writing the sequence 1, 0, −1, 0, . . . in the same way how
(−1)N is a way of writing the sequence 1, −1, 1, −1, . . ..
Leading coefficient
Knowing that TN (x) is a real polynomial of order N , we can obtain its leading
coeﬃcient aN by evaluating the limit
aN = lim
x→+∞
= lim
x→+∞
TN (x)
xN = lim
exp(N ln 2x)
2xN
x→+∞
cosh(N cosh−1 x)
xN
(2x)N
2xN = lim
x→+∞
x→+∞
exp(N cosh−1 x)
2xN
2N xN
2xN = 2N −1
= lim
x→+∞
= lim
=
For the purposes of this book’s material, we won’t need to be able to explicitly
ﬁnd the other coeﬃcients and will therefore skip this topic.
Zeros
Rather than being interested in the values of the coeﬃcients of Chebyshev poly-
nomials, for our purposes it will be more practical to know the locations of their
zeros. Letting TN (x) = 0 we have
v = π
(cid:19)
+ n
(cid:18) 1
2
9.6. CHEBYSHEV POLYNOMIALS
335
Respectively
and
u = π
1
2 + n
N
(cid:18)
x = cos
π
(cid:19)
1
2 + n
N
which means that the zeros are
(cid:18)
zn = cos
π
(cid:19)
1
2 + n
N
(9.38)
where there are N distinct values corresponding to 0 < u < π. Notice that the
zeros are all real and lie within (−1, 1). Also notice that zn = −zN −1−n, there-
fore the zeros are positioned symmetrically around the origin. Consequently, if
N is odd, one of zn will be at the origin.
Using (9.38) we can write TN (x) in the factored form:
TN (x) = xN ∧1 ·
(cid:89)
zn>0
x2 − z2
n
1 − z2
n
(9.39)
where we are taking the product only over the positive zeros using the symmetry
of the zeros relatively to the origin, and the odd factor xN ∧1 (where N ∧ 1
denotes bitwise conjunction) appears only for odd N where one of the zeros is
at the origin. The normalizations by (1 − z2
n) are simply appearing from the
requirement that each factor must be equal to 1 at x = 1, so that TN (x) = 1.
Renormalized Chebyshev polynomials
The factored form (9.39) oﬀers some nice insights into the comparison of the
behavior of TN (x) and xN . Writing xN is a comparable factored form we have
xN = xN ∧1 ·
(cid:89)
x2
zn>0
(9.40)
where “taking the product over zn > 0” means that we are having as many
factors as there are positive zeros in the Chebyshev polynomial TN (x). Thus
the diﬀerence between xN and TN (x) is that the factors x2 are replaced by
(x2 − z2
n)/(1 − z2
n).
Apparently, if zn → 0 ∀n then (x2−z2
n) → x2 and respectively (9.39)
is approaching xN . Unfortunately we cannot express this as simply TN (x) →
xN , since the zeros of TN (x) are ﬁxed.
n)/(1−z2
To mathematically express this variation of zeros, we can notice that the
value of (9.39) at x = 1 is always unity. Therefore we can introduce the poly-
nomials
T N (x, λ) =
ˆ
TN (x/λ)
TN (1/λ)
(9.41)
to which we refer as renormalized Chebyshev polynomials. By construction
T N (1, λ) = 1 ∀λ, while the zeros of
ˆ
zn = λzn. Therefore
ˆ
T N (x, λ) are
ˆ
T N (x, λ) = xN ∧1 ·
ˆ
(cid:89)
zn>0
x2 − (λzn)2
1 − (λzn)2
(9.42)
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
336
and
T N (x, λ) = xN
lim
λ→0ˆ
The formula (9.41) cannot be evaluated for λ = 0, however we apparently can
take the limit at λ → 0 as the value of TN (x, 0) and thus
T N (x, 0) = xN
ˆ
T N (x, 1) = TN (x)
ˆ
Notice that the formula (9.42) perfectly works at λ = 0.
Since the equiripple amplitude of TN (x) is unity, by (9.41) the equiripple
T N (x) is 1/TN (1/λ). The equiripple range x ∈ [−1, 1] of TN (x)
amplitude of
ˆ
is respectively transformed into the equiripple range x ∈ [−λ, −λ] of
T N (x, λ).
ˆ
Thus λ simultaneously controls the equiripple amplitude and the equiripple
range of
T N (x, λ) (Fig. 9.22).
ˆ
T7(x, λ)
ˆ
1
0
-1
−1
1
x
Figure 9.22: Renormalized Chebyshev polynomial
1 (solid), λ = 0.93 (dashed) and λ = 0 (thin dashed).
T 7(x, λ) for λ =
ˆ
As we won’t need λ < 0, we won’t consider that option. As for the large
values of λ, it will be practical to restrict the value of λ so that |λzn| < 1 ∀n.
Apparently this means λmax = 1/ max{zn} where 0 ≤ λ < λmax. Notice that
since |zn| < 1 ∀n, it follows that λmax > 1.
Often it will be even more practical to restrict λ to 0 ≤ λ ≤ 1. At λ = 1
the equiripple amplitude of
T N (x, λ) is already unity. As λ grows further the
ˆ
equiripple amplitude quickly grows, reaching ∞ at λ = λmax. Also the equiripple
range exceeds [−1, 1], which could become inconvenient for our purposes.
In order to simplify the notation, often we will omit the λ parameter, un-
derstanding it implicitly, and simply write
T N (x) instead of
ˆ
T N (x, λ).
ˆ
Slope at |x| ≥ 1
Let’s compare the factors of (9.42) and (9.40). Computing the diﬀerences:
x2 − (λzn)2
1 − (λzn)2 − x2 =
x2 − (λzn)2 − x2 + (λzn)2x2
1 − (λzn)2
=
(λzn)2(x2 − 1)
1 − (λzn)2
(9.43)
and taking into account that 0 < zn < 1, we notice that for |x| > 1 and
0 < λ ≤ λmax the diﬀerences (9.43) are strictly positive and respectively the
9.7. CHEBYSHEV TYPE I FILTERS
337
factors of (9.42) are larger than those of (9.40). In the range x > 1, since all
factors are positive, we have
T N (x) > xN
ˆ
(x > 1, N > 1)
By the even/odd symmetries of
T N (x) and xN :
ˆ
T N (x)| > |xN |
|
ˆ
(|x| > 1, N > 1)
From (9.43) we can also notice that the diﬀerence grows with λ, thus at larger λ
T N (x) exceeds xN (in absolute magnitude) by a larger amount.
the polynomial
ˆ
At λ = 1 we have
T N (x) = TN (x). For this speciﬁc case we would like
ˆ
to get a more exact estimation of the steepness of the slope at x = 1, to get
an idea of how much steeper is the slope of TN (x) compared to xN . We have
already seen that the leading coeﬃcient of TN (x) is 2N −1, which means that at
x → ∞ the polynomial TN (x) grows 2N −1 times faster than xN . It would be
also informative to compare their slope at x = 1.
An attempt to compute the derivative of TN (x) at x = 1 in a straightforward
manner results in an uncertainty, thus it’s easier to take a way around. At points
inﬁnitely close to x = 1 we expand the cosine into Taylor series up to the second
order term:
x = cos u = 1 −
TN (x) = cos v = 1 −
u2
2
v2
2
This scaling by N times in the preimage domain (v = N u) corresponds to
scaling by N 2 times in the representation domain (v2 = N 2u2) and we obtain
d
dx
(cid:12)
(cid:12)
TN (x)
(cid:12)x=1
= N 2
On the other hand
d
dx
Thus at x = 1 Chebyshev polynomials grow N times faster than xN .
xN (cid:12)
(cid:12)
(cid:12)x=1
= N
9.7 Chebyshev type I filters
Chebyshev (or, more precisely, Chebyshev type I) ﬁlters arise by using renormal-
T N (ω) as f (ω) in (9.18).6 The main motivation
ized Chebyshev polynomials
ˆ
to use renormalized Chebyshev polynomials instead of ωN (which is used in
Butterworth ﬁlters) is that, as we already know they grow faster than ωN for
|ω| > 1, which results in a steeper transition band compared to Butterworth
ﬁlters. The tradeoﬀ is that in order to achieve a steeper transition band we need
6Classically, Chebyshev filters are obtained from Chebyshev polynomials TN (ω) by letting
f (ω) = εTN (ω) where ε > 0 is some small value. This way however usually requires some
cutoff correction afterwards. The way how we introduce Chebyshev filters is essentially the
same, but directly results in a better cutoff positioning. One way is related to the other via
(9.44) combined with a cutoff adjustment by the factor λ.
338
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
to allow ripples in the passband. At the same time, the analytical expressions
(9.34) and (9.36) allow to easily obtain the function inversion of the polyno-
mial, allowing analytical computation of the ﬁlter’s internal variables (such as
pole positions) for arbitrarily high polynomial orders N , which would have been
impossible for polynomials of a fully general form.
Thus, in (9.18) we let
that is
f (ω) =
T N (ω)
ˆ
|H(jω)|2 =
1 +
1
T 2
N (ω)
ˆ
The λ parameter of
T N and
ˆ
thereby the equiripple amplitude in the passband of |H(jω)|. It is convenient
to introduce the additional variable
T N (ω) is aﬀecting the equiripple amplitude of
ˆ
ε =
1
TN (1/λ)
(9.44)
which is simply equal to the equiripple amplitude of
particularly may write
T N . Using (9.44) we
ˆ
f (ω) =
T N (ω) = εTN (ω/λ)
ˆ
Notice that (9.44) allows to compute ε from λ and vice versa. Therefore, if
we are given a desired equiripple band [−λ, λ], we thereby have speciﬁed λ and
can use (9.44) to compute ε. Conversely, if we are given a desired equiripple
amplitude (which is a more common case), we thereby have speciﬁed ε and can
invert (9.44) to compute λ:
1
λ
= T −1
N (1/ε) = cosh
(cid:18) 1
N
cosh−1 1
ε
(cid:19)
(where T −1
N denotes the inverted function TN ).
√
The amplitude response |H(jω)| is thus varying within [1/
1 + ε2, 1] on
the equiripple range ω ∈ [−λ, λ]. On the other hand, λ (or, equivalently, ε)
T N (and respectively the slope of |H(jω)|) at ω = 1. The
aﬀects the slope of
ˆ
slope steepness is thereby traded against the equiripple amplitude, where steeper
slopes are achieved at larger equiripple amplitudes. Fig. 9.23 illustrates.
Poles of Chebyshev type I filters
We have mentioned that the (9.34) is actually a polynomial of x. Therefore the
denominator of (9.18) is a polynomial of ω and we can ﬁnd the roots of this
polynomial, which are simultaneously the poles of |H(s)|2 = H(s)H(−s). The
equation for these poles is thus
or
or
1 +
N (ω) = 0
T 2
ˆ
T N (ω) = ±j
ˆ
εTN (ω/λ) = ±j
9.7. CHEBYSHEV TYPE I FILTERS
339
|H(jω)|, dB
0
-6
-12
-18
1/8
1
8
ω
Figure 9.23: Chebyshev type I ﬁlter’s amplitude responses for N =
4 and ε = 1 (solid), ε = 0.6 (dashed) and ε = 0 (Butterworth, thin
dashed).
or, introducing ¯ω = ω/λ
TN (¯ω) = ±
j
ε
(9.45)
It is quite helpful to use the interpretation of TN in terms of representation
preimage domain to solve (9.45). Recall that TN maps ellipses (of a special
ellipse family, where the real and imaginary semiaxes a and b are related as
a2 − b2 = 1, so that they can be represented as a = cosh β, b = sinh β for
some β) to ellipses (of the same family). In the preimage domain these ellipses
correspond to lines Im u = β parallel to the real axis.
Suppose ¯ω is moving in such an ellipse. This corresponds to its preimages
moving along two lines Im u = ±β. Let u be one of the preimages in Im u = β,
to which we will refer as the principal preimage. Respectively the full family
of preimages is ±u + 2πn. The principal preimage v of TN (¯ω) is therefore
v = N u, moving along the line Im v = N β. The full family of preimages of
TN (¯ω) is respectively ±N u + 2πN n and is moving along the lines Im v = ±N β.
Therefore TN (¯ω) is moving in an ellipse whose real and imaginary semiaxes are
cosh N β and sinh N β respectively.
We wish TN (¯ω) to move in a counterclockwise direction along an ellipse which
goes through the ±j/ε points. Then at the moments when TN (¯ω) = ±j/ε
we will obtain solutions of (9.45). We additionally wish the real part of the
preimage v of TN (¯ω) to be increasing during such movement,7 therefore for a
counterclockwise movement of TN (¯ω) we need Im v = N β < 0. Therefore, in
order for the ellipse to go through ±j/ε, the imaginary semiaxis sinh N β of this
ellipse must be equal to −1/ε and thus we obtain:
β = −
1
N
sinh−1 1
ε
7This choice is arbitrary, we simply better like the option of increasing real part of v.
Alternatively we could let the real part of v decrease, obtaining β > 0. However then we
would need to have a negative coefficient in front of n in (9.46).
340
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
According to (9.29a), the purely imaginary values of a cosine are attained
2 + n), where n ∈ Z.
when the real part of the cosine’s argument is equal to π( 1
Thus, the values ±j/ε will be attained by TN (¯ω) at
(cid:19)
v = jN β + π
+ n
(9.46)
(cid:18) 1
2
where, since β < 0, the value TN (¯ω) = j/ε is attained at n = 0 and other even
values of n. Thus, the solutions of the even pole equation f = j will occur at
even values of n. Fig. 9.24 illustrates.
Im v
Im TN (¯ω)
TN (¯ω) = cos v
0
2π
0
Re v
Re TN (¯ω)
Figure 9.24: Preimages of TN (¯ω) = ±j/ε (qualitatively, the scales
of the real and imaginary axes in the v plane are not equal).
From (9.46) we obtain
u = jβ + π
1
2 + n
N
where there are 2N essentially diﬀerent preimages of ¯ω occuring at 2N consecu-
tive values of n all lying on the line Im u = β. Going back to the representation
domain we obtain ¯ω lying on the respective ellipse:
(cid:18)
¯ω = cos
jβ + π
(cid:19)
1
2 + n
N
Fig. 9.25 illustrates.
Switching to ω = λ¯ω we have:
(cid:18)
ω = λ cos
jβ + π
(cid:19)
1
2 + n
N
(9.47)
(9.48)
Note that formally allowing n to take real values and letting n = −1/2 we
obtain u = jβ and ω = λ cos(jβ) = λ cosh β which is a real positive value.
Since the imaginary part of the cosine’s argument is negative, the values of
ω are moving counterclockwise for increasing n, starting from the value on
the positive real semiaxis occuring at n = −1/2. That is, the values of ω
are moving counterclockwise starting from the positive real semiaxis. As we
already found out, the values occurring at even/odd n correspond to even/odd
poles respectively, and thus the even and odd poles are interleaved on the ellipse.
Switching from ω to s = jω we obtain the expression for the poles:
(cid:18)
s = jλ cos
jβ + π
(cid:19)
1
2 + n
N
=
9.7. CHEBYSHEV TYPE I FILTERS
341
Im u
0
2π
Re u
Im ¯ω
0
¯ω = cos u
Re ¯ω
Figure 9.25: Transformation of Fig. 9.24 by u = v/N (for N = 2).
The white and black dots on the ellipse are even/odd Chebyshev
poles in terms of ¯ω. (The picture is qualitative, as the scales of the
real and imaginary axes in the u plane are not equal.)
= λ sinh β sin π
1
2 + n
N
+ jλ cosh β cos π
1
2 + n
N
(9.49)
Since the values of ω are moving counterclockwise starting from the real positive
semiaxis, the values of s are moving counterclockwise starting from the imagi-
nary “positive” semiaxis, which means that starting at n = 0 we ﬁrst obtain the
stable poles at n = 0, . . . , N − 1. The next N values of n will give the unstable
poles.
Note that since ( 1
2 + n)/N never takes integer values, the real part of s is
never zero and there are no poles on the imaginary axis. The poles are also
symmetric relatively to the real and imaginary axes and we can discard the half
of the poles located in the right complex semiplane in the same way how we did
it with the Butterworth ﬁlter. Figs. 9.26 and 9.27 illustrate.8
In Figs. 9.26 and 9.27 one could notice that the poles are condensed close
to the imaginary axis while the Butterworth poles were evenly spacing. This is
easily explained in terms of (9.29a), which gives
tan arg cos(u + jv) = −
sinh v sin u
cosh v cos u
= − tan u · tanh v
Now, if tanh v had been equal to 1, we would have had arg cos(u + jv) = −u,
which would have resulted in an even angular distribution of poles. However,
since | tanh v| < 1, the poles are located closer to the real axis in the ω plane or
closer to the imaginary axis in the s plane.
Gain adjustments
Having obtained the poles and keeping in mind that there are no zeros, we can
obtain the transfer function in the form
H(s) = g ·
(cid:89) 1
s − pn
8It might seem that the imaginary semiaxes of the ellipses in Figs. 9.26 and 9.27 are of
unit length. This is not exactly so, although they are very close, being equal to approximately
1.00003 and 1.00004 respectively.
342
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Im s
(Re ω)
−1
j
0
−j
1
Re s
(−Im ω)
Figure 9.26: Chebyshev type I ﬁlter’s even (white) and odd (black)
poles for N = 6 (including the poles of H(−s)).
Im s
(Re ω)
−1
j
0
−j
1
Re s
(−Im ω)
Figure 9.27: Chebyshev type I ﬁlter’s even (white) and odd (black)
poles for N = 5 (including the poles of H(−s)).
where the gain g could be obtained by evaluating the above product at ω = 0
and comparing to H(0). It could be a bit more practical though, to write H(s)
as a product of 1-pole lowpasses with unity passband gains:
H(s) = g ·
(cid:89)
1
s/(−pn) + 1
(9.50)
9.7. CHEBYSHEV TYPE I FILTERS
343
where −pn are the (possibly complex) cutoﬀs and where the coeﬃcient g is
diﬀerent than in the previous formula. For (9.50) we are having H(0) = g and
thus, using (9.18), we can obtain g from
H(0) =
=
1
(cid:112)1 +
T 2
ˆ

1

1 + ε2
1

=
1
(cid:112)1 + ε2T 2
N (0)
=
1
1 + ε2 (Re jN )2 =
N (0)
for N even
for N odd
(9.51)
Another option is to obtain the leading gain g from the requirement |H(j)| =
√
2 arising from
1/
|H(j)|2 =
1 +
1
T 2
N (1)
ˆ
=
1
2
However this might accidentally result in a 180◦ phase response at ω = 0, (since
we used |H(j)| rather than H(j) as a reference) therefore one needs to be careful
in this regard.
Using the default normalization of the Chebyshev ﬁlter’s gain given by (9.51),
1 + ε2, 1] on the range
we obtain the amplitude response varying within [1/
ω ∈ [−λ, λ]. We could choose some other normalizations, though. E.g. we could
require |H(0)| = 1, which will be automatically achieved if we simply let g = 1
in (9.50). Or we could require the ripples to be symmetric relatively to the zero
decibel level, which is achieved by multiplying (9.51) by (1 + ε2)1/4:
√
H(0) =
(cid:115) √
1 + ε2
1 + ε2T 2
N (0)
so that |H(jω)| varies within [1/(1 + ε2)1/4, (1 + ε2)1/4] within the equiripple
band.
Butterworth limit
Since at λ → 0 we have
ﬁlter turns into a Butterworth ﬁlter of the same order N .
T N (x) → xN , in the limit λ → 0 Chebyshev type I
ˆ
Simultaneously the ellipse semiaxes λ sinh β and λ cosh β in (9.49) are both
Indeed, letting ε → 0 (which is equivalent to
approaching the unity length.
λ → 0), we have
1
λ
= cosh
= exp
cosh β = cosh
(cid:19)
(cid:18) 1
N
ln (cid:0)ε−1 +
cosh−1 1
ε
√
ε−2 − 1(cid:1)
∼ exp
(cid:18)
(cid:19)
N
sinh−1 1
ε
√
ε−2 + 1(cid:1)
−
1
N
ln (cid:0)ε−1 +
∼ exp
(cid:18) 1
N
cosh−1 1
ε
(cid:19)
=
ε−1 +
(cid:18) 1
N
sinh−1 1
ε
(cid:19)
=
= exp
N
(cid:16)
=
ε−1 +
(cid:112)
ε−2 + 1
(cid:17)1/N
∼ (cid:0)2ε−1(cid:1)1/N
(cid:16)
=
(cid:112)
ε−2 − 1
(cid:17)1/N
∼ (cid:0)2ε−1(cid:1)1/N
Thus 1/λ and cosh β are asymptotically identical and therefore λ cosh β → 1.
In a similar way we can show that λ sinh β → 1.
344
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
9.8 Chebyshev type II filters
Chebyshev polynomials TN (x) have equiripple behavior on [−1, 1] and grow to
inﬁnity outside of that range. By reciprocating the argument: TN (1/x), we
obtain equiripple behavior on |x| ≥ 1 and inﬁnite growth for x → 0. By further
reciprocating the value of the polynomial: 1/TN (1/x), we have again small
values on the range [−1, 1], while for |x| ≥ 1 the polynomial’s value exhibits
equiripple oscillations around inﬁnity. We therefore introduce the function
TN (x) =
1
TN (1/x)
where Fig. 9.28 illustrates the behavior of TN . We will refer to TN (x) as a
double-reciprocated (once in argument and once in value) Chebyshev polynomial.
T6(x)
1
0
−1
1
-1
x
Figure 9.28: Double-reciprocated Chebyshev polynomial TN .
The equiripple oscillations around inﬁnity are also better visible in the arc-
tangent scale (Fig. 9.29). More speciﬁcally, on [1, +∞) and (−∞, −1] the value
oscillates between ±1 and ∞, never becoming less than 1 in the absolute mag-
nitude. We could refer to that fact by saying that the amplitude of these os-
cillations around ∞ is unity, thereby taking the minimum absolute magnitude
of the oscillating value as the oscillation amplitude, even though that might
be considered some kind of a misnomer. We will be using this deﬁnition of
amplitude of oscillations around inﬁnity further in the text.
We also introduce the renormalized version of the double-reciprocated Cheby-
shev polynomials by double-reciprocating
TN (x, λ) =
ˆ
1
T N (1/x, λ)
ˆ
=
T N :
ˆ
TN (1/λ)
TN (1/λx)
=
TN (λx)
TN (λ)
(9.52)
where we have
The idea is that 0 ≤ λ ≤ 1 and that
TN (1, λ) = 1. Notice that we didn’t reciprocate the λ parameter.
ˆ
TN (x, 0) =
ˆ
1
T N (1/x, 0)
ˆ
=
1
1/xN = xN
9.8. CHEBYSHEV TYPE II FILTERS
345
TN (x)
−1
∞
1
0
−1
1
∞
x
∞
−1
Figure 9.29: Double-reciprocated Chebyshev polynomials of even
(solid) and odd (dashed) orders, using arctangent scale in both
axes.
TN (x, 1) =
ˆ
1
T N (1/x, 1)
ˆ
=
1
1/TN (1/x)
= TN (x)
Fig. 9.30 illustrates. As usual, we will often omit the λ parameter, understanding
it implicitly.
T6(x, λ)
ˆ
−1
∞
1
0
−1
1
∞
x
∞
−1
Figure 9.30: Renormalized double-reciprocated Chebyshev poly-
T6(x, λ) for λ = 1 (solid), λ = 0.93 (dashed) and λ = 0
nomial
ˆ
(thin dashed).
By (9.52) the amplitude of the equiripples of
TN (x) is TN (1/λ). By using
ˆ
the same equation (9.44) as we have been using for Chebyshev type II ﬁlters we
have TN (1/λ) = 1/ε, that is the equiripple amplitude is 1/ε. This is actually
a convenient notation, since in this case we are having smaller (closer to ∞)
346
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
equiripples at smaller ε. We also thereby have:
TN (x) =
ˆ
1
εTN (1/λx)
=
TN (λx)
ε
By writing the Chebyshev polynomial as a polynomial:
TN (x) =
N
(cid:88)
n=0
anxn
we ﬁnd that the double-reciprocated Chebyshev polynomial is a rational func-
tion of x:
TN (x) =
1
N
(cid:88)
anx−n
=
N
(cid:88)
xN
anxN −n
The same apparently is true for
as f (ω) in (9.18).
n=0
n=0
TN (x) and therefore we could try using
ˆ
TN (ω)
ˆ
Letting f (ω) =
TN (ω) in (9.18) we obtain a Chebyshev type II ﬁlter, this
ˆ
time trading the ripples in the stopband against the transition band’s rolloﬀ
TN (ω) = 1/ε, thus the ripple
(Fig. 9.31). The stopband peaks are achieved at
ˆ
amplitude is 1/
1 + ε−2.
√
|H(jω)|
1
0.5
0
1/8
1
8
ω
Figure 9.31: Chebyshev type II ﬁlter’s amplitude responses for N =
6 and ε = 0.5 (solid), ε = 0.1 (dashed) and ε = 0 (Butterworth,
thin dashed). Notice the usage of the linear amplitude scale, which
is chosen in order to be able to show the amplitude response zeros.
TN (x) as representations of linear scaling
In order to ﬁnd the poles of a Chebyshev type II ﬁlter we are going, as usual, to
interpret the function TN (x) as a representation of the linear scaling v = N u.
This time we need the following mapping:
x =
1
cos u
= sec u
9.8. CHEBYSHEV TYPE II FILTERS
347
v = N u
1
cos v
TN (x) =
= sec v
Same as with TN (x), the multiplication by N expands the principal real pe-
riod [0, 2π] to [0, 2πN ] and there are similar transformations of the preimages
of the real axis. The transformations of quasielliptic curves (shown in Fig. 9.20)
which are representations of horizontal lines Im u = const in the preimage do-
main are also occurring in a similar fashion, each such curve being transformed
into another such curve.
Poles of Chebyshev type II filters
The pole equation is
The even/odd pole equations are respectively
1 +
T2
N (ω) = 0
ˆ
or
or, introducing ¯ω = λω
TN (ω) = ±j
ˆ
TN (λω)
ε
= ±j
TN (¯ω) = ±jε
where the “+” sign corresponds to the even poles and the “−” sign to odd poles.
Suppose ¯ω is moving in a counterclockwise direction in a quasielliptic curve
which is a representation of Im u = β (where, according to our previous discus-
sion of the properties of the x = 1/ cos u mapping, β > 0). This results in a
similar counterclockwise motion of TN (¯ω). We wish TN (¯ω) to pass through the
points ±jε going counterclockwise.
At this point we could follow similar steps as we did for Chebyshev type I
ﬁlters, using the preimage linear scaling interpretation of TN to obtain the points
¯ω where TN (¯ω) = ±jε. However we also could notice that TN (¯ω) = ±jε ⇐⇒
TN (¯ω−1) = ∓j/ε. Therefore we could reuse the results of our discussion of
Chebyshev type I ﬁlters, where we needed TN to go clockwise through ∓j/ε.
That is, we need the value of TN to move in the same trajectory going through
the same points as in the Chebyshev type I case, just in the opposite direction.
This can be achieved by ﬂipping its preimage line Im v = N β from the lower
semiplane to the upper semiplane. Therefore the values of TN passing through
∓j/ε going clockwise should occur at sinh N β = 1/ε and thus
β =
1
N
sinh−1 1
ε
The other diﬀerence to the case of Chebyshev type I ﬁlters is that the argument
of TN is ¯ω−1 rather than ¯ω. Therefore we need to replace ¯ω in (9.47) with ¯ω−1
obtaining
¯ω−1 = cos
(cid:18)
jβ + π
(cid:19)
1
2 + n
N
348
and
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
ω−1 = λ cos
(cid:18)
jβ + π
(cid:19)
1
2 + n
N
Since s = jω = j/ω−1, we have −1/s = −ω−1/j = jω−1 and thus
−s−1 = jλ cos
(cid:18)
jβ + π
(cid:19)
=
1
2 + n
N
1
2 + n
N
= λ sinh β sin π
+ jλ cosh β cos π
1
2 + n
N
(9.53)
The poles of Chebyshev type II ﬁlters are therefore negated reciprocals of the
poles of Chebyshev type I ﬁlters.9 By the interpretation of TN as linear scaling
in the preimage domain, Chebyshev type II poles should lie on quasielliptic
trajectories, such as the ones shown in Fig. 9.19 and Fig. 9.20. Notice that,
despite the negated reciprocation, the formula (9.53) still ﬁrst gives the stable
poles, due to the ﬂipped sign of β compared to (9.49).
Zeros of Chebyshev type II filters
According to our discussion in Section 9.4 of using rational f (ω), Chebyshev
type II ﬁlters also should have zeros, which in terms of ω coincide with poles of
f (ω). The zero equation is thereby
or
or, equivalently,
TN (ω, λ) = ∞
ˆ
TN (λω) = ∞
TN (1/λω) = 0
(9.54)
The solutions of (9.54) thereby obtained from the zeros zn of TN (given by
(9.38)) by
1
λω
= zn = cos
π
(cid:18)
(cid:19)
1
2 + n
N
or
or, in terms of s
ω−1 = λ cos
(cid:18)
π
(cid:19)
1
2 + n
N
−s−1 = jλ cos
(cid:18)
π
(cid:19)
1
2 + n
N
An additional consideration arises at odd N where one of the values given by
(9.38) occurs at the origin, which after the reciprocation gives the inﬁnity. This
9Alternatively one could notice that the poles of Chebyshev type II (lowpass) filters are
identical to the poles of Chebyshev type I hipass filters, since both can be obtained from the
poles of Chebyshev type I lowpass filters by the LP to HP transformation. If Chebyshev type
II lowpass poles are obtained this way, the order of their enumeration will be symmetrically
flipped relatively to the one of the prototype Chebyshev type I lowpass poles. That is, if
Chebyshev type I poles are going counterclockwise starting from the “positive” imaginary
axis, then Chebyshhev type II poles obtained by the LP to HP tranfromation will be going
clockwise from the “negative” imaginary axis (thereby stable poles will be converted to stable
poles, but the even/odd property of the poles will be switched if N is even).
9.9. JACOBIAN ELLIPTIC FUNCTIONS
349
means that there is no corresponding ﬁnite zero of H(s) and no corresponding
factor in the numerator of H(s). Respectively the order of the numerator of
H(s) becomes 1 less than the order of the denominator. This automatically
results in H(∞) = 0, that is H(s) has a zero at the inﬁnity, as required by the
reciprocation of the values given by (9.38). Fig. 9.32 provides an example.
Im s
(Re ω)
j
0
−j
−1
1
Re s
(−Im ω)
Figure 9.32: Poles (white and black dots) and zeros (white squares)
of a Chebyshev type II ﬁlter of order N = 5. Each of the zeros
is duplicated, but the duplicates are dropped together with the
unstable poles.
Filter gain
Since there are no ripples in the passband, there is little reason to attempt
any gain adjustments and the ﬁlter gain needs simply to be chosen from the
requirement H(0) = 1, thereby deﬁning the leading gain coeﬃcient g of the
cascade form (8.1).
Butterworth limit
Since
Butterworth ﬁlter.
TN (x, 0) = xN in the limit λ → 0 Chebyshev type II ﬁlter turns into
ˆ
Notice that the pole formula (9.53) is essentially the negated reciprocal of
(9.49). On the other hand, negation and/or reciprocation turn Butteworth poles
into themselves (if nonstable poles are included), therefore, since in the limit
(9.49) gives Butterworth poles, (9.53) also does the same.
9.9 Jacobian elliptic functions
The next class of equiripple ﬁlters which we would like to introduce are elliptic
ﬁlters. Chebyshev ﬁlters were based on the cosine function and required a wider
350
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
spectrum of trigonometric and hyperbolic functions for their analysis. Similarly,
elliptic ﬁlters are based on Jacobian elliptic cosine function and require other
Jacobian elliptic functions for their analysis. Since Jacobian elliptic functions
are not a part of widely spread common knowledge, on the contrary, the freely
available resources are rather scarce, we are going to introduce them and discuss
their properties relevant for this book’s material.
Additional information can be found in the reference texts listed at the end
of this chapter. The results presented here and in the rest of this chapter without
any kind of proof or justiﬁcation are either taken directly or derived from these
texts.
Elliptic integrals of the first kind
One of the most common ways to introduce Jacobian elliptic functions is as
some kind of special inverses of the elliptic integral of the ﬁrst kind, which we
therefore will brieﬂy discuss ﬁrst.
The elliptic integral of the first kind is the function notated F (ϕ, k) deﬁned
by the formula:
F (ϕ, k) =
(cid:90) ϕ
0
dθ
1 − k2 sin2 θ
(cid:112)
(9.55)
The parameter k is referred to as elliptic modulus. Normally 0 ≤ k ≤ 1. At
k = 0 we simply have F (ϕ, 0) = ϕ.
The value of F (ϕ, k) at ϕ = π/2 is often of a particular interest, which
motivates the introduction of the complete elliptic integral of the first kind :
K(k) = F (π/2, k)
Respectively we are having
K(0) = F (π/2, 0) =
π
2
(cid:90) π/2
(9.56a)
(cid:90) π/2
K(1) = F (π/2, 1) =
dθ
1 − sin2 θ
The graph of K(k) is shown in Fig. 9.33. Notice that K(k) grows with k (which
is obvious from (9.55)).
dθ
cos θ
(9.56b)
= ∞
(cid:112)
=
0
0
The elliptic modulus is sometimes expressed as k = sin α where α is referred
to as the modular angle. Given a modular angle α, often one also needs the
complementary modular angle α(cid:48) which is simply deﬁned as
α(cid:48) =
π
2
− α
Respectively there is the complementary elliptic modulus:
k(cid:48) =
(cid:112)
1 − k2
and the complementary complete elliptic integral of the ﬁrst kind:
K (cid:48)(k) = K(k(cid:48)) = F (π/2, k(cid:48))
Notice that k(cid:48) decreases as k increases and vice versa. On the other hand
K(k) is a monotonically increasing function. Therefore the ratio K (cid:48)(k)/K(k)
monotonically decreases with growing k. Fig. 9.34 illustrates, where we use the
modular angle in the abscissa scale in order to make the symmetry between K
and K (cid:48) explicitly visible.
9.9. JACOBIAN ELLIPTIC FUNCTIONS
351
K(k)
π
π/2
0
1
k
Figure 9.33: Complete elliptic integral of the ﬁrst kind K(k).
K, K (cid:48)
3π/2
π
π/2
0
π/2
α
Figure 9.34: Complete elliptic integral of the ﬁrst kind K (solid
line) and the complemetary complete elliptic integral of the ﬁrst
kind K (cid:48) (dashed line), plotted against the modular angle α.
Jacobian elliptic functions
There are 12 diﬀerent Jacobian elliptic functions but we will concentrate only
on 6 of them. The ones we introduce will bear strong similarities to certain
trigonometric and/or hyperbolic functions, becoming equal to them in the limit.
We will deﬁne Jacobian elliptic functions in terms of the so called amplitude,
which is deﬁned as a function ϕ = am(x, k), which is the inverse of the elliptic
integral of the ﬁrst kind:
F (am(x, k), k) = x
(9.57)
That is, for a given x the function ϕ = am(x, k) gives such ϕ that F (ϕ, k) = x.
Note that since in the limit k → 0 we have F (ϕ, 0) = ϕ, we are respectively
having am(x, 0) = x.
As with elliptic integral F (ϕ, k), the second argument k serves a role of the
function’s parameter, the “primary” argument of the function being x. Often
this parameter is simply omitted and understood implicitly: ϕ = am x. Even
more commonly, this is done for Jacobian elliptic functions (which are having
352
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
exactly the same arguments as the amplitude).
Now, from the six Jacobian elliptic functions that we are going to introduce,
the four of our primary interest will be:
- Jacobian elliptic “sine” sn(x, k) is deﬁned by the equation
sn(x, k) = sin ϕ
(9.58)
where ϕ = am(x, k). Or simply, sn(x, k) = sin am(x, k).
Fig. 9.35 provides example graphs of sn x, where we could also notice
that sn x is 4K-periodic. The value K is simply a short notation for the
complete elliptic integral K(k), evaluated for the same modulus k which is
used in sn(x, k). Please also note that the graphs in Fig. 9.35 are plotted
“in terms of K”, that is diﬀerent abscissa scales are used for diﬀerent
graphs in the same ﬁgure. This has been done in order to provide a better
visual comparison of diﬀerent sn(x, k) with diﬀerent periods.
Since the limit k → 0 we have ϕ = am(x, 0) = x, the elliptic sine turns
into into sn(ϕ, 0) = sin ϕ.
- Jacobian elliptic “cosine”10 cd(x, k) is deﬁned by the equation:
cd(x, k) =
cos ϕ
1 − k2 sin2 ϕ
(cid:112)
(9.59)
where ϕ = am(x, k). Fig. 9.36 illustrates, where one could observe that
cd x is 4K-periodic. Apparently cd(x, 0) = cos x.
- Jacobian elliptic “tangent”/elliptic “hyperbolic sine” sc(x, k) is deﬁned by
the equation:
sc(x, k) = tan ϕ
(9.60)
where ϕ = am(x, k). Fig. 9.37 illustrates, where one could observe that
sc x is 2K-periodic.
Apparently sc(x, 0) = tan x. However, the function sc x also bears strong
similarities to the hyperbolic sine, becoming equal to it in the limit k → 1.
In this book we will be mostly using the similarity of sc to sinh, therefore
we will typically refer to sc as elliptic “hyperbolic sine”.
- Jacobian elliptic “hyperbolic cosine” nd(x, k) is deﬁned by the equation:
nd(x, k) =
1
1 − k2 sin2 ϕ
(cid:112)
(9.61)
where ϕ = am(x, k). Fig. 9.38 illustrates, where one could observe that
nd x is 2K-periodic. This function becomes equal to cosh in the limit
k → 1.
(cid:112)
10There is yet another Jacobian elliptic function, which is simply equal to cos ϕ rather than
1 − k2 sin2 ϕ. Depending on the purpose either of these functions may be referred
(cos ϕ)/
to as elliptic cosine. Each of these two functions inherits different properties of cos ϕ. For the
purposes of this book we will need the one defined by (9.59) and this is the one to which we
will refer to as elliptic cosine.
9.9. JACOBIAN ELLIPTIC FUNCTIONS
353
We will also introduce two “auxiliary” functions:
- Jacobian elliptic “cosecant” ns x = 1/ sn x (Fig. 9.39)
- Jacobian elliptic “secant” dc x = 1/ cd x (Fig. 9.40).
Since these two are simply reciprocals of sn and cd, we won’t be discussing them
much, however they will be used occasionally.
From the previous discussion we could conclude that 4K is the common
period of all six introduced elliptic functions (where the elliptic “trigonomet-
ric” functions are 4K-periodic and the elliptic “hyperbolic” functions are 2K-
periodic). For that reason K = K(k) is referred to as the quarter-period.
−3K
sn x
1
0
−2K
−K
K
2K
-1
3K
x
Figure 9.35: Jacobian elliptic sine for k = 0.8 (dashed) and k =
0.999 (solid).
−3K
−2K
−K
cd x
1
0
-1
K
2K
3K
x
Figure 9.36: Jacobian elliptic cosine for k = 0.8 (dashed) and
k = 0.999 (solid).
−3K
−2K
−K
sc x
2
1
0
K
2K
3K
x
Figure 9.37: Jacobian elliptic “hyperbolic sine” (or “trigonometric
tangent”) for k = 0.5 (dashed) and k = 0.94 (solid).
354
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
nd x
2
1
0
K
2K
3K
x
−3K
−2K
−K
Figure 9.38: Jacobian elliptic “hyperbolic cosine” k = 0.8 (dashed)
and k = 0.94 (solid). The maxima are at 1/k(cid:48).
−2K
−K
−3K
ns x
2
1
0
K
−1
−2
2K
3K
x
Figure 9.39: Jacobian elliptic cosecant k = 0.8 (dashed) and k =
0.999 (solid).
−3K
−2K
−K
dc x
2
1
0
−1
−2
K
2K
3K
x
Figure 9.40: Jacobian elliptic secant k = 0.8 (dashed) and k =
0.999 (solid).
Complex argument
Jacobian elliptic functions can be generalized to complex argument values. Re-
markably, all of the six introduced functions can be obtained from each other
by shifts and/or rotations of the argument in the complex plane (with some
possible scaling of the resulting function’s value).
Before discussing any Jacobian elliptic function on the complex plane we
need to introduce the imaginary quarter period K (cid:48) which is simply equal to
the complementary complete elliptic integral: K (cid:48) = K (cid:48)(k) = K(k(cid:48)). Jacobian
elliptic functions are also periodic in the imaginary direction, where the elliptic
‘trigonometric” functions are 2jK (cid:48)-periodic and the elliptic ‘hyperbolic” func-
tions are 4jK (cid:48)-periodic, e.g. cd(x, k) = cd(x + 2jK (cid:48), k).
The real and imaginary quarter periods create a virtual grid on the complex
9.9. JACOBIAN ELLIPTIC FUNCTIONS
355
plane (Fig. 9.41). We will be particularly interested in the values that Jacobian
elliptic functions are taking along the lines of this grid.
j Im x
2jK (cid:48)
jK (cid:48)
−3K
−2K
−K
0
K
2K
3K
Re x
−jK (cid:48)
−2jK (cid:48)
Figure 9.41: Quarter-period grid.
Let’s start with cd x. It turns out that the values of cd x on this grid are
always equal to the (possibly scaled by some real or imaginary coeﬃcient) values
of one of the six introduced Jacobian functions evaluated for the real or the
imaginary part of cd x. Fig. 9.42 illustrates.
cd u
0
v
1
0
−1
0
1
√
−j/
k
IV
√
k
1/
I
√
j/
k
II
√
k
−1/
III
√
−j/
k
IV
√
k
1/
k−1dc u
∞
1/k
∞
−1/k
∞
1/k
√
j/
k
I
√
k
1/
IV
√
−j/
k
III
√
k
−1/
II
√
j/
k
I
√
k
1/
cd u
0
1
0
−1
0
1
u
√
−j/
k
IV
√
k
1/
I
√
j/
k
II
√
k
−1/
III
√
−j/
k
IV
√
k
1/
k−1dc u
∞
1/k
∞
−1/k
∞
1/k
v
(cid:48)
c
s
j
v
(cid:48)
d
n
v
(cid:48)
c
s
j
−
v
(cid:48)
d
n
−
v
(cid:48)
c
s
j
v
(cid:48)
d
n
Figure 9.42: Values of cd x = cd(u+jv) on the quarter-period grid.
356
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
The expressions at the ends of the quarter-grid lines in Fig. 9.42 are what
cd(x, k) is equal to on each of these lines, where the notation is u = Re x, v =
Im x. E.g. for x = u + jK (cid:48) the function’s value is cd x = cd(u + jK (cid:48)) = k−1 dc u.
For x = K the function’s value is cd x = cd(K + jv) = −j sc(cid:48) v = −j sc(v, k(cid:48)),
that is the primed notation denotes the usage of the complementary elliptic
modulus. Apparently, the complementary modulus needs to be used for all grid
lines parallel to the imaginary axis, since the quarter period in that direction is
K (cid:48).
The roman numerals in the middle of the grid cells denote the complex
quadrant to which the values of cd x belong for x inside the respective grid
cell. The quadrants are numbered starting from the positive real semiaxis in
the counterclockwise direction. Fig. 9.42 also shows the function values at the
intersections of the grid lines. Additionally the values exactly in the middle
between the horizontal grid lines are shown. E.g. cd(jK (cid:48)/2) = 1/
k. The
readers are encouraged to compare the values listed in Fig. 9.42 to the graphs
in Figs. 9.35 through 9.40.
√
Similarly to how the trigonometric sine is obtained from the trigonometric
cosine by a shift by the quarter-period π/2 (which also holds for complex ar-
guments), the Jacobian sine is obtained from the Jacobian cosine by a shift by
the quarter period K: sn x = cd(x − K). Respectively, the content of Fig. 9.42
becomes shifted by K resulting in the picture in Fig. 9.43.
sn u
−1
v
0
1
0
−1
0
√
−1/
k
III
√
−j/
k
IV
√
k
1/
I
√
j/
k
II
√
k
−1/
III
√
−j/
k
k−1ns u
−1/k
∞
1/k
∞
−1/k
∞
√
−1/
k
II
√
j/
k
I
√
k
1/
IV
√
−j/
k
III
√
k
−1/
II
√
j/
k
sn u
−1
0
1
0
−1
0
u
√
−1/
k
III
√
−j/
k
IV
√
k
1/
I
√
j/
k
II
√
k
−1/
III
√
−j/
k
k−1ns u
−1/k
∞
1/k
∞
−1/k
∞
v
(cid:48)
d
n
−
v
(cid:48)
c
s
j
v
(cid:48)
d
n
v
(cid:48)
c
s
j
−
v
(cid:48)
d
n
−
v
(cid:48)
c
s
j
Figure 9.43: Values of sn x = sn(u+jv) on the quarter-period grid.
From Fig. 9.42 one could notice that cd(jv, k) = nd(v, k(cid:48)).
It turns out
that this equality holds not only for real v but for any complex v. That is,
Jacobian hyperbolic cosine can be obtained from Jacobian cosine by rotation of
9.9. JACOBIAN ELLIPTIC FUNCTIONS
357
the complex plane by 90◦ and swapping of k and k(cid:48) (which eﬀectively swaps K
and K (cid:48)).11 Fig. 9.44 illustrates.
√
k(cid:48)
1/
I
√
j/
k(cid:48)
II
√
−1/
k(cid:48)
III
√
−j/
k(cid:48)
IV
√
1/
k(cid:48)
I
√
j/
k(cid:48)
nd u
1/k(cid:48)
−jsc u
∞
−nd u
−1/k(cid:48)
jsc u
∞
nd u
1/k(cid:48)
−jsc u
∞
v
(cid:48)
c
d
1
−
k
v
1
0
−1
0
1
0
v
(cid:48)
d
c
√
k(cid:48)
1/
IV
√
−j/
k(cid:48)
III
√
−1/
k(cid:48)
II
√
j/
k(cid:48)
I
√
k(cid:48)
1/
IV
√
−j/
k(cid:48)
1/k(cid:48)
∞
−1/k(cid:48)
∞
1/k(cid:48)
∞
v
(cid:48)
c
d
1
−
k
√
k(cid:48)
1/
I
√
j/
k(cid:48)
II
√
−1/
k(cid:48)
III
√
−j/
k(cid:48)
IV
√
1/
k(cid:48)
I
√
j/
k(cid:48)
1
0
−1
0
1
0
v
(cid:48)
d
c
u
Figure 9.44: Values of nd x = nd(u + jv) on the quarter-period
grid.
In a similar fashion, in Fig. 9.43 one could notice that sn(jv, k) = j sc(v, k(cid:48)).
This equality also holds not only for real v but for any complex v. That is,
Jacobian hyperbolic sine can be obtained from Jacobian sine by rotation of the
complex plane by 90◦ clockwise, swapping of k and k(cid:48) and dividing the result
by j (or, equivalently, multiplying by −j). Alternatively, recalling that sn is
obtained from cd by a shift by a real quarter period, we could have simply
shifted the content of Fig. 9.44 downwards by an imaginary quarter period and
divided it by j. Fig. 9.45 illustrates.
The functions dc x and ns x are easily obtainable from Figs. 9.42 and 9.43
by a shift by one imaginary period (and a multiplication by k).
Properties of Jacobian elliptic functions
There are lots of analogies between trigonometric/hyperbolic and Jacobian ellip-
tic functions including similarities between their shapes, which one can see from
Figs. 9.35 through 9.40. We are going to list some of the properties of Jacobian
elliptic functions comparing them against similar properties of their trigonomet-
ric/hyperbolic counterparts, where possible. The value of the argument x will
be assumed complex, unless otherwise noted. It is highly recommended to refer
11Both cd and nd are even functions. Therefore it doesn’t matter in which direction to
rotate by 90◦.
358
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
sc u
∞
−jnd u
−j/k(cid:48)
√
k(cid:48)
−1/
III
√
−j/
k(cid:48)
IV
√
1/
k(cid:48)
I
√
j/
k(cid:48)
II
√
−1/
k(cid:48)
III
√
−j/
k(cid:48)
−sc u
∞
jnd u
j/k(cid:48)
sc u
∞
−jnd u
−j/k(cid:48)
v
(cid:48)
s
n
1
−
k
j
(cid:48)
v
0
−j
0
j
0
−j
v
(cid:48)
n
s
j
√
k(cid:48)
1/
IV
√
−j/
k(cid:48)
III
√
−1/
k(cid:48)
II
√
j/
k(cid:48)
I
√
k(cid:48)
1/
IV
√
−j/
k(cid:48)
∞
−j/k(cid:48)
√
k(cid:48)
−1/
III
√
−j/
k(cid:48)
IV
√
1/
k(cid:48)
I
√
j/
k(cid:48)
II
√
−1/
k(cid:48)
III
√
−j/
k(cid:48)
∞
j/k(cid:48)
∞
−j/k(cid:48)
v
(cid:48)
s
n
1
−
k
j
(cid:48)
0
−j
0
j
0
−j
v
(cid:48)
n
s
j
u
Figure 9.45: Values of sc x = sc(u + jv) on the quarter-period grid.
to Figs. 9.35 through 9.40 and to Figs. 9.42 through 9.45 while studying the
properties below.
- Reduction to trigonometric/hyperbolic functions at k → 0 or k → 1:
sn(x, 0) = sin x
cd(x, 0) = cos x
sc(x, 0) = tan x
nd(x, 0) ≡ 1
sc(x, 1) = sinh x
nd(x, 1) = cosh x
ns(x, 0) = csc x
dc(x, 0) = sec x
(9.62a)
(9.62b)
(9.62c)
(9.62d)
(9.62e)
(9.62f)
(9.62g)
(9.62h)
- The functions are analytic (except at their poles).
- Real function values for real argument x ∈ R:
sn x ∈ R
etc.
sin x ∈ R
- The functions commute with complex conjugation:
sn x∗ = (sn x)∗
sin x∗ = (sin x)∗
9.9. JACOBIAN ELLIPTIC FUNCTIONS
359
etc.
- Odd/even symmetries
sn(−x) = − sn x
sin(−x) = − sin x
cd(−x) = cd x
cos(−x) = cos x
sc(−x) = − sc x
sinh(−x) = − sinh x
nd(−x) = nd x
cosh(−x) = cosh x
- Imaginary argument
sn(jx, k) = j sc(x, k(cid:48))
cd(jx, k) = nd(x, k(cid:48))
sc(jx, k) = j sn(x, k(cid:48))
nd(jx, k) = cd(x, k(cid:48))
sin(jx) = j sinh x
cos(jx) = cosh x
sinh(jx) = j sin x
cosh(jx) = cos x
(9.63a)
(9.63b)
(9.63c)
(9.63d)
where we intuitively assume x ∈ R, although the properties hold for any
x ∈ C.
- Periodicity along real axis:
sn(x + 4K) = sn x
sin(x + 2π) = sin x
cd(x + 4K) = cd x
cos(x + 2π) = cos x
sc(x + 2K) = sc x
nd(x + 2K) = nd x
and along imaginary axis:
sn(x + 2jK (cid:48)) = sn x
cd(x + 2jK (cid:48)) = cd x
sc(x + 4jK (cid:48)) = sc x
nd(x + 4jK (cid:48)) = nd x
n/a
n/a
n/a
n/a
sinh(x + 2jπ) = sinh x
cosh(x + 2jπ) = cos xh
Note that the periodicity property of sn x and cd x along the imaginary
axis is the dual of the periodicity property of sc x and nd x along the real
axis, the duality arising from the imaginary argument property.
- Shift by function’s half-period12 in the real direction
sn(x ± 2K) = − sn x
sin(x ± π) = − sin x
cd(x ± 2K) = − cd x
cos(x ± π) = − cos x
sc(x ± K) = −1/k(cid:48) sc x
nd(x ± K) = 1/k(cid:48) nd x
and in the imaginary direction
sn(x ± jK (cid:48)) = 1/k sn x
n/a
n/a
n/a
(9.65a)
(9.65b)
(9.65c)
(9.65d)
(9.65e)
12Note that here (and further where we specifically refer to function’s period, or half- or
quarter-period) we mean the period of the function itself, rather than one of the least common
periods 4K and 4K(cid:48) of all Jacobian elliptic functions.
360
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
cd(x ± jK (cid:48)) = 1/k cd x
sc(x ± 2jK (cid:48)) = − sc x
nd(x ± 2jK (cid:48)) = − nd x
n/a
sinh(x ± jπ) = − sinh x
cosh(x ± jπ) = − cosh x
- Shift by function’s quarter-period
sn(x + K) = cd x
sin(x + π/2) = cos x
cd(x − K) = sn x
sc(x + jK (cid:48)) = j nd x
nd(x + jK (cid:48)) = j sc x
cos(x − π/2) = sin x
sinh(x + jπ/2) = j cosh x
cosh(x + jπ/2) = j sinh x
(9.65f)
(9.65g)
(9.65h)
(9.66a)
(9.66b)
(9.66c)
(9.66d)
- Symmetry around function’s quarter-period point (this follows from the
odd/even symmetries and the shift by function’s half-period property) in
the real direction:
sn(2K − x) = sn(x)
sin(π − x) = sin x
cd(2K − x) = − cd(x)
cos(π − x) = − cos x
sc x sc(K − x) = 1/k(cid:48)
nd x nd(K − x) = 1/k(cid:48)
and in the imaginary direction:
sn x sn(jK (cid:48) − x) = −1/k
cd x cd(jK (cid:48) − x) = 1/k
sc(2jK (cid:48) − x) = sc x
nd(2jK (cid:48) − x) = − nd x
- Pythagorean theorem
n/a
n/a
n/a
n/a
sinh(jπ − x) = sinh x
cosh(jπ − x) = − cosh x
(9.67a)
(9.67b)
(9.67c)
(9.67d)
(9.67e)
(9.67f)
(9.67g)
(9.67h)
sn2 x + cd2 x = 1 + k2 sn2 x cd2 x
sin2 x + cos2 x = 1
(9.68)
(we won’t need the respective properties for the “hyperbolic” functions).
There is another useful Pythagorean-like identity:
k2 cd2 x + k(cid:48)2 nd2 x = 1
- Sum of arguments
cd(x + y, k) =
cd x cd y − sn x sn y
1 − k2 sn x sn y cd x cd y
cos(x + y) = cos x cos y − sin x sin y
(we won’t need the respective properties for the other functions).
- Complex argument
cd(u + jv, k) =
cd u nd(cid:48) v − j sn u sc(cid:48) v
1 − jk2 sn u sc(cid:48) v cd u nd(cid:48) v
cos(u + jv) = cos u cosh v − j sin u sinh v
(9.69)
(9.70)
(9.71)
where nd(cid:48) v = nd(v, k(cid:48)), sc(cid:48) v = sc(v, k(cid:48)). This is a direct corollary of
(9.70). One can use (9.71) to show that the values of cd on a single grid
cell in Fig. 9.42 belong to one and the same complex quadrant.
9.9. JACOBIAN ELLIPTIC FUNCTIONS
361
- Logarithmic derivative
ln cd x = −k(cid:48)2 sc x nd x
(cid:18)
d
dx
d2
dx2 ln cd x = k
k cd2 x −
1
k cd2 x
(cid:19)
ln cos x = − tan x
d
dx
d2
dx2 ln cos x = −
1
cos2 x
(9.72a)
(9.72b)
Periodicity
As we already mentioned, Jacobian elliptic functions are periodic in real and
imaginary direction. E.g. cd x is 4K- and 2jK (cid:48)-periodic. Thus the periods
of cd x are rectangles in the complex plane, the horizontal dimension of each
rectangle being equal to 4K and the vertical dimension being equal to 2K (cid:48).
Fig. 9.46 illustrates.
Im x
2K (cid:48)
K (cid:48)
−8K
−4K
0
4K 8K
Re x
−K (cid:48)
−2K (cid:48)
Figure 9.46: Periods of cd x in the complex plane. All dots are
preimages of one and the same value.
Due to the even symmetry of the elliptic cosine, almost every value occurs
twice on a period (as illustrated by the dots in Fig. 9.46). That is if the value
y occurs at x (that is y = cd x), then y also occurs at −x. The exceptions
√
are being cd x = ±1 and cd x = ±1/
k, which are mapped to themselves by
x ← −x if the periodicities of cd x are taken into account.
Similar considerations apply to sn x, sc x and nd x.
Preimages of the real line
By (9.71) cd x attains purely real values iﬀ x ∈ R or Re x = 2Kn where n ∈ Z,
which is also illustrated by Fig. 9.42. Similarly to cos x, we would like to choose
a principal preimage of the real line with respect to the transformation y = cd x.
Since cd x → cos x for k → 0, we would like the principal real line preimage for
cd x to approach to the respective principal preimage for cos x as k → 0. Under
this requirement there is only one choice, which is shown in Fig. 9.47.
This principal preimage of the real axis thereby consists of ﬁve parts:
x ∈ [0, 2K]
x ∈ [0, jK (cid:48)]
⇐⇒ y ∈ [−1, 1]
⇐⇒ y ∈ [1, 1/k]
362
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Im x
2K (cid:48)
K (cid:48)
−4K
−2K
0
2K
4K
Re x
−K (cid:48)
Figure 9.47: The principal preimage (solid line) of the real axis,
with respect to y = cd x, and its periodic repetitions (dashed lines).
x ∈ [2K, 2K + jK (cid:48)]
x ∈ [jK (cid:48), jK (cid:48) + K)
x ∈ (jK (cid:48) + K, jK (cid:48) + 2K] ⇐⇒ y ∈ (−∞, −1/k]
⇐⇒ y ∈ [1/k, +∞)
⇐⇒ y ∈ [−1/k, 1]
where Fig. 9.42 can serve as additional reference.
The punctured point at x = K + jK (cid:48) in Fig. 9.47 corresponds to y = ∞.
In principle it can be included into the preimage if we consider the extended
complex plane C ∪ ∞ as the codomain of cd x, in which case the preimage
consists only of four parts:
⇐⇒ y ∈ [−1, 1]
x ∈ [0, 2K]
x ∈ [0, jK (cid:48)]
x ∈ [2K, 2K + jK (cid:48)] ⇐⇒ y ∈ [−1/k, 1]
x ∈ [jK (cid:48), jK (cid:48) + 2K] ⇐⇒ y ∈ [1/k, −1/k]
⇐⇒ y ∈ [1, 1/k]
where [1/k, −1/k] = [1/k, +∞) ∪ ∞ ∪ (−∞, −1/k] denotes a range on the real
Riemann circle containing the inﬁnity in its middle.
As with cos x, the principal preimage alone doesn’t cover all preimage points
of the real line. Neither does it if we add its periodic repetitions in Fig. 9.47,
since we are convering only half of the entire length of each of the lines Re x =
πn. We can cover the remaining halves by rotating all preimages in Fig. 9.47
around the origin, which corresponds to multiplication of all points x by −1.
Notice that by adding periodic repetitions we addressed the periodicity of cd x,
while by adding the preimages multiplied by −1 we addressed the evenness
property of cd x.
9.10 Normalized Jacobian elliptic functions
In Fig. 9.42 we can observe that the “four building blocks” of the Jacobian
cosine’s values on the quarter-period grid lines are cd, k−1 dc, nd(cid:48), j sc(cid:48), − nd(cid:48)
and −j sc(cid:48). Plotting these functions in the arctangent scale we obtain the picture
in Fig. 9.48.
9.10. NORMALIZED JACOBIAN ELLIPTIC FUNCTIONS
363
y = −k−1/2
−2K (cid:48)
−K (cid:48)
−3K
−2K
−K
y = −k−1/2
y
−1
∞
1
0
−1
y = k−1/2
K (cid:48)
2K (cid:48)
K
2K
3K
x
Figure 9.48: cd(x, k), nd(x, k(cid:48)), k−1 dc(x, k) and sc(x, k(cid:48)).
The collection of the function graphs in Fig. 9.48 has obvious symmetries
with respect to the horizontal lines y = 0 and y = ∞. It is also approximately
√
symmetric with respect to y = ±1/
k (where, since k < 1, it follows that
1/
k is located above y = 1). This approximate
symmetry obviously arises from the reciprocal symmetries due to (9.67):
k > 1, so the line y = 1/
√
√
nd(K (cid:48) − x, k(cid:48)) = 1/k nd(x, k(cid:48))
sc(K (cid:48) − x, k(cid:48)) = 1/k sc(x, k(cid:48))
k−1 dc(x, k) = 1/k cd(x, k)
(9.73a)
(9.73b)
(9.73c)
(where (9.73c) is not due to (9.67) but simply follows from the deﬁnition of the
dc function: dc(x, k) = 1/ cd(x, k)).
Therefore the centers of this reciprocal symmetry are at ±1/
k. By multi-
plying all functions plotted in Fig. 9.48 by
k we will shift the centers of the
reciprocal symmetry to y = ±1. Thus consideration motivates the introduction
of normalized Jacobian elliptic functions
√
√
cd(x, k) =
sn(x, k) =
√
√
k cd(x, k)
k sn(x, k)
√
√
k(cid:48) sc(x, k)
ns(x, k) = 1/
√
sc(x, k) =
√
nd(x, k) =
k(cid:48) nd(x, k)
dc(x, k) = 1/
k · dc(x, k) = 1/ cd(x, k)
k · ns(x, k) = 1/ sn(x, k)
(note that for the “hyperbolic” functions we are using
the normalization!). Thereby (9.73) become:
√
k(cid:48) rather than
√
k for
nd(K (cid:48) − x, k(cid:48)) = 1/ nd(x, k(cid:48))
(9.74a)
364
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
sc(K (cid:48) − x, k(cid:48)) = 1/ sc(x, k(cid:48))
dc(x, k) = 1/ cd(x, k)
(9.74b)
(9.74c)
and Fig. 9.48 turns into Fig. 9.49.
y
−1
∞
1
0
−1
K (cid:48)
2K (cid:48)
K
2K
3K
x
−2K (cid:48)
−K (cid:48)
−3K
−2K
−K
Figure 9.49: cd(x, k), nd(x, k(cid:48)), dc(x, k) and sc(x, k(cid:48)).
(cid:48)
, j sc(cid:48), − nd
(cid:48)
Apparently, cd, dc, nd
and −j sc(cid:48) are the building blocks of cd x
in the same way how cd, k−1 dc, nd(cid:48), j sc(cid:48), − nd(cid:48) and −j sc(cid:48) are the building
blocks of cd x. Fig. 9.50 illustrates. Notice that we don’t need non-unity scaling
by k−1 anymore, only shifts, rotations and scaling by ±j are required to convert
between the respective functions. Fig. 9.51 provides a similar illustration for nd.
The diagrams Fig. 9.43 and 9.45 are transformed in a similar way.
For normalized elliptic functions the reciprocal symmetries of (9.67) take
the form
sc x sc(K − x) = 1
nd x nd(K − x) = 1
sn x sn(jK (cid:48) − x) = −1
cd x cd(jK (cid:48) − x) = 1
with the analogous shift properties (9.65) taking the form
sc x sc(x ± K) = −1
nd x nd(x ± K) = 1
sn x sn(x ± jK (cid:48)) = 1
cd x cd(x ± jK (cid:48)) = 1
(9.75a)
(9.75b)
(9.75c)
(9.75d)
(9.76a)
(9.76b)
(9.76c)
(9.76d)
9.10. NORMALIZED JACOBIAN ELLIPTIC FUNCTIONS
365
cd u
0
v
√
k
−j
IV
1
I
dc u
cd u
∞
j
0
√
1/
k
√
k
−j
IV
1
I
dc u
∞
v
(cid:48)
c
s
j
√
1/
k
v
(cid:48)
d
n
0
j
∞
0
j
∞
v
(cid:48)
c
s
j
−
√
−
k
0
√
k
II
−1
III
−j
IV
1
√
k
−1/
√
−
k
∞
j
0
√
k
1/
I
1
√
k
u
II
−1
III
−j
IV
1
√
k
−1/
v
(cid:48)
d
n
−
∞
v
(cid:48)
c
s
j
√
k
1/
v
(cid:48)
d
n
I
1
IV
−j
III
−1
II
Figure 9.50: Values of cd x = cd(u + jv) on the quarter-period grid
(compare to Fig. 9.42).
nd u
√
1/
k(cid:48)
−jsc u
∞
−nd u
√
−1/
k(cid:48)
jsc u
∞
nd u
√
1/
k(cid:48)
−jsc u
∞
v
(cid:48)
c
d
1
I
j
II
−1
III
−j
IV
1
I
j
v
√
k(cid:48)
0
√
−
k(cid:48)
0
√
k(cid:48)
0
v
(cid:48)
d
c
1
IV
−j
III
−1
II
j
I
1
IV
−j
√
k(cid:48)
1/
∞
√
−1/
k(cid:48)
∞
√
k(cid:48)
1/
∞
v
(cid:48)
c
d
1
I
j
II
−1
III
−j
IV
1
I
j
√
k(cid:48)
0
√
k(cid:48)
−
0
√
k(cid:48)
0
v
(cid:48)
d
c
u
Figure 9.51: Values of nd x = nd(u + jv) on the quarter-period
grid (compare to Fig. 9.44).
366
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Derivatives
In terms of normalized functions the logarithmic derivative formulas (9.72) take
the form
ln cd x = −k(cid:48) sc x nd x
(cid:18)
d
dx
d2
dx2 ln cd x = k
x −
cd
2
1
2
cd
(cid:19)
x
(9.77a)
(9.77b)
By checking the complex quadrants of the values of sc and nd in Figs. 9.44
and 9.45, one could establish that the ﬁrst logarithmic derivative is lying in the
lower complex semiplane on even imaginary periods and in the upper complex
semiplane on odd imaginary periods:
Im
Im
d
dx
d
dx
ln cd x < 0
for Im x ∈ (2n(cid:48)K (cid:48), (2n(cid:48) + 1)K (cid:48))
ln cd x > 0
for Im x ∈ ((2n(cid:48) + 1)K (cid:48), (2n(cid:48) + 2)K (cid:48))
or simply
sgn Im
d
dx
ln cd x = (−1)n(cid:48)+1
for Im x ∈ (2n(cid:48)K (cid:48), (2n(cid:48) + 1)K (cid:48))
(9.78)
where n(cid:48) is the imaginary quarter period index.
The second logarithmic derivative is apparently lying in the upper complex
semiplane if cd x is in the I or III complex quadrant and in the lower complex
semiplane if cd x is in the II or IV complex quadrant:
Im
Im
d
dx2 ln cd x > 0
d
dx2 ln cd x < 0
if cd x ∈ I or III
if cd x ∈ II or IV
or, using Fig. 9.50,
sgn Im
d
dx2 ln cd x = (−1)n+n(cid:48)+1
(9.79)
where n and n(cid:48) are respectively the real and imaginary quarter period indices.
Horizontal and vertical preimage lines of cd x
The formulas (9.77) can be used to obtain more information about the behavior
of cd x (and respectively cd x) on its quarter periods. For cos x this kind of in-
formation can be directly obtained from the complex argument formula (9.29a).
For cd x the same formula (9.71) is a bit more complicated and cannot be as
easily used for analysis.
Given u = Re x, v = Im x we obtain:
d
du
arg cd(u + jv) =
d
du
Im ln cd(u + jv) = Im
d
du
ln cd(u + jv) =
= Im
d
dx
ln cd x
(9.80a)
9.10. NORMALIZED JACOBIAN ELLIPTIC FUNCTIONS
367
d
jdv
d
dx
d
dv
ln | cd(u + jv)| =
d
dv
Re ln cd(u + jv) = Re
d
dv
= Re j
ln cd(u + jv) = Re j
ln cd(u + jv) =
d
dx
ln cd(u + jv) =
= − Im
ln cd x
(9.80b)
d2
du2 arg cd(u + jv) =
d2
du2 Im ln cd(u + jv) = Im
d2
du2 ln cd(u + jv) =
= Im
d2
dx2 ln cd x
(9.80c)
d2
dv2 arg cd(u + jv) =
d2
dv2 Im ln cd(u + jv) = Im
d2
dv2 ln cd(u + jv) =
= − Im
d2
dx2 ln cd x
(9.80d)
Suppose the point x = u+jv is moving horizontally to the right within the n(cid:48)-th
imaginary quarter period, that is ˙u > 0 and v = const ∈ (2n(cid:48)K (cid:48), (2n(cid:48) + 1)K (cid:48)).
Then we have the following.
- By (9.80a) and (9.78)
sgn
d
du
arg cd(u + jv) = sgn Im
d
dx
ln cd x = (−1)n(cid:48)+1
(9.81a)
therefore the value of cd x is moving clockwise on even imaginary quarter
periods and counterclockwise on odd imaginary quarter periods. By (9.71)
and using the complex quadrants in Fig. 9.42 or 9.50 as a reference, we
additionally ﬁnd that
arg cd(Kn + jv) = (−1)n(cid:48)+1 ·
π
2
n
(9.81b)
that is at integer multiples of K (u = Kn) the value of cd x is crossing the
real and imaginary axes, starting with the real axis at u = 0. Fig. 9.52
illustrates. The family of curves generated by such horizontal preimage
lines in shown in Fig. 9.53.
- By (9.80c) and (9.79)
sgn
d
du2 arg cd(u + jv) = sgn Im
d
dx2 ln cd x = (−1)n+n(cid:48)+1
(9.81c)
Comparing (9.81c) to (9.81a) and (9.81b) we ﬁnd that, given ˙u = const,
the trajectories in Fig. 9.53 are speeding up when going away from the
real axis and slowing down when going towards the real axis.
Now suppose the point x = u + jv is moving in a vertical line towards the
top:
˙v > 0, u = const ∈ (2nK, (2n + 1)K).
- By (9.80b) and taking into account (9.78)
sgn
d
dv
| cd(u + jv)| = − sgn Im
d
dx
ln cd x = (−1)n(cid:48)
(9.82a)
368
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
v
2K (cid:48)
K (cid:48)
Im y
y = cd(u + jv)
−8K
−4K
0
4K 8K
0
u
Re y
−K (cid:48)
−2K (cid:48)
Figure 9.52: A quasielliptic trajectory and its preimages. The
picture is qualitative. Particularly, the principal preimage line,
shown by the solid arrow line, is actually closer to the real axis (it
must be closer that K (cid:48)/2).
Im y
y = cd(u + jv)
−1
j
0
−j
1
Re y
Figure 9.53: A family of quasielliptic trajectories generated from
horizontal preimages v = const ∈ [−K (cid:48), 0]. The unit circle trajec-
tory occurs at v = −K (cid:48)/2.
where n(cid:48) is the imaginary quarter period index corresponding to the cur-
rent value of v. That is | cd x| will be increasing on even imaginary quarter
periods and decreasing on odd imaginary quarter periods.
In fact, the movement trajectories will be as shown in Fig. 9.54, where the
movement around y = 1 will be occurring on even real quarter-periods
and the movement around y = −1 will be occurring on odd real quarter-
periods. At the even boundaries u = 2nK the movement will be oscillating
along the real line between (−1)n
k. At the odd bound-
k and (−1)n/
√
√
9.10. NORMALIZED JACOBIAN ELLIPTIC FUNCTIONS
369
aries u = (2n + 1)K the movement will be occurring along the entire
imaginary axis looping through the ∞, going downwards all the time if n
is even and going upwards all the time if n is odd (referring to Fig. 9.50 is
recommended for understanding these boundary cases). The trajectories
in Fig. 9.54 complete a full cycle over one imaginary period 2K (cid:48) of cd x.
- By (9.80d) and (9.79)
sgn
d
dv2 arg cd(u + jv) = − sgn Im
d
dx2 ln cd x = (−1)n+n(cid:48)
(9.82b)
Equation (9.82b) means that the second derivative of arg cd x doesn’t
change sign during vertical motion within a single imaginary quarter pe-
riod. This doesn’t seem much, but it will be a quite useful property.
Im y
y = cd(u + jv)
0
Re y
Figure 9.54: A family of trajectories generated from vertical preim-
ages u = const. Notice that the trajectories intersect the unit circle
(shown by the dashed line) at right angles.
Since | cd x| monotonic in the vertical direction on a single quarter period
and arg cd x is monotonic in the horizontal direction, it follows that within a
single quarter-period grid cell the function cd x is taking each value no more than
once. Respectively, the quasielliptic curves in Fig. 9.53 are all distinct (that is
they don’t intersect or overlap) within a single imaginary quarter-period of the
domain of cd x. Conversely, each imaginary quarter period of the domain of cd x
contains exactly one preimage of any given such curve, as shown in Fig. 9.52.
In a similar way one can argue the distinctness of the curves in Fig. 9.54.
Unit circle symmetries
In Figs. 9.53 and 9.54 we have speciﬁcally highlighted the unit circle, which is
related some of the properties of cd x. It turns out that cd x has some symmetries
in respect to the unit circle and its preimage.
Let’s take two points jK (cid:48)/2 + x and jK (cid:48)/2 + x∗, which are located sym-
metrically to the line Im x = jK (cid:48)/2 on the complex plane, and consider the
product
cd(jK (cid:48)/2 + x) (cid:0) cd(jK (cid:48)/2 + x∗)(cid:1)∗
= cd(jK (cid:48)/2 + x) (cid:0) cd(x − jK (cid:48)/2)∗(cid:1)∗
=
= cd(x + jK (cid:48)/2) cd(x − jK (cid:48)/2) =
370
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
= cd(x + jK (cid:48)/2) cd(cid:0)(x + jK (cid:48)2) − jK (cid:48)(cid:1) = 1
where the latest is by (9.76d). That is the corresponding values of the Jacobian
elliptic cosine are conjugate-reciprocal:
cd(jK (cid:48)/2 + x) (cid:0) cd(jK (cid:48)/2 + x∗)(cid:1)∗
= 1
(9.83)
and the line Im x = jK (cid:48)/2 is the axis of the conjugate-reciprocal symmetry of
cd x. From the evenness property of the elliptic cosine (and the fact that x in
(9.83) is arbitrary) it follows that
cd(−jK (cid:48)/2 + x) (cid:0) cd(−jK (cid:48)/2 + x∗)(cid:1)∗
= 1
that is the line Im x = −jK (cid:48)/2 is also the axis of the conjugate-reciprocal
symmetry of cd x. Since cd x is 2K (cid:48)-periodic along the imaginary axis, any
other lines of the form Im x = jK (cid:48)/2 + K (cid:48)n(cid:48) are also the axes of the conjugate-
reciprocal symmetry of cd x:
cd(jK (cid:48)/2 + jK (cid:48)n(cid:48) + x) (cid:0) cd(jK (cid:48)/2 + jK (cid:48)n(cid:48) + x∗)(cid:1)∗
= 1
(9.84)
Taking the absolute value of both sides of (9.84) we obtain
| cd(jK (cid:48)/2 + jK (cid:48)n(cid:48) + x)| · | cd(jK (cid:48)/2 + jK (cid:48)n(cid:48) + x∗)| = 1
Further, assuming a purely real x (so that x = x∗) the above turns into
(cid:12) cd(jK (cid:48)/2 + jK (cid:48)n(cid:48) + x)(cid:12)
(cid:12)
2
(cid:12)
= 1
or simply
(cid:12)
(cid:12) cd(jK (cid:48)/2 + jK (cid:48)n(cid:48) + x)(cid:12)
(cid:12) = 1
(9.85)
that is the absolute magnitude of cd x is unity on the line Im x = jK (cid:48)/2+jK (cid:48)n(cid:48),
exactly corresponding to the unit circle trajectory in Fig. 9.53. As another
illustration, in Fig. 9.50 one could notice that cd x is taking the values ±1 and
±j at the intesections of vertical grid lines with the line Im x = jK (cid:48)/2 + jK (cid:48)n(cid:48).
Since we showed that the quasielliptic trajectories in Fig. 9.53 are all distinct,
there are no other points within the imaginary quarter period where | cd x| = 1,
and respectively the lines Im x = jK (cid:48)/2 + jK (cid:48)n are the only preimages of the
unit circle.
Taking the complex argument of both parts of (9.84) we have
arg cd(jK (cid:48)/2 + jK (cid:48)n(cid:48) + x) − arg cd(jK (cid:48)/2 + jK (cid:48)n(cid:48) + x∗) = 0
or
arg cd(jK (cid:48)/2 + jK (cid:48)n(cid:48) + x) = arg cd(jK (cid:48)/2 + jK (cid:48)n(cid:48) + x∗)
(9.86)
That is the complex arguments of cd x taken at the points symmetric relatively
to the line Im x = jK (cid:48)/2 + jK (cid:48)n(cid:48) are equal.
Fig. 9.55 provides an illustration for the range 0 ≤ Im x ≤ K (cid:48). Apparently
on the ends of that range the elliptic cosine has purely real values (which can
9.10. NORMALIZED JACOBIAN ELLIPTIC FUNCTIONS
371
be seen from the properties of cd x and from Fig. 9.50), corresponding to the
complex argument being equal to 0 or π. Inside that range the value is becoming
complex, where it is “maximally complex” (in the sense of arg cd x having the
maximal deviation from 0 or π) exactly in the middle, that is at Im x = K (cid:48)/2.
This corresponds to the trajectories in Fig. 9.54 crossing the unit circle at right
angles, so that the tangent lines of the trajectories taken at the intersection
points are going through the origin, and therefore the angular deviation from
the real line is attaining a maximum at these intersection points.
arg cd(u + jv)
π
π/2
0
−π/2
−π
K (cid:48)
v
Figure 9.55: Deviation of Jacobian cosine’s value from the real axis
as a function of the imaginary part v of its argument, plotted for
various real parts u and various elliptic moduli k.
In order to explain this maximum angular deviation at Im x = K (cid:48)/2 consider
the following. The symmetry of the graphs in Fig. 9.55 is directly following from
(9.86). Therefore there must be an extremum at the point in the middle of the
range Im x ∈ [0, K (cid:48)]. By (9.82b) this is the only extremum on that range and
therefore this is the point of the maximum deviation.
Since the functions sn, sc and nd can be obtained from cd by shifts and/or
rotations of the complex plane (and a multiplication by j or by −j for sc), they
also exhibit similar symmetries. We won’t go into detail of these. The functions
dc and ns being the reciprocals of cd and sn are having similar symmetries as
well.
Normalized argument
It will be also often convenient to use the following notation:
cdK x = cd Kx
scK(cid:48) x = sc K (cid:48)x
cdK x = cd Kx
etc.
that is we write the quarter-period multiplier of the argument as a subscript of
the function’s name. In this notation e.g. the real quarter-period of cdK x be-
372
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
comes equal to 1, therefore we will refer to this notation as normalized-argument
Jacobian elliptic functions.
Note that we can’t normalize the argument simultaneously for real and imag-
inary quarter periods, that is we need to choose between e.g. cdK x and cdK(cid:48) x,
depending on our needs. Noticing that
cd(Ku + jK (cid:48)v) = cd K
(cid:18)
u + j
(cid:19)
K (cid:48)
K
v
(cid:18)
= cdK
u + j
(cid:19)
K (cid:48)
K
v
cd(Ku + jK (cid:48)v) = cd K (cid:48)
(cid:19)
(cid:18) K
K (cid:48) u + jv
= cdK(cid:48)
(cid:19)
(cid:18) K
K (cid:48) u + jv
we can see that the imaginary quarter period of cdK x is K (cid:48)/K and the real
quarter period of cdK(cid:48) x is K/K (cid:48). The same obviously holds for other Jacobian
elliptic functions.
Notice that Figs. 9.35 through 9.38 are eﬀectively plotting snK, cdK, scK
and ndK, since the argument scale is scaled by K.
In Figs. 9.35 through 9.38 one could notice that the values of snK, cdK, scK
and ndK seem to be growing (in absolute magnitude) with k. Let’s see if this
is always the case.
In the beginning we are going to establish the fact that the argument-
normalized amplitude ϕ(x, k) = amK(x, k) = am(K(k)x, k) grows with k on
x ∈ (0, 1), that is
∂ϕ
∂k
=
∂
∂k
amK(x, k) > 0
(0 < x < 1, 0 < k < 1)
(9.87)
Before analysing the partial derivative of amK(x, k) with respect to k, we need
to note the range in which amK(x, k) is varying for x ∈ (0, 1):
amK(x, k) ∈ (0, π/2)
∀x ∈ (0, 1)
(0 ≤ k < 1)
(9.88)
Indeed, by (9.55) F (ϕ, k) is strictly increasing for 0 ≤ k < 1, therefore, since
F (0, k) = 0 and F (π/2, k) = K(k), the range ϕ ∈ (0, π/2) is mapped to
F (ϕ, k) ∈ (0, K) and vice versa. Therefore am(x, k) is monotonically chang-
ing from 0 to π/2 for x changing from 0 to K, and respectively amK(x, k) is
monotonically changing from 0 to π/2 for x changing from 0 to 1.
Now, given ϕ(x, k) = amK(x, k), by (9.57)
F (ϕ, k) = K(k)x
Since we are interested in the partial derivative of ϕ with respect to k, we will
consider x to be ﬁxed and ϕ and k varying in the above equation. Then, taking
the logarithm, we have
or
ln F (ϕ, k) = ln K(k)x
ln F (ϕ, k) = ln K(k) + ln x
Let’s take a full derivative in respect to k of both sides, where, since x = const,
the respective term fully disappears:
d
dk
ln F (ϕ, k) =
d
dk
ln K(k)
9.10. NORMALIZED JACOBIAN ELLIPTIC FUNCTIONS
373
∂ ln F
∂ϕ
dϕ
dk
+
∂ ln F
∂k
=
d ln K
dk
Since x = const, we have ∂ϕ/∂k = dϕ/dk and thus
∂ ln F
∂ϕ
∂ϕ
∂k
+
∂ ln F
∂k
=
d ln K
dk
Since by (9.55)
∂ ln F
∂ϕ
=
1
F
∂F
∂ϕ
=
1
(cid:112)
F
1 − k2 sin2 ϕ
> 0
it is suﬀcient to show that
d ln K
dk
>
∂ ln F
∂k
and then ∂ϕ/∂k > 0 will automatically follow.
The previous inequality can be equivalently rewritten as
dK
dk
>
∂F
∂k
or, noticing that K(k) = F (π/2, k) and reintroducing the explicit argument
notation F = F (ϕ, k),
∂
∂k
F (π/2, k) >
∂
∂k
F (ϕ, k)
∂
∂k
(cid:0)F (π/2, k) − F (ϕ, k)(cid:1) > 0
∂
∂k
(cid:90) π/2
(cid:112)
dθ
1 − k2 sin2 θ
(cid:33)
1
1 − k2 sin2 θ
(cid:112)
ϕ
d
dk
(cid:90) π/2
(cid:32)
ϕ
> 0
dθ > 0
(cid:90) π/2
ϕ
k sin2 θ
(cid:0)1 − k2 sin2 θ(cid:1)3/2
dθ > 0
(9.89)
Obviously the integral in (9.89) is positive for any 0 < k < 1 and 0 < ϕ < π/2
and therefore (9.87) holds. It follows that
amK(x, k1) < amK(x, k2)
∀x ∈ (0, 1)
(0 ≤ k1 < k2 < 1)
(9.90)
Notice that we have allowed k1 = 0 in (9.90). Strictly speaking, at k = 0
the integral in (9.89) turns to zero, respectively ∂ϕ/∂k = 0. However it
doesn’t matter much: since ∂ϕ/∂k > 0 starting with arbirarily small k, we
have amK(x, k) > amK(x, 0) ∀x ∈ (0, 1) and respectively (9.90) also holds for
k1 = 0.
Using (9.90), (9.88) and (9.58) we obtain
snK(x, k1) < snK(x, k2)
∀x ∈ (0, 1)
(0 ≤ k1 < k2 < 1)
374
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Using shift and symmetry properties of sn we can extend the above to the entire
real axis (with the exception of purely integer points where snK x has the same
values independently of k):
| snK(x, k1)| < | snK(x, k2)|
∀x (cid:54)∈ Z (0 ≤ k1 < k2 < 1, x ∈ R)
The same property for cdK follows from the fact that cdK can be obtained
from snK by a quarter-period shift, and we have
| cdK(x, k1)| < | cdK(x, k2)|
∀x (cid:54)∈ Z (0 ≤ k1 < k2 < 1, x ∈ R)
(9.91)
The same property for scK follows from (9.90), (9.88) and (9.60). The same
property for ndK follows from (9.90), (9.88) and (9.61).
Notably, the same property doesn’t hold if the argument is not normalized
by the real period K. Indeed, it is easily noticed that F (ϕ, k) grows with both
ϕ and k, therefore, given F (ϕ, k) = const, the value of ϕ will be decreasing for
growing k, which means that
∂
∂k
am(x, k) < 0
9.11 Landen transformations
Given an elliptic modulus k and the associated quarter periods K and K (cid:48) we
could desire to ﬁnd another elliptic modulus, such that the period ratio13 K (cid:48)/K
is increased or decreased by an integer factor (compared to the original ratio
K (cid:48)/K). We will speciﬁcally focus on the transformation which changes the
period ratio by a factor of 2. It will be particularly (but not only) useful as a
means of evaluation of Jacobian elliptic functions and their inverses.
Given an elliptic modulus k0 and the corresponding period ratio K (cid:48)
0/K0,
let k1 denote the elliptic modulus such that the corresponding period ratio is
halved: K (cid:48)
It turns out that k1 can be found by a simple
formula: k1 = 2
k0/(1 + k0). We deﬁne the ascending Landen transformation:
1/K1 = K (cid:48)
√
0/2K0.
L(k) =
√
2
k
1 + k
(9.92a)
It is easily veriﬁed that L(k) > k ∀k ∈ (0, 1), which explains the name “ascend-
ing”. Intuitively, an increase of the elliptic module k increases the real period K
and reduces the imaginary period K (cid:48), therefore the ratio K (cid:48)/K is also reduced.
Inverting the ascending Landen transformation we obtain the descending
Landen transformation:
L−1(k) =
1 − k(cid:48)
1 + k(cid:48) =
(cid:18) k
(cid:19)2
1 + k(cid:48)
(9.92b)
√
1 + k2 is the corresponding complementary modulus.14 The read-
where k(cid:48) =
ers are encouraged to check that L−1(L(k)) = L(L−1(k)) = k. Obviously, the
descending Landen transformation doubles the period ratio K (cid:48)/K.
13The ratio of the periods is of course formally speaking not K(cid:48)/K but 4K(cid:48)/4K. However,
obviously these values are equal.
14The second expression in (9.92b), in comparison to the first one, reduces the computation
precision losses at small k.
9.11. LANDEN TRANSFORMATIONS
375
It is easily found that the ascending and descending transformations are dual
with respect to swapping k and k(cid:48) (or, which is the same, K and K (cid:48)):
L(cid:48)(k) = L−1(k(cid:48))
(9.93)
where L(cid:48)(k) = (cid:112)1 − (L(k))2 denotes the elliptic modulus complementary to
L(k). Another property which follows from (9.92) is
(1 + k)(1 + L(cid:48)(k)) = 2
which also can be equivalently written as
(1 + k(cid:48))(1 + L−1(k)) = 2
Landen sequences of elliptic moduli
(9.94a)
(9.94b)
Given some elliptic modulus k0, Landen transformation establishes a bilateral
sequence of elliptic moduli:
. . . < k−2 < k−1 < k0 < k1 < k2 < . . .
(9.95a)
where kn+1 = L(kn). Due to (9.93) this also automatically establishes a se-
quence of complementary moduli
. . . > k(cid:48)
−2 > k(cid:48)
−1 > k(cid:48)
0 > k(cid:48)
1 > k(cid:48)
2 > . . .
where k(cid:48)
n+1 = L−1(k(cid:48)
n). Note that by (9.94) we have
(1 + kn)(1 + k(cid:48)
(1 + k(cid:48)
n+1) = 2
n)(1 + kn−1) = 2
(9.95b)
(9.96a)
(9.96b)
At small k (9.92b) turns to
L−1(k) ≈
k2
4
(for k ≈ 0)
(9.97)
Thus, as n grows, the moduli k−n quickly decrease to zero. Conversely, kn
quickly grows to 1. E.g. starting at k = 0.999 we have a sequence
k0 = 0.999
k−1 ≈ 0.914
k−2 ≈ 0.424
k−3 ≈ 0.0494
k−4 ≈ 6 · 10−4
k−5 ≈ 1 · 10−7
At this point the “trigonometric” elliptic functions become practically equal
to their trigonometric counterparts (recall the property (9.62)), while the real
quarter period becomes practically equal to π/2. Thus we almost exactly know
the value of the real quarter period and we also can evaluate the respective
trigonometric functions instead of an elliptic ones. Using the relationships that
we are about to establish below, one can relate the elliptic function values at
k ≈ 0 to the values at larger k, which then provides a way to evaluate the elliptic
functions for arbitrary k. Obviously, the same applies to the “hyperbolic” elliptic
functions at k → 1.
376
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Ascending recursion for quarter period K
Landen transformation changes the real quarter period as
K(L(k)) = (1 + k)K(k)
(9.98)
(where K(k) is the complete elliptic integral of the ﬁrst kind).
Considering the sequence (9.95), let Kn = K(kn), K (cid:48)
n = K(k(cid:48)
n) denote the
real and imaginary quarter periods corresponding to moduli kn. By (9.98)
Kn+1 = (1 + kn)Kn
n)K (cid:48)
n−1 = (1 + k(cid:48)
K (cid:48)
n
(9.99a)
(9.99b)
One can verify that (9.99) are in agreement with the fact that the period ratio
is changed by a factor of 2:
K (cid:48)
n+1
Kn+1
=
K (cid:48)
n
(1 + k(cid:48)
n+1) · (1 + kn)Kn
=
K (cid:48)
n
2Kn
where we have used (9.99) and (9.96).
The formula (9.99a) can be used as a means to compute K(k) (for 0 < k < 1).
Notice that as kn is getting small, the factors (1 + kn) are becoming very close
to zero. Therefore
K0
K−n =
n
(cid:89)
(1 + k−ν)
ν=1
by (9.56) should converge to K−∞ = K(0) = π/2. In practical computations,
starting from some n the factor (1 + k−n) will be indistinguishable from zero
within the available computation precision, and so (by (9.97)) will be the sub-
sequent factors. At this point the computations may be stopped and we can
assume that K−n = π/2 within the computation precision. Respectively
K0 = K−n ·
n
(cid:89)
ν=1
(1 + k−ν) =
π
2
·
n
(cid:89)
(1 + k−ν)
(9.100)
ν=1
Thus we arrive at the following algorithm.
Given k0 we wish to evaluate K(k0). Use descending Landen transformation
to build a sequence of decreasing moduli k0, k−1, k−2, . . ., until at some step
n the values k−n becomes suﬃciently small so that 1 + k−n = 1 within the
available computation precision. Then ascend back to k0 using (9.99a), thereby
computing K−n+1, K−n+2, . . . , K−1, K0.
Using (9.100) the same algorithm can be expressed iteratively rather than
recursively:
// compute K from k
K := pi/2;
for i:=1 to 5 do
k’ := sqrt(1+k^2);
k := (k/(1+k’))^2; // descending Landen transformation
K := K*(1+k);
endfor;
9.11. LANDEN TRANSFORMATIONS
377
Ascending recursion15 for sn x and cd x
Let’s introduce the notation snn x = snKn (x, kn) = sn(Knx, kn), snn x =
snKn (x, kn) = sn(Knx, kn) etc. Note that thereby the imaginary period of
snn+1 is halved compared to snn. Let’s also introduce the notation for the
arithmetic average of x and its reciprocal 1/x:
A(x) =
x + 1
x
2
Then
snn+1 x =
1
(cid:112)kn+1A(snn x)
(9.101a)
For the purposes of numeric evaluation it is usually more practical to rewrite
(9.101a) in the form:
snn+1 x =
(1 + kn) snn x
1 + kn sn2
n x
(9.101b)
which particularly avoids the division by zero if snn x = 0.
Substituting x + 1 for x in (9.101) and using the shift property (9.66) we
obtain
cdn+1 x =
1
(cid:112)kn+1A (cid:0) cdn x(cid:1)
or the version for numeric evaluation:
cdn+1 x =
(1 + kn) cdn x
1 + kn cd2
n x
(9.102a)
(9.102b)
The formulas (9.101), (9.102) can be used to compute the elliptic sine and
cosine for k ∈ (0, 1) by using a similar approach to how we used (9.99) to
evaluate K(k). Let’s start with the elliptic cosine. The idea is that by (9.62)
lim
n→+∞
cd−n x = lim
n→+∞
cd(K−nx, k−n) = cos
π
2
x
Now notice that in (9.102b) we have cdn+1 x = cdn x within the available com-
putation precision, provided
1 + kn = 1
n x = 1
1 + kn cd2
(9.103a)
(9.103b)
within the same computation precision. Apparently, at this moment the se-
quence cdn x (where n → −∞) converges to cos(πx/2).
The condition (9.103a) is the same that we had in the evaluation of K(k).
However additionally we have the requirement (9.103b) which is redundant if x
is real (since then 0 ≤ cd2
n x ≤ 1), but becomes essential if Im x (cid:54)= 0.
15This technique is commonly referred to as descending Landen transformation, since it
expresses elliptic functions with higher values of the modulus k via elliptic functions with
lower values of the modulus. However the recursion formula itself is applied to compute
elliptic functions with higher k from elliptic functions with lower k, thus the recursion itself
is ascending.
378
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Since we don’t know the value of cdn x in advance, we can’t directly estimate
at which n (9.103b) begins to hold. Simply assuming that (9.103a) will suﬃce is
not the best idea, since cdn x can easily have values comparable to or exceeding
1/
kn in absolute magnitude. Suppose however that
√
| Im x| ≤
K (cid:48)
n
2Kn
(9.104)
that is the imaginary part of the argument of cdn doesn’t exceed half of the
imaginary quarter period.16 From our previous discussion of the behavior of
cd and cd we should remember that cd attains unit values in the middle of the
imaginary quarter period and that its absolute magnitude grows away from the
real axis (within the ﬁrst imaginary quarter period). That is
Respectively
and
| cd x| ≤ 1
for | Im x| ≤ K (cid:48)/2
| cd x| ≤
| cdn x| ≤
1
√
k
1
√
kn
for | Im x| ≤ K (cid:48)/2
for | Im x| ≤
K (cid:48)
n
2Kn
(9.105)
√
Thus we have established that under the condition (9.104) the values of cdn x
do not exceed 1/
kn in absolute magnitude. Apparently this is by far not
good enough for (9.103b) to hold, since we only guarantee that |kn cd2
n x| ≤ 1,
however the situation will improve if we decrease n by one or more steps.
First notice that if (9.104) holds at some n0, then it will hold ∀n ≤ n0 and
n x| ≤ 2. Under further
n x| ≤ 1 and |1 + kn cd2
so will (9.105), therefore |kn cd2
assumption of (9.103a), from (9.102b) we have
| cdn x| ≤ 2 · | cdn+1 x|
However by (9.97) we have kn = k2
n+1/4 and therefore
|kn cd2
n x| ≤
k2
n+1
4
· 4 · | cd2
n+1 x| = kn+1 · |kn+1 cd2
n+1 x|
That is kn cd2
n x will turn essentially to zero after just decreasing n by one step
and respectively the sequence cdn x (for n → −∞) will immediately converge.
In principle, we could now allow | Im x| to be arbitrarily large. As we decrease
n step by step, the imaginary period K (cid:48)
n/Kn of the function cdn is doubling each
time, thus sooner or later (9.104) will hold. However we don’t want to do un-
necessarily many iterations, not only for performance reasons, but also because
the precision losses will accumulate. Therefore it might be more straightforward
to simply wrap the argument of cd using the imaginary periodicity property.
Thus we arrive at the following algorithm. Suppose we want to evaluate
cd(x, k). If | Im x| > K (cid:48), we should use the periodicity property to get x into
the range | Im x| ≤ K (cid:48). Then introduce u = x/K and k0 = k, so that we
16Notice that we needed to divide by Kn in (9.104) because the notation cdn includes the
automatic multiplication of the argument by Kn.
9.11. LANDEN TRANSFORMATIONS
379
have cd(x, k) = cd0 u. Then we use the descending Landen transformation to
decrease k−n to almost zero,17 where
cd−n u = cd K−nu ≈ cd
π
2
u ≈ cos
π
2
u
At this point we compute cos(πu/2) instead of cd−n u and ascend back using
(9.102b). In pseudocode this could be expressed as:
// compute cd(x/K,k), assuming |Im x|<=K’
function cdK(u,k,steps=5)
if steps=0 then return cos(pi/2*u) endif;
k’ := sqrt(1+k^2);
k := (k/(1+k’))^2; // descending Landen transformation
y := cdK(u,k,steps-1);
return ((1+k)*y)/(1+k*y^2);
endfunction;
Notice that u may be complex in the above, where we would need a cosine
routine supporting a complex argument, which, if missing, could be implemented
by (9.29a).
Evaluation of sn x is done in the same way, except that we have to compute
sin(πu/2) as the approximation of sn−n u. The evaluation routines for sn and
cd can be also reused for evaluation of sc and nd using (9.63).
Descending recursion for sn x and cd x
We could invert the formulas (9.101) and (9.102) to express snn−1 and cdn−1 in
terms of respectively snn or cdn:
snn−1 x = A−1
(cid:18)
(cid:19)
√
1
kn snn x
or its “numerical” version, avoiding the divisions by zero for snn x = 0
snn−1 x =
1
1 + kn−1
·
2 snn x
1 ± (cid:112)1 − k2
n sn2
n x
and
cdn−1 x = A−1
(cid:18)
(cid:19)
√
1
kn cdn x
cdn−1 x =
1
1 + kn−1
·
1 ±
(cid:113)
2 cdn x
1 − k2
n cd2
n x
(9.106a)
(9.106b)
(9.107a)
(9.107b)
The ambiguity in formulas (9.106) and (9.107)18 is apparently due to the fact
that the imaginary periods of snn−1 and cdn−1 are doubled compared to snn
and snn, therefore the formulas “do not know which of the two imaginary half-
periods to choose”.
17Note that thereby after one iteration we are guaratneed that | Im u| ≤ K(cid:48)
18Note that the inverse of A gives two different values.
n/2Kn.
380
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
The descending recursion can be used to evaluate the inverses of sn and
cd. Given an equation of the form cd(x, k) = y (where we want to ﬁnd x =
cd−1(y, k)), we introduce u = x/K, k0 = k and y0 = y, so that y0 = cd0 u.
Then we use (9.107) to descend to k−n ≈ 0, where we have
y−n = cd−n u ≈ cos
π
2
u
with high precision and therefore we can simply ﬁnd u by u = (2/π) cos−1 y−n,
thereby obtaining cd−1(y, k) = x = K0u.
Note that, even though (9.107) gives ambiguous results, any of those results
will give a correct answer in the sense that we will get one of the possible
solutions of cd(x, k) = y at the end of the recursion procedure. However, in order
to avoid getting too far away from the origin (and in order to keep u within
the real numbers range if x is a real number not exceeding 1 in magnitude),
it is recommended to choose the value with the smaller absolute magnitude
from the two values of A−1 in (9.107a), or, equivalently choose the “+” sign in
the denominator of (9.107b). In case of complex values “choosing the + sign”
also means that the complex square root operation should yield a value with a
nonnegative real part, that is we should use the principal value (9.30).
Computing the inverse of sn is done in the same way using (9.106) (where
it is preferable to choose the smaller-magnitude one from the two values of A−1
in (9.106a) and to use the “+” sign in the denominator of (9.106b)), ﬁnally
computing u by u = (2/π) sin−1 y−n, thereby obtaining sn−1(y, k) = x = K0u.
The respective pseudocode routine could be e.g.:
// compute x=sn^-1(y,k)
Kbypi2:=1; // accumulate ratio K/(pi/2)
for i:=1 to 5 do
k’ := sqrt(1+k^2);
k_1 := (k/(1+k’))^2; // descending Landen transformation
y := 2/(1+k_1) * y/(1+sqrt(1-k^2*y^2));
k := k_1; Kbypi2 := Kbypi2*(1+k);
endfor;
x := Kbypi2*arcsin(y);
The routine for cd−1 is identical, except that it should use arccos instead of
arcsin. Alternatively notice that cd−1 and sn−1 are related via shift and sym-
metry properties of cd and sn,, e.g. cd−1 x = K − sn−1 x, so that one function
can be expressed in terms of the other. The functions sc−1 and nd−1 can be
expressed via sn−1 and cd−1 using (9.63).
If the argument of sn−1 and cd−1 is restricted to real values, all respective
computations will be real. Otherwise we need sqrt, arcsin and arccos functions
to support complex argument, where sqrt must return the principal value (with
nonnegative real part). These functions, if missing, can be implemented using
(9.30) and (9.32).
We mentioned that the ambiguity of (9.106) and (9.107) is due to the dou-
bling of the imaginary period of sn and cd on each step. Instead of that, we
could have had the imaginary period ﬁxed and the real period halved on each
step, resulting in the same change of the period ratio. E.g. for the elliptic cosine,
9.11. LANDEN TRANSFORMATIONS
introducing cdn(cid:48) = cd(K (cid:48)
nx, kn), we have another relationship:
cdn−1(cid:48) x =
n) cd2
(1 + k(cid:48)
1 − (1 − k(cid:48)
n(cid:48) x − 1
n) cd2
n(cid:48) x
381
(9.108)
(where cdn−1(cid:48) x = cd(K (cid:48)
n−1x, kn−1)).
Unfortunately, while (9.108) avoids the ambiguity of (9.106) and (9.107),
it is not useful for evaluation of the inverses of sn and cd, as there is another
ambiguity popping up. Due to periodicity and symmetries of cos x along the
real axis, we won’t know which of the possible values of the inverse of cos x to
take. When using (9.106) and (9.107) the real period of sn and cd was always
exactly preserved by the transformation, therefore this ambiguity didn’t matter
as any of the values of cos−1 and sin−1 would do. If however the real period is
not kept intact, the value returned by cos−1 might result in a wrong value after
rescaling back to the original periods K0 and K (cid:48)
0.
One further issue is related to the preservation of the imaginary period. Par-
ticularly the range y−n ∈ [1, 1/k−n] is mapped to the range y−n−1 ∈ [1, 1/k−n−1],
respectively for a real y−n above that range (that is y−n > 1/k−n) we obtain a
real y−n−1 > 1/k−n−1. Respectively cos−1 will return a purely imaginary result
(while what we expect from cd−1 is clearly not purely imaginary, as one can see
e.g. from Fig. 9.42) no matter how many times we apply the recursion (9.108)
before evaluating the inverse cosine.
Ascending recursion for nd x
Using the imaginary argument property (9.63) of the elliptic cosine and the
Landen transformation’s duality (9.93) we can convert the descending recursion
formula (9.108) for cd x into an ascending recursion for nd x, which takes the
form
ndn+1 x =
(1 + kn) nd2
1 − (1 − kn) nd2
n x − 1
n x
(9.109)
The main value of this recursion formula for us will be that we’ll use it to derive
another transformation.
Double Landen transformation
Consider two subsequent Landen transformation steps occurring from kn−1 to
kn+1. Inverting (9.108) we obtain
cd2
n(cid:48) x =
cdn−1(cid:48) x + 1
(1 + k(cid:48)
n) + (1 − k(cid:48)
n) cdn−1(cid:48) x
=
1
1 + k(cid:48)
n
·
cdn−1(cid:48) x + 1
1 + kn−1 cdn−1(cid:48) x
Now we switch to the real period-based notation by substituting K (cid:48)
n−1/Kn−1 = 2K (cid:48)
This is also equivalent to K (cid:48)
thus K (cid:48)
cdn x and cdn−1(cid:48) x with cdn−1 2x, resulting in
nx ← Knx.
n/Kn and
n = 2Kn−1/Kn. Therefore the substitution replaces cdn(cid:48) x with
n−1x ← 2Kn−1x since K (cid:48)
n−1/K (cid:48)
cd2
n x =
cdn−1 2x + 1
(1 + k(cid:48)
n) + (1 − k(cid:48)
n) cdn−1 2x
=
1
1 + k(cid:48)
n
·
cdn−1 2x + 1
1 + kn−1 cdn−1 2x
Inverting (9.109) we obtain
nd2
n x =
ndn+1 x + 1
(1 + kn) + (1 − kn) ndn+1 x
=
1
1 + kn
·
ndn+1 x + 1
1 + k(cid:48)
n+1 ndn+1 x
382
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
By (9.69)
n cd2
k2
n nd2
n x =
n(cid:48) x + k(cid:48)2
k2
n
1 + k(cid:48)
n
=
·
cdn−1 2x + 1
1 + kn−1 cdn−1 2x
+
k(cid:48)2
n
1 + kn
·
ndn+1 x + 1
1 + k(cid:48)
n+1 ndn+1 x
=
= (1 + k(cid:48)
n)kn−1 ·
= (1 + k(cid:48)
n)
(cid:18)
1 −
= (1 + k(cid:48)
n)
(cid:18)
1 −
+ (1 + kn)k(cid:48)
n+1 ·
cdn−1 2x + 1
1 + kn−1 cdn−1 2x
1 − kn−1
1 + kn−1 cdn−1 2x
(cid:19)
+
ndn+1 x + 1
1 + k(cid:48)
n+1 ndn+1 x
=
(cid:18)
+ (1 + kn)
1 −
2k(cid:48)
n/(1 + k(cid:48)
1 + kn−1 cdn−1 2x
n)
(cid:19)
+
(cid:18)
+ (1 + kn)
1 −
1 − k(cid:48)
n+1
n+1 ndn+1 x
1 + k(cid:48)
(cid:19)
=
(cid:19)
=
2kn/(1 + kn)
1 + k(cid:48)
n+1 ndn+1 x
2kn
= 1
1 + k(cid:48)
n+1 ndn+1 x
= (1 + k(cid:48)
n) −
2k(cid:48)
n
1 + kn−1 cdn−1 2x
+ (1 + kn) −
Solving for k(cid:48)
n+1 ndn+1 x:
2kn
1 + k(cid:48)
n+1 ndn+1 x
= 1 + kn + k(cid:48)
n −
2k(cid:48)
n
1 + kn−1 cdn−1 2x
=
=
(1 + kn + k(cid:48)
n)(1 + kn−1 cdn−1 2x) − 2k(cid:48)
n
1 + kn−1 cdn−1 2x
1 + k(cid:48)
n+1 ndn+1 x =
2kn(1 + kn−1 cdn−1 2x)
(1 + kn + k(cid:48)
n)(1 + kn−1 cdn−1 2x) − 2k(cid:48)
n
k(cid:48)
n+1 ndn+1 x =
=
=
2kn(1 + kn−1 cdn−1 2x) + 2k(cid:48)
n − (1 + kn + k(cid:48)
n)(1 + kn−1 cdn−1 2x)
(1 + kn + k(cid:48)
n − 1) − (1 + kn + k(cid:48)
n) + (1 + kn + k(cid:48)
(kn + k(cid:48)
(1 + kn − k(cid:48)
n)kn−1 cdn−1 2x
n)kn−1 cdn−1 2x
n)(1 + kn−1 cdn−1 2x) − 2k(cid:48)
n
=
By (9.92) and (9.93)
kn =
k(cid:48)
n =
k(cid:48)
n+1 =
2(cid:112)kn−1
1 + kn−1
1 − kn−1
1 + kn−1
1 − kn
1 + kn
=
1 + kn−1 − 2(cid:112)kn−1
1 + kn−1 + 2(cid:112)kn−1
(cid:16)
(cid:33)2
=
(cid:32)
=
1 − (cid:112)kn−1
1 + (cid:112)kn−1
(cid:17)(cid:17)2
(cid:16)(cid:112)kn−1
=
−ρ−1
=
and thus
or
9.12. ELLIPTIC RATIONAL FUNCTIONS
383
where we have noticed that the obtained expression can be conveniently written
in terms of the Riemann sphere rotation ρ−1. Therefore
(cid:113)
k(cid:48)
n+1 = −ρ−1
(cid:16)(cid:112)kn−1
(cid:17)
=
1 − (cid:112)kn−1
1 + (cid:112)kn−1
Continuing the transformation of k(cid:48)
k(cid:48)
n+1 ndn+1 x =
n+1 ndn+1 x we obtain
(2(cid:112)kn−1 − 2kn−1) − (2 − 2(cid:112)kn−1)kn−1 cdn−1 2x
(2kn−1 + 2(cid:112)kn−1) + (2 + 2(cid:112)kn−1)kn−1 cdn−1 2x
1 − cdn−1 2x
1 + cdn−1 2x
1 − (cid:112)kn−1 cdn−1 2x
1 + (cid:112)kn−1 cdn−1 2x
k(cid:48)
n+1 ·
(cid:113)
=
·
1 − (cid:112)kn−1
1 + (cid:112)kn−1
(9.110)
=
ndn+1 x =
1 − cdn−1 2x
1 + cdn−1 2x
= −ρ−1
(cid:0) cdn−1 2x(cid:1)
ndn+1
x
2
=
1 − cdn−1 x
1 + cdn−1 x
= −ρ−1
(cid:0) cdn−1 x(cid:1)
(9.111)
where the respective elliptic modulus is found from (9.110).
Notice that the halving of the argument in (9.111) is matched by the fact
n−1/Kn−1 =
n+1/Kn+1. At the same time the real and imaginary periods of cdn−1 x are
n−1/Kn−1, while the real and imaginary periods of ndn+1(x/2) are 4
n−1/Kn−1. Thus we have identically periodic functions
that the period ratio K (cid:48)/K is changed by a factor of 4, That is K (cid:48)
4K (cid:48)
4 and 2K (cid:48)
and 8K (cid:48)
in the left- and right-hand sides of (9.111).
n+1/Kn+1 = 2K (cid:48)
9.12 Elliptic rational functions
Landen transformation was changing the period ratio by a factor of 2, which
resulted in various elliptic functions after the transformation being expressed
as a rational function of the same elliptic function prior to the transformation.
There is a generalization of Landen transformation where the period ratio is
changed by an arbitrary positive integer factor N . Such transformation is re-
ferred to as N -th degree transformation and the factor N is referred to as the
degree of the transformation.
Suppose we are having an elliptic modulus k with respective quarter periods
K (cid:48) and K. Let ˜k be another elliptic modulus with respective quarter periods
˜K (cid:48) and ˜K, such that the quarter period ratio is increased N times:
˜K (cid:48)
˜K
= N
K (cid:48)
K
(9.112)
(the equation (9.112) is referred to as degree equation). Notice that since the
period ratio is increased, the modulus is decreased: ˜k < k.
Apparently k and ˜k are interdependent, where from (9.112) we obtain that
increasing k decreases the ratios K (cid:48)/K and ˜K (cid:48)/ ˜K, and thus increases ˜k as well.
Thus ˜k = ˜k(k) is an increasing function, where at N equal to a power of 2 we
384
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
obtain a log2 N times repeated Landen transformation. The way to compute ˜k
from a given k (and back) for arbitrary N will be discussed later.
In the transformation from k to ˜k we wish to obtain the relationship for cd,
such that the imaginary period (in terms of normalized argument) is ﬁxed. It
turns out that such relationship always has the form:
cd ˜K(cid:48) u = RN
(cid:0)cdK(cid:48) u(cid:1)
(9.113)
where cdK(cid:48) u = cd(K (cid:48)u, k), cd ˜K(cid:48) u = cd( ˜K (cid:48)u, ˜k) and RN (x) is some real rational
function of order N . We already had a particular case of this formula for N = 2
in (9.108) where
R2(x) =
(1 + k(cid:48)
1 − (1 − k(cid:48)
n)x2 − 1
n)x2
(9.114)
The function RN (x) is referred to as elliptic rational function of order N .
Notice that RN (x) depends on the elliptic modulus k, even though we don’t
explicitly notate it as function’s parameter. Example graphs of RN (x) are given
in Fig. 9.56.
RN (x)
−1
∞
1
0
−1
1
∞
x
∞
−1
Figure 9.56: Elliptic rational functions of even (solid) and odd
(dashed) orders for k = 0.99. The graphs do not cross the horizon-
tal axis at x = 1, rather RN (1) = 1 ∀N , however the resolution of
the ﬁgure is insuﬃcient to see that. The poles of RN are occurring
at the intersections of the respective graph with the thin horizontal
dashed line at ∞.
By using (9.112) we could rewrite (9.113) in terms of the real periods:
cd ˜K N u = RN
(cid:0)cdK u(cid:1)
(9.115)
where cdK u = cd(Ku, k), cd ˜K u = cd( ˜Ku, ˜k). We could also rewrite (9.113)
and (9.115) in a form without argument normalization, giving:
cd(N ˜Ku, ˜k) = RN
(cid:0)cd(Ku, k)(cid:1)
(9.116)
9.12. ELLIPTIC RATIONAL FUNCTIONS
385
Notice that any of the formulas (9.113), (9.115), (9.116) implies that
RN ·M (x) = RN (RM (x)) = RM (RN (x))
(9.117)
(with the properly chosen elliptic moduli for each of RN ·M , RN and RM ), as
we are eﬀectively simply chaining an N-th and an M-th degree transformation.
RN (x) as representation of linear scaling
In an obvious way, equation (9.113) can be expressed in terms of the preimage
domain:
x = cdK(cid:48) u
RN (x) = cd ˜K(cid:48) u
Alternatively, (9.115) can be expressed as
x = cdK u
v = N u
RN (x) = cd ˜K v
(9.118a)
(9.118b)
(9.119a)
(9.119b)
(9.119c)
Diﬀerently from xN , TN (x) and T −1
N (x−1), this time there are two diﬀerent
mappings from the preimage to the representation domain in each case, corre-
sponding to the two diﬀerent moduli k and ˜k. The linear scaling is explicitly
present only in (9.119), however this is purely due to the implicit scaling con-
tained in the period-normalized notation. The explicit notation form is the same
for both (9.118) and (9.119) and contains the linear scaling:
x = cd(u, k)
v = N
˜K
K
u =
˜K (cid:48)
K (cid:48) u
RN (x) = cd(v, ˜k)
(9.120a)
(9.120b)
(9.120c)
The mappings are however still diﬀerent, since k (cid:54)= ˜k.
By (9.112) the scaling (9.120b) exactly matches the imaginary periods and
expands a single real period to exactly N real periods. For that reason the
shifts of u by an integer number of real and/or imaginary periods do not aﬀect
the values of x and RN (x). By the even symmetry of cd a change of sign of
u doesn’t aﬀect the values of x and RN (x) either. This however exhausts the
set of possible preimages of a given x, since, as we know, cd x takes each value
only once per quater-period grid cell (where the complex quadrants in Fig. 9.42
provide additional reference). Thus we can pick any preimage of x as the value
of u and therefore can rewrite (9.118), (9.119) and (9.120) in their respective
explicit forms:
RN (x) = cd ˜K(cid:48)
RN (x) = cd ˜K
(cid:32)
RN (x) = cd
K(cid:48) x(cid:1)
(cid:0)cd−1
K x(cid:1)
(cid:0)N cd−1
˜K
K
N
cd−1(x, k), ˜k
(cid:33)
(9.121a)
(9.121b)
(9.121c)
386
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
where cd−1 denote the inverse functions of the respective cd functions. Equa-
tion (9.121c) is the commonly known explicit expression for elliptic rational
functions.
As with TN (x) and TN (x), an important class of preimages will be the
horizontal lines in the complex planes u and v. From our discussion of cd and
cd we should recall that these lines produce distinct quasielliptic curves as their
respective images, the full cycle of these curves corresponding to a single real
period of u or v respectively (Figs. 9.52 and 9.53 serve as reference). Therefore x
moving in such quasielliptic curve will be mapped to RN (x) moving in a similar
curve, each cycle of x producing N cycles of RN (x).
Recall that with cosine-based preimages we were preferring the preimages
in the lower complex semiplane, so that preimage movement towards the right
was corresponding to counterclockwise rotation in the representation domain.
Similarly, we are going to choose the elliptic cosine-based preimages within the
imaginary quarter period strip located immediately below the real axis, as shown
in Fig. 9.52, therefore preimage movement towards the right will correspond to
counterclockwise rotation in the representation domain.
Given a preimage u located in the imaginary quarter period immediately
below the real axis, the preimage v will also be located in the imaginary quarter
period immediately below the real axis, since the imaginary quarter periods of
u are mapped exactly onto the respective imaginary quarter periods of v. Also,
apparently, u and v either both move simultaneously to the right or both to the
left. Thus x and RN (x) move either both counterclockwise or both clockwise.19
Bands of RN (x)
The four diﬀerent parts of the principal preimage of the real line in Fig. 9.47 will
correspond to the bands of elliptic ﬁlters which we are going to construct later.
It is convenient to introduce the respective terminology at this point already.
In terms of (9.120) the principal preimage of the real axis x ∈ R is
⇐⇒ x ∈ [−1, 1]
u ∈ [0, 2K]
u ∈ [0, jK (cid:48)]
u ∈ [jK (cid:48), jK (cid:48) + 2K] ⇐⇒ x ∈ [1/k, −1/k]
u ∈ [2K, 2K + jK (cid:48)] ⇐⇒ x ∈ [−1/k, 1]
⇐⇒ x ∈ [1, 1/k]
(a)
(b)
(c)
(d)
Respectively the principal preimage of the real axis RN (x) ∈ R is:
⇐⇒ RN (x) ∈ [−1, 1]
⇐⇒ RN (x) ∈ [1, 1/˜k]
v ∈ [0, 2 ˜K]
v ∈ [0, j ˜K (cid:48)]
v ∈ [j ˜K (cid:48), j ˜K (cid:48) + 2 ˜K] ⇐⇒ RN (x) ∈ [1/˜k, −1/˜k]
v ∈ [2 ˜K, 2 ˜K + j ˜K (cid:48)] ⇐⇒ RN (x) ∈ [−1/˜k, 1]
(˜a)
(˜b)
(˜c)
(˜d)
The linear scaling (9.120b) maps (b) to (˜b) one-to-one (imaginary period is pre-
served). The mapping from (a) to (˜a) and from (c) to (˜c) is one-to-N (real period
19Obviously, we could have chosen any other imaginary quarter period preimage strip. We
have chosen the one right below the real axis simply to have a better defined reference in the
preimage domain.
9.12. ELLIPTIC RATIONAL FUNCTIONS
387
is multiplied by N ). This is responsible for the appearance of the equiripples
for x ∈ [−1, 1] and x ∈ [1/k, −1/k] in Fig. 9.56. The mapping from (c) re-
sults either in some (non necessarily principal) preimage (˜d) if N is odd or in a
non-principal preimage (˜b) if N is even.
Naming these four bands of RN (x) after the respective bands of the elliptic
ﬁlters, we have:
Passband:
x ∈ [−1, 1]
Two transition bands:
x ∈ [−1/k, −1] ∪ [1, 1/k]
Stopband:
x ∈ [1/k, −1/k]
|RN (x)| ≤ 1
1 ≤|RN (x)| ≤ 1/˜k
|RN (x)| ≥ 1/˜k
The readers are advised to compare the above results to Fig. 9.56, identifying
the equiripples of amplitudes 1 and 1/˜k in the pass- and stop-bands respectively.
Since k is very close to 1, the transition bands are very narrow and aren’t visible
in Fig. 9.56, however at smaller k the stopband equiripples would become too
small to be visible in the same ﬁgure.
The value 1/k is determining the width of the transition band(s) and is
therefore referred to as the selectivity factor. The value 1/˜k determines the
ratio of the equiripple amplitudes in the pass- and stop-bands and is referred to
as the discrimination factor. Since k and ˜k increase or decrease simultaneously,
so do 1/k and 1/˜k. Therefore decreasing the transition band width (which is
the same as decreasing the selectivity factor 1/k) decreases the discrimination
factor of 1/˜k, thereby making the stop-band equiripples larger. Thus there is a
tradeoﬀ between the transition band width (which we, generally speaking, want
to be small) and the discrimination factor (which we, generally speaking, want
to be large). Fig. 9.57 illustrates.
R4(x)
−1
∞
1
0
−1
1
1.1
1.2
1.3
x
Figure 9.57: Transition region of R4(x) for k = 0.998 (solid) and
k = 0.99 (dashed). The horizontal axis is linear, the vertical axis
is using the arctangent scale.
388
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Even/odd property
Since cd(u ± 2K) = − cd u, a negation of x corresponds to a shift of its preimage
u by 2K. Respectively v is shifted by 2N ˜K, which will result in a negation of
RN (x) if N is odd and will not change RN (x) is N is even. Therefore RN (x) is
even/odd if N is even/odd:
RN (−x) = (−1)N RN (x)
(9.122)
Values at special points
The principal preimage of x = 1 is u = 0. Therefore v = 0 and RN (x) = 1.
Therefore
By (9.122)
RN (1) = 1
RN (−1) = (−1)N
The principal preimage of x = 1/k is u = jK (cid:48). By (9.112) v = j ˜K (cid:48) and
RN (x) = 1/˜k. Therefore
By (9.122)
RN (1/k) = 1/˜k
RN (−1/k) = (−1)N /˜k
Since equiripples begin exactly at the boundaries of the respective bands of
RN (x), the values at x = ±1 and x = ±1/k also give the amplitudes of the
equiripples of RN (x), which are thereby 1 in the passband and 1/˜k in the stop-
band.
The principal preimage of x = 0 is u = K. By (9.112) v = N ˜K and
(cid:40)
RN (0) =
0
(−1)N/2
if N is odd
if N is even
The principal preimage of x = ∞ is u = K + jK (cid:48). By (9.112) v = N ˜K + jK (cid:48).
With the help of (9.65) we reuse the result for RN (0), obtaining
(cid:40)
RN (∞) =
∞
if N is odd
(−1)N/2/˜k if N is even
Two other interesting points are logarithmic midpoints of the transition band
k is the transition
k. The princial preimage of x = 1/
occurring at x = ±1/
band’s preimage midpoint u = jK (cid:48)/2 and respectively v = j ˜K (cid:48)/2. Thus
√
√
√
k) = 1/
(cid:112)˜k
RN (1/
That is the logarithmic midpoint of the transition band [1, 1/k] is mapped to
the logarithmic midpoint of the respective value range [1, 1/˜k]. By (9.122)
√
RN (−1/
k) = (−1)N /
(cid:112)˜k
9.12. ELLIPTIC RATIONAL FUNCTIONS
389
Normalized elliptic rational functions
The graphs of RN (x) in Fig. 9.56 look somewhat asymmetric regarding the
boundaries of the bands, which are at ±1 and ±1/k and the amplitudes of the
ripples which are 1 and 1/˜k respectively. This can be addressed by switching to
normalized elliptic cosine. The equations (9.113), (9.115) and (9.116) thereby
respectively turn into
cd ˜K(cid:48) u = ¯RN
cd ˜K N u = ¯RN
cd(N ˜Ku, ˜k) = ¯RN
(cid:0) cdK(cid:48) u(cid:1)
(cid:0) cdK u(cid:1)
(cid:0) cd(Ku, k)(cid:1)
x = cd(u, k)
v = N
˜K
K
u =
˜K (cid:48)
K (cid:48) u
¯RN (x) = cd(v, ˜k)
while (9.120) turn into
and (9.121) turn into
(cid:16)
(cid:17)
cd
−1
K(cid:48) x
N cd
(cid:17)
−1
K x
¯RN (x) = cd ˜K(cid:48)
(cid:16)
¯RN (x) = cd ˜K
(cid:32)
¯RN (x) = cd
N
(cid:33)
cd
−1
(x, k), ˜k
˜K
K
where
¯RN (x) =
(cid:112)˜kRN
(cid:19)
(cid:18) x
√
k
(9.123a)
(9.123b)
(9.123c)
(9.124a)
(9.124b)
(9.124c)
(9.125a)
(9.125b)
(9.125c)
(9.126)
is the normalized elliptic rational function. Note that ¯RN (x) is essentially simply
a notational shortcut for the right-hand side of (9.126), however due to the more
pure symmetries, it is often more convenient to work in terms of ¯RN (x) than
RN (x).
The bands of ¯RN (x) are therefore
Passband:
Transition bands:
Stopband:
√
√
|x| ≤
k
√
(cid:112)˜k
|RN (x)| ≤
(cid:112)˜k ≤|RN (x)| ≤ 1/
|RN (x)| ≥ 1/
(cid:112)˜k
(cid:112)˜k
k
k
k ≤|x| ≤ 1/
√
|x| ≥ 1/
while the special points of ¯RN (x) respectively are:
√
√
√
√
¯RN (
¯RN (−
¯RN (1/
¯RN (−1/
(cid:112)˜k
k) =
k) = (−1)N (cid:112)˜k
(cid:112)˜k
k) = (−1)N /
k) = 1/
(cid:112)˜k
390
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
(cid:40)
(cid:40)
¯RN (0) =
¯RN (∞) =
0
if N is odd
(−1)N (cid:112)˜k if N is even
∞
(−1)N /
if N is odd
(cid:112)˜k if N is even
¯RN (1) = 1
¯RN (−1) = (−1)N
The graphs of ¯RN (x) are plotted in Fig. 9.58.
¯RN (x)
−1
∞
1
0
−1
1
∞
x
∞
−1
Figure 9.58: Normalized elliptic rational functions of even (solid)
and odd (dashed) orders for k = 0.9.
The interpretation of ¯RN (x) as a linear scaling representation is essentially
the same as in (9.118), (9.119) and (9.120) respectively, except that cd must be
replaced with cd. E.g. (9.120) becomes
x = cd(u, k)
v = N
˜K
K
u =
˜K (cid:48)
K (cid:48) u
¯RN (x) = cd(v, ˜k)
(9.127a)
(9.127b)
(9.127c)
Notice that the chain rule (9.117) works in the same form for normalized
elliptic rational functions:
¯RN ·M (x) = ¯RN ( ¯RM (x)) = ¯RM ( ¯RN (x))
(9.128)
since the elliptic modulus ˜k for one stage is equal to the elliptic modulus k for
the next stage, thus the coeﬃcients
k cancel between each pair of
stages.
(cid:112)˜k and 1/
√
9.12. ELLIPTIC RATIONAL FUNCTIONS
391
Symmetries of ¯RN (x)
In Fig. 9.58 one can notice that the graphs of ¯RN (x) have certain symmetries.
One symmetry is relatively to the origin:
¯RN (−x) = (−1)N ¯RN (x)
(9.129)
(which is simultaneously the symmetry relative to x = ∞). Apparently this is
simply the even/odd property of RN (x) which is preserved in the ¯RN (x) form.
The other symmetry is relatively to the point ¯RN (1) = 1:
¯RN (1/x) = 1/ ¯RN (x)
(9.130)
with a similar symmetry around the point at x = −1 which follows from the
ﬁrst two symmetries. The proof of (9.130) follows from (9.75d). Given x and its
preimage u, the preimage of 1/x is jK (cid:48) − u. Similarly, the preimage of 1/ ¯RN (x)
is j ˜K (cid:48) − v, however simultaneously
j ˜K (cid:48) − v =
˜K (cid:48)
K (cid:48) (jK (cid:48) − u)
therefore j ˜K (cid:48) − v is also the preimage of ¯RN (1/x) and ¯RN (1/x) = 1/ ¯RN (x).
In terms of RN (x) the symmetry (9.130) takes the form
RN (1/kx) = 1
(cid:46)˜kRN (x)
(9.131)
Similarly to RN (x), the normalized elliptic rational function ¯RN (x) maps
the quasielliptic curves Fig. 9.53 to other quasielliptic curves from the same
family. By Fig. 9.53 and (9.85) the unit circle will be mapped to the unit circle,
since the preimage line Im u = jK (cid:48)/2 + jK (cid:48)n(cid:48) will be mapped to the preimage
line Im v = j ˜K (cid:48)/2 + j ˜K (cid:48)n(cid:48). There won’t be other preimages of the unit circle,
since all trajectories in Fig. 9.53 are distinct and occur for diﬀerent imaginary
parts of the preimage. Thus
|x| = 1 ⇐⇒ | ¯RN (x)| = 1
(9.132)
Poles and zeros of RN (x)
Letting RN (x) = 0 and using the representation form (9.119) we obtain
v = 2n + 1 = 2
(cid:19)
+ n
(cid:18) 1
2
(where the second form is given for comparison with the respective derivations
of the zeros of xN and TN (x)). Respectively
and
u =
2n + 1
N
= 2
1
2 + n
N
x = cdK
2n + 1
N
(cid:18)
= cdK
2
(cid:19)
1
2 + n
N
392
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
which means that the zeros of RN (x) are
zn = cdK
2n + 1
N
(cid:18)
= cdK
2
(cid:19)
1
2 + n
N
(9.133)
where there are N distinct values corresponding to 0 < u < 2. Notice that the
zeros are all real and lie within (−1, 1). Also notice that zn = −zN −1−n, there-
fore the zeros are positioned symmetrically around the origin. Consequently, if
N is odd, one of zn will be at the origin.
By (9.131) the poles can be obtained from zeros as
pn = 1/kzn
Note that if N is odd, then so is RN (x) and one of the zeros will be at x = 0.
In this case one of the poles will be at the inﬁnity and there will be only N − 1
ﬁnite poles.
Given zn and taking into account that pn = 1/kzn, we can write RN (x) in
the form
RN (x) = g ·
(cid:89)
(x − zn)
(cid:89)
(x − 1/zn)
zn(cid:54)=0
which we could also write as
RN (x) = g · xN ∧1 ·
(cid:89)
zn(cid:54)=0
x − zn
x − 1/kzn
= g · xN ∧1 ·
(cid:89)
zn>0
x2 − z2
n
x2 − 1/k2z2
n
The value of the gain coeﬃcient g can be obtained from the fact that RN (x)
must satisfy RN (1) = 1. Alternatively we can satisfy RN (1) = 1 by simply
forcing each of the factors of RN (x) to be equal to unity at x = 1 (where prior
to the factor normalization we also have multiplied each of the factors by −k2z2
n)
RN (x) = xN ∧1 ·
(cid:18) 1 − k2z2
n
1 − z2
n
·
x2 − z2
n
nx2
1 − k2z2
(cid:19)
(cid:89)
zn>0
(9.134)
Poles and zeros of ¯RN (x)
By (9.126) the zeros of ¯RN (x) can be obtained from the zeros of RN (x) as
√
¯zn =
kzn = cdK
2n + 1
N
(cid:18)
= cdK
2
(cid:19)
1
2 + n
N
(9.135)
Notice that thereby ¯zn ∈ (−
zeros as
√
√
k,
k). By (9.130) the poles are related to the
The factored form (9.134) respectively becomes
¯pn ¯zn = 1
¯RN (x) = xN ∧1 ·
(cid:89)
¯zn>0
x2 − ¯z2
n
nx2
1 − ¯z2
(9.136)
where we don’t have explicit normalization factors anymore, since the factors
under the product sign are already all equal to unity at x = 1, thereby giving
9.12. ELLIPTIC RATIONAL FUNCTIONS
393
¯RN (1) = 1 as the transition band’s midpoint (which is exactly what it should
be according to the previously discussed special point values of ¯RN (x)).
By obtaining equations (9.134) and (9.136) we have constructed RN (x) and
¯RN in the explicit rational function form.
Relationship between k and ˜k
Note that the explicit rational function forms (9.134) and (9.136) were obtained
without using the yet unknown to us ˜k (or ˜K or ˜K (cid:48)). On the other hand,
having constructed RN (x) and/or ¯RN (x), we can obtain ˜k from the condition
RN (1/k) = 1/˜k or ¯RN (
We can also obtain an explicit expression for ˜k in terms of k. Substituting
(cid:112)˜k.
k) =
√
(9.134) into RN (1/k) = 1/˜k we obtain
(cid:18) 1 − k2z2
n
1 − z2
n
1/˜k = k−(N ∧1) ·
(cid:89)
·
1/k2 − z2
n
1 − z2
n
(cid:19)
=
= k−N ·
zn>0
(cid:18) 1 − k2z2
n
1 − z2
n
(cid:89)
zn>0
(cid:19)
·
1 − k2z2
n
1 − z2
n
= k−N ·
(cid:19)2
(cid:18) 1 − k2z2
n
1 − z2
n
(cid:89)
zn>0
By (9.133) zn = cdK un where
un =
2n + 1
N
Therefore
1
sn2
K un
where the latter transformation is by (9.68). Noticing that
K un
K un
1 − k2z2
n
1 − z2
n
1 − k2 cd2
1 − cd2
=
=
we obtain
zn > 0 ⇐⇒ 0 < un < 1
˜k = kN ·
(cid:89)
sn4
K un
0<un<1
(9.137)
(where the number of factors under the product sign is equal to the integer part
of N/2) which formally gives an explicit expression for ˜k. However practically
this expression is exactly the same as ˜k = 1/RN (1/k) and thus we can simply
ﬁnd ˜k (and respectively ˜K) from the latter condition.
Finding k from ˜k can be done by using the duality of the N -th degree
transformation in respect to k and k(cid:48) (which is pretty much the same as the
respective duality of the Landen transformation). Let ˜k = N (k) denote the
N -th degree transformation of k deﬁned by (9.137) (or by explicit usage of
(cid:112)˜k). If the ratio K (cid:48)/K is decreased N times by
RN (1/k) = 1/˜k or ¯RN (
the N -th degree transformation from k to ˜k, then the ratio K/K (cid:48) is increased N
times, which means we are performing an N −1-th degree transformation from
k(cid:48) to ˜k(cid:48), that is ˜k(cid:48) = N −1(k(cid:48)). Conversely, ˜k(cid:48) and k(cid:48) are related via an N -th
degree transformation: k(cid:48) = N (˜k(cid:48)), which by (9.137) means
k) =
√
k(cid:48) = ˜k(cid:48)N ·
(cid:89)
sn4
˜K(cid:48) un
0<un<1
(9.138)
394
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Renormalized elliptic rational functions
Similarly to renormalized Chebyshev polynomials we introduce renormalized
elliptic rational functions where we will renormalize only ¯RN (x) (although we
could take similar steps to renormalize RN (x) as well):
¯RN (x, λ) =
ˆ
¯RN (x/λ)
¯RN (1/λ)
(9.139)
√
k, while the stopband is |x| ≥ λ/
¯RN (x) is |x| ≤ λ
ˆ
As with renormalized Chebyshev polynomials, the parameter λ aﬀects the bands
of the elliptic rational functions and the equiripple amplitudes. Apparently the
k. Thus
passband of
if λ becomes smaller, the passband shrinks while the stopband simultaneously
expands and vice versa. The equiripple amplitudes in the pass and stop-bands
(cid:112)˜k ¯RN (1/λ) respectively. Therefore
are becoming equal to
both values increase if λ grows and decrease if λ becomes smaller, which means
that the equiripple sizes in the pass- and stop-bands are traded against each
other20 (Fig. 9.59). Notice that thereby a smaller bandiwith of the pass- or
stop-band corresponds to a smaller equiripple amplitude in the same band and
vice versa.
(cid:112)˜k/ ¯RN (1/λ) and 1/
√
¯R4(x)
−1
∞
1
0
−1
1
∞
x
∞
−1
Figure 9.59: Renormalized elliptic rational function for λ = 1
(solid), λ = k1/4 (small dashes) and λ = k−1/4 (large dashes).
As the used elliptic modulus k = 0.9 is pretty close to 1, the cor-
responding variations of the transition band width are not well
visible.
The reasonable range of λ is equal to the transition band [
k]. For λ
within this range the equiripples (both in the pass- and stopbands) do not grow
further than to unit amplitude. As a somewhat excessive range of λ we could
take (max{¯zn}, 1/ max{¯zn}), limiting λ between the zeros and poles of ¯RN (x).
In this case the equiripples may become arbitrarily large.
k, 1/
√
√
As with renormalized Chebyshev polynomials, we may omit the parameter
λ, understanding it implicitly, and simply write
¯RN (x).
ˆ
20By the earlier given definition of the amplitude of oscillations around infinity, the size of
the stop-band equiripples is smaller if the formal amplitude of the equiripples is larger.
9.12. ELLIPTIC RATIONAL FUNCTIONS
395
Relation to xN , TN (x) and TN (x)
In (9.136) it is easily noticed that at ¯zn → 0 the right-hand side turns into xN .
On the other hand |¯zn| ≤
k, therefore, if k → 0, then ¯zn → 0 and respectively
by (9.136)
√
¯RN (x) = xN
lim
k→0
(9.140)
or simply ¯RN (x) = xN for k = 0 (Fig. 9.60).
¯R4(x)
−1
∞
1
0
−1
1
∞
x
∞
−1
Figure 9.60: Normalized elliptic rational function for k = 0.9
(solid), k = 0.7 (dashed) and k = 0 (thin dashed).
At k → 0 we have cd x → cos x and cdK x → cos πx/2, therefore (9.115)
turns into
cos
π
2
N u = RN
(cid:16)
cos
(cid:17)
u
π
2
By replacing πu/2 with u it can be equivalently written as
cos N u = RN
(cid:0)cos u(cid:1)
which is identical to (9.35). Therefore RN (x) becomes identical to TN and thus
we have
lim
k→0
RN (x) = TN (x)
(9.141)
or simply RN (x) = TN (x) for k = 0. Notice that as K (cid:48) → ∞ and 1/k → ∞,
the transition band of RN (x) becomes inﬁnitely large in both preimage and
representation domains, while the stopband |x| ≥ 1/k disappears into inﬁnity.
Using (9.126) and (9.139) we can express the approaching to TN (x) in terms
of ¯RN (x) and
¯RN (x):
ˆ
¯RN
lim
k→0
(cid:17)
k
√
(cid:16)
x
(cid:112)˜k
¯RN
= lim
k→0 ˆ
(cid:16)
√
x, 1/
(cid:17)
k
= TN (x)
(9.142)
where we had to take the limit to avoid divisions and multiplications by zero.
By the deﬁnition of TN (x) equation (9.142) can be rewritten as
¯RN
lim
k→0
√
(cid:16)
x
(cid:112)˜k
(cid:17)
k
=
1
TN (1/x)
396
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Substituting 1/x for x and reciprocating both sides
(cid:112)˜k
(cid:16)√
k/x
lim
k→0
¯RN
(cid:17) = TN (1/x)
By (9.130) and (9.139)
(cid:112)˜k ¯RN
(cid:16)
lim
k→0
√
(cid:17)
k
x/
¯RN
= lim
k→0 ˆ
(cid:16)
x,
√
(cid:17)
k
= TN (1/x)
(9.143)
Transition band slope of ¯RN (x)
By (9.140) ¯RN (x) turns into xN at k = 0. On the other hand at the transition
band’s midpoint x = 1 we have ¯RN (x) = xN = 1 ∀k. It would be therefore
interesting to compare the slopes of ¯RN (x) and xN within the transition band.
In order to do that we are going to take similar steps to what we did in the
comparison of TN (x) and xN . Comparing (9.136) to (9.40) we compare their
individual factors by computing their respective diﬀerences:
x2 − ¯z2
n
1 − ¯z2
nx2 − x2 =
x2 − ¯z2
nx4
n − x2 + ¯z2
nx2
1 − ¯z2
√
=
n(x4 − 1)
¯z2
nx2
1 − ¯z2
(9.144)
k < 1 in the above. Thus in the range
Assuming k > 0, we have 0 < ¯zn <
1 < |x| < 1/ max{¯zn} the diﬀerences (9.144) are strictly positive and the factors
of (9.136) are larger than those of (9.40). Since for 1 < x < 1/ max{¯zn} all
factors of (9.136) and (9.40) are positive, we have
¯RN (x) > xN
(1 < x < 1/ max{¯zn}, N > 1)
By the even/odd symmetries of ¯RN (x) and xN :
| ¯RN (x)| > |xN |
(1 < |x| < 1/ max{¯zn}, N > 1)
By the symmetry (9.130) and by the same symmetry of xN
| ¯RN (x)| < |xN |
(max{¯zn} < |x| < 1, N > 1)
√
k, 1/
Note that thereby our discussion has completely covered the band |x| ∈
(max{¯zn}, 1/ max{¯zn}) which also includes the entire transition band |x| ∈
√
[
k]. From (9.144) we can also notice that the diﬀerence grows in mag-
nitude as ¯zn grow in magnitude. However by (9.135) and (9.91) the absolute
magnitudes of ¯zn should simply grow with k. Thus the diﬀerences (9.144) grow
with k and respectively ¯RN (x) deviates stronger for xN within the transition
band as k grows (which can be seen in Fig. 9.59).
In order to explicitly compute the derivative of ¯RN (x) at the transition
midpoint x = 1, we will evaluate the derivative of cd u at u = jK (cid:48)/2 (which is
the point where cd u = 1). By (9.72)
d
du
cd u =
√
d
du
k cd u =
√
k cd u ·
√
ln cd u = −
d
du
k · k(cid:48)2 cd u sc u nd u
9.12. ELLIPTIC RATIONAL FUNCTIONS
397
We already know that cd(jK (cid:48)/2) = 1/
obtained by (9.69) giving
√
k. The value of nd(jK (cid:48)/2) can be
1 − k2 cd2 jK (cid:48)
2
(cid:19)
=
1
k(cid:48)2
(cid:18)
1 − k2 cd2 jK (cid:48)
2
(cid:19)
=
1
k(cid:48)2 (1 − k) =
nd2 jK (cid:48)
2
=
=
(cid:18)
1
k(cid:48)2
1 − k
1 − k2 =
1
1 + k
By Fig. 9.44, (9.63) and Fig. 9.36 we choose the positive result of taking the
square root
nd
jK (cid:48)
2
=
√
1
1 + k
sc
jK (cid:48)
2
= j nd
−jK (cid:48)
2
= j nd
jK (cid:48)
2
=
√
j
1 + k
By (9.66)
Thus
(cid:18) d
du
cd
(cid:19) (cid:18) jK (cid:48)
2
(cid:19)
√
= −
k · k(cid:48)2 ·
1
√
k
·
√
j
1 + k
·
√
1
1 + k
=
= −j
k(cid:48)2
1 + k
= −j
1 − k2
1 + k
= −j(1 − k)
Now by (9.127) the function ¯RN (x) is obtained as a sequence of three transfor-
mations. Diﬀerentiating each of these transformations at the point correspond-
ing to x = 1 we obtain
du
dx
=
(cid:18) dx
du
˜K
K
(cid:19)−1
=
1
−j(1 − k)
=
˜K (cid:48)
K (cid:48)
= N
= −j(1 − ˜k)
dv
du
d ¯RN
dv
and thus
d ¯RN
dx
=
d ¯RN
dv
·
dv
du
·
du
dx
=
1 − ˜k
1 − k
N
˜K
K
=
1 − ˜k
1 − k
˜K (cid:48)
K (cid:48)
·
(at x = 1)
(9.145)
At k = 0 we have ˜k = 0, K = ˜K = π/2 and therefore ¯R(cid:48)
to the fact that ¯RN (x) = xN . At higher k the derivative grows (Fig. 9.61).
N (1) = N corresponding
In order to convince ourselves that ¯R(cid:48)
N (1) → +∞ for k → 1 we could notice
that ˜K (cid:48) → π/2, and K (cid:48) → π/2, therefore ˜K (cid:48)/K (cid:48) → 1. Now, for a given k, the
value ˜k will obviously decrease with growing N (as we are using a higher degree
transformation of the period ratio). Therefore (1 − ˜k)/(1 − k) can be bounded
from below by its own value at N = 2. A bit later in the text we will obtain
an explicit expression for ¯R(cid:48)
2(1) → +∞ as
k → 1. Therefore (1 − ˜k)/(1 − k) → +∞ at N = 2, and so it does for all other
N > 2.
2(1) where it will be obvious that ¯R(cid:48)
398
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
¯R(cid:48)
4(1)
4
0
1
k
Figure 9.61: Transition midslope derivative of ¯R4(x) for various k.
Transition band slope of RN (x)
By (9.141) the elliptic rational function RN (x) turns into a Chebyshev polyno-
mial of the same order N . We would be now in the position to compare their
slopes within the transition band of RN (x).
Comparing the factors of (9.134) and (9.42) (where in (9.42) we let λ = 1
T N (x) into TN (x)) we ﬁrst pretend that the zeros of RN (x) and TN (x)
ˆ
to turn
are identical. In that case the diﬀerence of the respective factors is
·
·
·
=
=
1 − k2z2
1 − k2z2
x2 − z2
n
1 − k2z2
(cid:18) 1 − k2z2
n
1 − k2z2
k2z2
1 − k2z2
x2 − z2
n
1 − z2
n
x2 − z2
n
1 − z2
n
1 − k2z2
n
1 − z2
n
x2 − z2
n
1 − z2
n
x2 − z2
n
nx2 −
1 − z2
n
n − 1 + k2z2
nx2
nx2
Since within the transition band [1, 1/k] of RN (x) we have 1 − k2z2
x2 − z2
nx2 > 0 and
n > 0, the diﬀerence (9.146) is positive for x ∈ (1, 1/k] (for 0 < k < 1).21
By (9.133) and (9.91) the zeros zn grow with k. On the other hand, the
zeros zn of RN (x) become equal to Chebyshev polynomial’s zeros at k = 0.
Thus the zeros of TN (x) are smaller in absolute magnitude than those of RN .
Temporarily notating Chebyshev polynomial’s zeros as λnzn where 0 < λn < 1
(assuming k > 0), we notice that
nx2 − 1
n(x2 − 1)
nx2
(cid:19)
=
(9.146)
=
·
x2 − z2
n
1 − z2
n
−
x2 − (λnzn)2
1 − (λnzn)2 =
(cid:18) x2 − z2
n
1 − z2
n
(cid:19)
−
− x2
(cid:18) x2 − (λnzn)2
1 − (λnzn)2 − x2
(cid:19)
=
=
n(x2 − 1)
z2
1 − z2
n
−
(λnzn)2(x2 − 1)
1 − (λnzn)2
(9.147)
where we have used (9.43). That is the diﬀerence (9.147) is again positive for
x > 1 and it is growing with λn → 0 (which corresponds to the growing k).
Adding (9.147) and (9.146) we obtain
1 − k2z2
n
1 − z2
n
·
x2 − z2
n
1 − k2z2
nx2 −
x2 − λnz2
n
1 − λnz2
n
=
21Actually the same holds a bit further than within the tranistion band, namely within
[1, 1/k max{zn}] but for simplicity we’ll talk of transition band.
9.12. ELLIPTIC RATIONAL FUNCTIONS
399
=
x2 − z2
n
1 − z2
n
·
n(x2 − 1)
k2z2
1 − k2z2
nx2 +
(cid:18) z2
n(x2 − 1)
1 − z2
n
−
(λnzn)2(x2 − 1)
1 − (λnzn)2
(cid:19)
> 0
(9.148)
where x ∈ (1, 1/k]. By our preceding discussion (9.148) becomes larger at larger
k.
Since all involved factors are greater than unity in the transition band of
RN (x), it follows that RN (x) > TN (x) in that range and the diﬀerence grows
with k. By even/odd symmetries of the respective functions we have
|RN (x)| > |TN (x)|
(|x| ∈ (1, 1/k], k > 0, N > 1)
and the diﬀerence becomes larger with k.
Elliptic rational functions of order 2N
Constructing an elliptic rational function of a power-of-2 order is especially easy,
since we can combine the explicit expression (9.114) for R2(x) with the chain
rule (9.117) to build functions of other power-of-2 orders.
It is therefore also helpful to have a ready explicit expression for ¯R2(x). By
(9.114) and (9.126)
¯R2(x) =
(cid:112)˜k ·
(1 + k(cid:48))
x2
k
1 − (1 − k(cid:48))
Recalling that for R2(x)
we further obtain
(cid:112)˜k ·
=
√
1 + k(cid:48)
1 − k(cid:48)2
x2 − 1
1 −
√
1 − k(cid:48)
1 − k(cid:48)2
x2
(cid:112)˜k ·
=
(cid:114) 1 + k(cid:48)
1 − k(cid:48) x2 − 1
(cid:114) 1 − k(cid:48)
1 + k(cid:48) x2
1 −
− 1
x2
k
˜k = L−1(k) =
1 − k(cid:48)
1 + k(cid:48)
x2
(cid:112)˜k
1 − x2(cid:112)˜k
Multiple expressions of the form (9.149) (with diﬀerent ˜k related through Landen
transformation) can be chained to obtain ¯RN (x) for other power-of-2 orders.
(cid:112)˜k
x2 −
1 − x2(cid:112)˜k
¯R2(x) =
(cid:112)˜k ·
(9.149)
− 1
=
The derivative at x = 1 is
(cid:16)
2x
¯R(cid:48)
2(1) =
1 − x2(cid:112)˜k
(cid:16)
(cid:16)
2
1 −
=
(cid:17)
(cid:112)˜k
(cid:16)
(cid:17)
(cid:112)˜k
+ 2x
(cid:17)2
1 − x2(cid:112)˜k
(cid:16)
(cid:112)˜k
(cid:17)2
(cid:112)˜k
1 −
+ 2
1 −
(cid:16)
x2 −
(cid:17)
(cid:112)˜k
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)x=1
=
(cid:17)
(cid:112)˜k
= 2
(cid:112)˜k
(cid:112)˜k
1 +
1 −
(9.150)
It is left as an excercise for the reader to verify that (9.150) is a particular case
of (9.145).
Notably, by (9.110), the formula (9.150) can be rewritten as ¯R(cid:48)
1 is a complementary modulus to k1 where k1 = L(k).
where k(cid:48)
2(1) = 2/(cid:112)k(cid:48)
1
400
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
9.13 Elliptic filters
Elliptic ﬁlters are obtained by using renormalized elliptic rational functions
¯RN (ω) as f (ω) in (9.18).22 The main motivation to use renormalized elliptic
ˆ
rational functions instead of ωN and
T N (ω) is that, as we already know they
ˆ
grow faster than ωN and
T N (ω) within their transition bands, which results
ˆ
in a steeper transition band’s slope. The tradeoﬀ is that in order to achieve a
steeper transition band we need to allow ripples in the pass- and stop-bands.
Thus, in (9.18) we let
f (ω) =
¯RN (ω)
ˆ
that is
|H(jω)|2 =
1
¯R2
N (ω)
ˆ
¯RN (ω) is aﬀecting the equiripple amplitudes of
ˆ
¯RN and
The λ parameter of
ˆ
thereby the equiripple amplitude in the pass- and stop-bands of |H(jω)|. It is
convenient to introduce the additional variable
1 +
ε =
1
¯RN (1/λ)
(9.151)
Using (9.151) we particularly may write
f (ω) =
¯RN (ω) = ε ¯RN (ω/λ)
ˆ
Given a desired ﬁlter order N , we still have two further freedom degrees to
play with, corresponding to the parameters k and λ, where, as we should recall
from the discussion of elliptic rational functions, k deﬁnes the tradeoﬀ between
the transition bandwidth and the ripple amplitudes, and λ deﬁnes the tradeoﬀ
between the ripple amplitudes in the pass- and stop-bands. One of the possible
scenarios to compute the elliptic ﬁlter parameters can be therefore the following.
Suppose we are given the desired ﬁlter order N and the desired pass- and stop-
band boundaries (where the passband boundary must be to the left of ω = 1
and the stopband boundary must be to the right of ω = 1). Recalling that the
pass- and stop-bands of
k] and
[λ/
¯RN are (in the positive frequency range) [0, λ
ˆ
k, +∞) respectively, we can ﬁnd λ and k.
Another scenario occurs if we are given the discrimination factor 1/˜k. By
(9.137) this deﬁnes selectivity factor 1/k and respectively the transition band
width, but we can still play with λ to control the tradeoﬀ between the pass- and
stop-band ripples.
√
√
√
Since the passband amplitude of ¯RN is
k, the amplitude response |H(jω)| is
1 + kε2, 1] in the passband. Since the stopband amplitude of
varying within [1/
√
k, the amplitude response |H(jω)| is varying within [0, 1/(cid:112)1 + ε2/k]
¯RN is 1/
in the stopband. The value of ε therefore aﬀects the tradeoﬀ between the equirip-
ple amplitudes of |H(jω)| in the pass- and stop-bands. Since ε depends on λ,
√
22Classically, elliptic filters are obtained from elliptic rational functions RN (ω) by letting
f (ω) = εRN (ω) where ε > 0 is some small value. This way however usually requires some
cutoff correction afterwards. The way how we introduce Chebyshev filters is essentially the
same, but directly results in a better cutoff positioning and better symmetries. Particularly
EMQF filters directly arise at λ = 1. One way is related to the other via ε = 1/RN (1/
kλ)
combined with a cutoff adjustment by the factor
kλ.
√
√
9.13. ELLIPTIC FILTERS
401
this is consistent with our previous conclusion that λ aﬀects the tradeoﬀ be-
¯RN , where a smaller bandwidth of the pass-
tween the equiripple amplitudes of
ˆ
or stop-band corresponds to smaller equiripples within the respective band. Re-
k] (or just a little
member that the range of λ is generally restricted to [
bit wider). Fig. 9.62 illustrates.
k, 1/
√
√
|H(jω)|
1
0.5
0
1/8
1
8
ω
Figure 9.62: Elliptic ﬁlter’s amplitude responses for N = 5, k =
0.98 and λ = 1 (solid) and λ = k−1/4 (dashed). Notice the usage
of the linear amplitude scale, which is chosen in order to be able
to show the amplitude response zeros.
Poles of elliptic filters
Since
have poles and zeros. The equation for the poles of |H(s)|2 = H(s)H(−s) is
¯RN (ω) is a rational function, the transfer function deﬁned by (9.18) will
ˆ
or
or
or, introducing ¯ω = ω/λ
1 +
N (ω) = 0
¯R2
ˆ
¯RN (ω) = ±j
ˆ
ε ¯RN (ω/λ) = ±j
¯RN (¯ω) = ±
j
ε
(9.152)
(9.153)
where the “+” sign corresponds to the even poles and the “−” sign to odd poles.
Recall the interpretation of ¯RN as a representation of linear scaling of the
preimage, which is given by (9.127). Suppose ¯ω is moving in a counterclockwise
direction in a quasielliptic curve which is a representation of some preimage line
Im u = β (Figs. 9.52, 9.53). Earlier we have agreed to chose the preimages within
the imaginary quarter period right below the real axis, that is β ∈ (−jK (cid:48), 0). In
this case the counterclockwise movement in the representation domain assumes
that u is moving towards the right.
402
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Since v = u ˜K (cid:48)/K (cid:48), the corresponding line in the preimage of ¯RN (¯ω) is Im v =
˜β, where ˜β = β ˜K (cid:48)/K (cid:48). Therefore Im v ∈ (−j ˜K (cid:48), 0), that is the line also goes
within the imaginary quarter period right below the real axis. Obviously, since
u moves towards the right, so does v, and ¯RN (¯ω) moves in a counterclockwise
direction.
We wish ¯RN (¯ω) to pass through the points ±j/ε going counterclockwise.
By (9.71) the intersections of the quasielliptic curve ¯RN (¯ω) with the imaginary
axis are occuring at ±j sc( ˜β, ˜k(cid:48)), therefore we choose
(which thereby belongs to (− ˜K (cid:48), 0))23 and
˜β = − sc −1(1/ε, ˜k(cid:48))
β =
K (cid:48)
˜K (cid:48)
˜β = −
K (cid:48)
˜K (cid:48)
sc −1(1/ε, ˜k(cid:48)) = −
K
N ˜K
sc −1(1/ε, ˜k(cid:48))
According to (9.71) the purely imaginary values of cd(v, ˜k) are attained when
the real part of the elliptic cosine’s argument is equal to (2n+1) ˜K, where n ∈ Z.
Thus, the values ±j/ε will be attained by ¯RN (¯ω) at
v = j ˜β + (2n + 1) ˜K
(9.154)
where, since ˜β < 0, the value ¯RN (¯ω) = j/ε is attained at n = 0 and other even
values of n. Thus, the solutions of the even pole equation f = j will occur at
even values of n. Fig. 9.63 illustrates.
Im v
2 ˜K (cid:48)
˜K (cid:48)
0
− ˜K (cid:48)
−2 ˜K (cid:48)
Im ¯RN (¯ω)
¯RN (¯ω) = cd(v, ˜k)
Re v
4 ˜K
0
Re ¯RN (¯ω)
Figure 9.63: Preimages of ¯RN (¯ω) = ±j/ε (qualitatively).
From (9.154) we obtain
u = jβ +
K
N ˜K
(2n + 1) ˜K = jβ + K
2n + 1
N
= jβ + 2K
1
2 + n
N
23Instead of − sc −1(1/ε, ˜k(cid:48)), which can be expected to have an unambiguous principal
(j/ε, ˜k(cid:48)), however, as there are multiple solutions
value, one can equivalently compute − cd
to the equation cd(x, ˜k(cid:48)) = j/ε, one needs to be careful to be sure that the cd
routine
returns the same value as would have been obtained by using sc −1. In principle any solution
of cd(x, ˜k(cid:48)) = j/ε would do, but may result in a different order of iteration of elliptic filter’s
poles, so that the unstable poles will be obtained first.
−1
−1
9.13. ELLIPTIC FILTERS
403
where there are 2N essentially diﬀerent preimages of ¯ω occuring at 2N consecu-
tive values of n all lying on the line Im u = β. Going back to the representation
domain we obtain ¯ω lying on the respective quasiellipse:
(cid:18)
¯ω = cd
jβ + K
(cid:19)
2n + 1
N
, k
Fig. 9.64 illustrates.
Im u
2K (cid:48)
K (cid:48)
0
−K (cid:48)
−2K (cid:48)
Im ¯ω
¯ω = cd(u, k)
Re u
Re ¯ω
4K
0
Figure 9.64: Transformation of Fig. 9.63 by u = Kv/N ˜K (for
N = 2). The white and black dots on the quasielliptic curve are
even/odd elliptic poles in terms of ¯ω. (The picture is qualitative.)
Switching to ω = λ¯ω:
(cid:18)
ω = λ cd
jβ + K
(cid:19)
2n + 1
N
, k
It is easily checked that the values of ω are moving counterclockwise starting
from the positive real semiaxis, where the values occurring at even/odd n cor-
respond to even/odd poles respectively.
Switching from ω to s = jω we obtain the expression for the poles:
(cid:18)
s = jλ cd
jβ + K
(cid:19)
2n + 1
N
, k
(9.155)
Since the values of ω are moving counterclockwise starting from the real positive
semiaxis, the values of s are moving counterclockwise starting from the imagi-
nary “positive” semiaxis, which means that starting at n = 0 we ﬁrst obtain the
stable poles at n = 0, . . . , N − 1. The next N values of n will give the unstable
poles.
For ﬁlter orders which are powers of 2 one can solve the equation (9.153) in
an easier way using (9.149) and (9.128).
Zeros of elliptic filters
The zeros of H(s), if expressed in terms of ω coincide with the poles of f (ω) =
¯RN (ω, λ), which can be found from the poles ¯pn = 1/¯zn of ¯RN (ω) as
¯pn = λ¯pn =
ˆ
ˆ
404
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
λ/¯zn, where ¯zn are given by (9.135). By s = jω the zeros can be reexpressed in
terms of s. For ﬁlter orders which are powers of 2 there is a simpler way using
(9.149) and (9.128).
One should remember that for odd N the function ¯RN has a zero at the origin
which doesn’t have a corresponding ﬁnite pole of ¯RN , respectively there is no
corresponding ﬁnite zero of H(s) and no corresponding factor in the numerator
of H(s). Respectively the order of the numerator of H(s) is N − 1 rather than
N , and the zero at the iniﬁnity occurs automatically due to the order of the
numerator being less than the order of the denominator. Fig. 9.65 provides an
example.
Im s
(Re ω)
j
−1
0
1
Re s
(−Im ω)
−j
Figure 9.65: Poles (white and black dots) and zeros (white squares)
of an elliptic ﬁlter of order N = 5. Each of the zeros is duplicated,
but the duplicates are dropped together with the unstable poles.
In Fig. 9.65 one can notice that the poles are more condensed closer to the
imaginary axis. Apparently, this is due to (9.81c) and the related explanation.
Gain adjustments
The default normalization of the elliptic ﬁlter’s gain is according to (9.18):
H(0) =
(cid:113)
1 +
1
¯R2
ˆ
N (0)
=
(cid:113)
1
1 + ε2 ¯R2
N (0)
=
(cid:113)
1
1 + ε2 (Re jN )2 ˜k
(9.156)
which thereby deﬁnes the leading gain coeﬃcient of the cascade form imple-
mentation (8.1). We could also ﬁnd the leading gain from the requirement
|H(j)|2 = 1/2, but we should mind the possibility of accidentally obtaining a
180◦ phase response at ω = 0.
With the leading gain deﬁned this way the amplitude response varies within
(cid:112)
1 + ˜kε2, 1] in the passband. We could choose some other normalizations,
[1/
though. E.g. we could require H(0) = 1. Or we could require the passband
SUMMARY
405
ripples to be symmetric relatively to the zero decibel level, which is achieved by
multiplying (9.156) by (1 + ˜kε2)1/4:
H(0) =
(cid:115) (cid:112)
1 + ˜kε2
1 + ε2 ¯R2
N (0)
so that |H(jω)| varies within [1/(1 + ˜kε2)1/4, (1 + ˜kε2)1/4] within the passband.
Elliptic minimum Q filters
At λ = 1 we have
(9.130). Simultaneously, by (9.151) ε = 1, respectively
¯RN (ω) = ¯RN (ω), thus f (ω) attains the reciprocal symmetry
ˆ
˜β = − sc −1(1, ˜k(cid:48)) = − ˜K (cid:48)/2
β =
K (cid:48)
˜K (cid:48)
˜β = −K/2
while (9.155) turns into
(cid:18)
s = j cd
j
K
2
+ K
(cid:19)
2n + 1
N
, k
(9.157)
By Fig. 9.53 and (9.85) the poles of H(s) are all lying on the unit circle.24
Further, by Figs. 9.54, 9.55 and the associated discussion, the values of cd in
(9.157) are having the maximum possible angular deviation (among the ones
arising from diﬀerent values for k) from the real axis. Respectively, the poles
given by (9.157) are having the maximum possible angular deviation from the
imaginary axis, which means that the corresponding cascade 2-pole sections will
have the minimum possible resonances. Therefore elliptic ﬁlters arising at λ = 1
are referred to as elliptic minimum Q filters, or shortly EMQF.
Butterworth and Chebyshev limits
By (9.140) at k = 0 EMQF ﬁlters turn into Butterworth ﬁlters. By (9.142)
at k → 0 and λ = 1/
k elliptic ﬁlters turn into Chebyshev type I ﬁlters. By
k elliptic ﬁlters turn into Chebyshev type II ﬁlters.
(9.143) at k → 0 and λ =
√
√
SUMMARY
Classical signal processing ﬁlters are deﬁned in terms of the squared amplitude
response equation (9.18). By choosing diﬀerent function types as f (ω) in (9.18)
one obtains the respective ﬁlter types:
Butterworth
Chebyshev type I
Chebyshev type II
Elliptic
f (x) = xN
f (x) =
f (x) =
f (x) =
T N (x)
ˆ
TN (x)
ˆ
¯RN (x)
ˆ
24From a slightly different angle, since λ = 1 and ε = 1, the pole equation turns into
¯RN (ω) = ±j. By (9.132) the solutions are lying on the unit circle.
406
CHAPTER 9. CLASSICAL SIGNAL PROCESSING FILTERS
Chapter 10
Special filter types
Butterworth ﬁlters of the 1st and 2nd kind as well as elliptic ﬁlters can serve
as a basis to contruct other ﬁlter types of a more specialzed nature, which are
going to be the subject of this chapter.
10.1 Reciprocally symmetric functions
The reciprocal symmetry (9.130) seems to be responsible for the special proper-
ties of EMQF ﬁlters. There is indeed a strong relationship between those, which
is worth a dedicated discussion, because we will have more uses of such functions
throughout this text. Let’s therefore suppose that f (x) (used in (9.18)) satisﬁes
f (1/x) = 1/f (x)
(10.1)
Reciprocal symmetry of the poles
An obvious conjecture which might appear from the discussion of EMQF ﬁlters
is that (10.1) implies the poles on the unit circle. This, however, is not exactly
true, although there is some relation.
The symmetry (10.1) actually implies the reciprocal symmetry of the ﬁlter’s
poles. That is, if s is a pole of H(s), then so is 1/s. Indeed, suppose f 2(−js) =
−1, which means s is a pole of H(s). Then f 2(−j/s) = f 2(1/js) = 1/f 2(js) =
1/f 2(−js) = −1 (where the latter transformation is by the fact that f is required
to be odd or even) and thus 1/s is also a pole of H(s).
The reciprocal symmetry of the ﬁlter’s poles manifests itself nicely for the
poles on the unit circle, where the reciprocation turns into simply conjugation,
and as poles of the real ﬁlters must be conjugate symmetric, they are also
reciprocally symmetric. But the poles do not really have to lie on the unit
circle.
407
408
CHAPTER 10. SPECIAL FILTER TYPES
Image of the unit circle
f (x) maps unit circle to the unit circle.1 Indeed, ﬁrst notice that |x| = 1 ⇐⇒
1/x = x∗. Suppose |x| = 1. Then, recalling that f is real,
f (1/x) = f (x∗) = f ∗(x) = 1/f (x) ⇐⇒ |f (x)| = 1
Therefore
|x| = 1 =⇒ |f (x)| = 1
(cid:54)=
The converse is however not necessarily true:
1, |f (x)| = 1. In other words, there may be other preimages of the unit circle
points.
it’s possible that ∃x : |x|
E.g. consider the function
f (x) = ρ+1
(cid:16)
(ρ+1(x))3(cid:17)
= x
x2 + 3
3x2 + 1
Apparently f (x) satisﬁes (10.1). However, it has three diﬀerent preimages of
the unit circle:
x(t) = ρ+1(ejαt)
where t ∈ R and α is one of the values π/6, 3π/6, 5π/6. At α = 3π/6 = π/2 we
obtain |x| = 1, however for other α this is not so.
Poles on the unit circle
Under the additional restriction that the zeros of f (x) (including a possible zero
at x = ∞) must be either all inside or all outside of the unit circle, the unit
circle will be the only preimage of the unit circle, that is
|x| = 1 ⇐⇒ |f (x)| = 1
(10.2)
We have already shown that |x| = 1 =⇒ |f (x)| = 1, therefore it remains
for us to show that |x| = 1 ⇐= |f (x)| = 1. First notice that (10.1) implies
that the poles of f (x) are reciprocals of the zeros. From (10.1) we also have
f (1) = ±1. Then f (x) can be written as
f (x) =
(cid:89)
n
x − zn
1 − znx
where zn are the zeros of f (x). Furthermore, since f is real, complex zeros must
come in conjugate pairs and so must complex poles, and we can write
f (x) =
(cid:89)
n
x − zn
1 − z∗
nx
(10.3)
Suppose all zeros of f (x) lie inside the unit circle. Let’s show that in this case
|x| > 1 =⇒ |f (x)| > 1. Suppose |x| > 1. In order to show that |f (x)| > 1 we
are going to show that each of each of the factors of f (x) has absolute magnitude
greater that unity:
(cid:12)
(cid:12)
(cid:12)
(cid:12)
x − zn
1 − z∗
nx
(cid:12)
(cid:12)
(cid:12)
(cid:12)
> 1
1Notice that incidentally this implies that f (x) is a discrete-time allpass transfer function,
although not necessarily describing a stable allpass.
10.1. RECIPROCALLY SYMMETRIC FUNCTIONS
409
By equivalent transformations we are having
|x − zn| > |1 − z∗
nx|
|x|2 − znx∗ − z∗
(x − zn)(x∗ − z∗
nx)(1 − znx∗)
n) > (1 − z∗
nx − |zn|2 > 1 − znx∗ − z∗
(|x|2 − 1)(1 − |zn|2) > 0
nx − |zn|2 · |x|2
which is obviously true, therefore each of the factors of f (x) is larger than
1 in absolute magnitude and so is f (x).
In a similar way we can show that
|x| < 1 =⇒ |f (x)| < 1. Therefore there are no other images of unit circle points
and we have shown that |x| = 1 ⇐= |f (x)| = 1. The case of all zeros lying
outside the unit circle is treated similarly, where we have |x| > 1 =⇒ |f (x)| < 1
and |x| < 1 =⇒ |f (x)| > 1.
From (10.2) it follows that the solutions of the pole equation f 2(x) = −1
are lying on the unit circle, and so do the poles of H(s) obtained from f (x).
Complementary symmetry of lowpass and highpass filters
The reciprocal symmetry of the poles implies that ﬁlters H(s) and H(1/s) (re-
lated by the LP to HP transformation) share the same poles. Furthermore,
it turns out that there is a complementary symmetry of squared amplitude
responses of these ﬁlters:
|H(jω)|2 + |H(1/jω)|2 = 1 ⇐⇒ f (1/x) = 1/f (x)
(10.4)
Indeed, by the Hermitian property of H(1/jω), the left-hand side of (10.4) can
be equivalently written as
|H(jω)|2 + |H(j/ω)|2 = 1
Using (9.18) we further rewrite it as
1
1 + f 2(ω)
+
1
1 + f 2(1/ω)
= 1
Transforming the equation further, we obtain
1
1 + f 2(1/ω)
= 1 −
1
1 + f 2(ω)
=
f 2(ω)
1 + f 2(ω)
=
1
1
f 2(ω)
1 +
or equivalently
or
f 2(1/ω) = 1/f 2(ω)
f (1/ω) = ±1/f (ω)
Noticing that f (1/ω) = −1/f (ω) implies f 2(1) = −1, which is impossible for a
real f (ω), we conclude that f (1/ω) = −1/f (ω) is not an option. Thus we simply
have f (1/ω) = 1/f (ω), which was obtained by an equivalent transformation
from |H(jω)|2 + |H(1/jω)|2 = 1 and thus both conditions are equivalent.
410
CHAPTER 10. SPECIAL FILTER TYPES
10.2 Shelving and tilting filters
We have made some attempts to construct shelving ﬁlters in the discussions of 1-
and 2-poles, but the results were lacking intuitively desired amplitude response
symmetries shown in Fig. 10.1. The kind of symmetry shown in Fig. 10.1 is
better expressed if we symmetrize the amplitude response further, obtaining
the one in Fig. 10.2.
|H(jω)|
|H(jω)|
0 dB
0 dB
1/8
1
8
ω
1/8
1
8
ω
Figure 10.1: Shelving amplitude responses, ideally symmetric in
fully logarithmic scale.
|H(jω)|
0 dB
1/8
1
8
ω
Figure 10.2: Tilting amplitude response, ideally symmetric in fully
logarithmic scale.
Fig. 10.1 apparently shows low-shelving (left) and high-shelving (right) am-
plitude responses. The amplitude response in Fig. 10.2 can be referred to as
tilting amplitude response. It is easy to notice that the low- and high-shelving
responses can be obtained from the tilting one by vertical shifts. Vertical shifts
in decibel scale are corresponding to multiplication of the signal by a constant.
That is tilting and low- and high-shelving ﬁlters can be obtained from each other
by a multiplication by a constant. We will therefore not make much distinction
between these types, arbitrarily jumping from one type to the other, whenever
the discussion requires so.
Reciprocal symmetry of poles and zeros
Treating |H(1)| = 1 as the logarithmic origin of an amplitude response graph we
can express the desired symmetry of the tilting amplitude response in Fig. 10.2
as an odd logarithmic symmetry:
log |H(j exp(−x))| = − log |H(j exp x)|
10.2. SHELVING AND TILTING FILTERS
411
which in linear scale becomes
or
|H(j/ω)| =
|H(j/ω)|2 =
1
|H(jω)|
1
|H(jω)|2
(10.5)
Writing |H(jω)|2 as H(jω)H(−jω) we introduce G(s) = H(s)H(−s). Then
(10.5) becomes
1
G(jω)
Taking into account that G(s) is even, we have
G(j/ω) =
G(jω)G(1/jω) = 1
Apparently, the latter equality must be true not only for ω ∈ R but also for any
ω ∈ C, thus
G(s)G(1/s) = 1
(s ∈ C)
Therefore, G(s) = 0 ⇐⇒ G(1/s) = ∞ and G(s) = ∞ ⇐⇒ G(1/s) = 0. That
is the poles of G(s) are reciprocals of the zeros of G(s) and vice versa.
Conversely, given G(s) = H(s)H(−s) such that its poles are reciprocals of
its zeros and additionally requiring that |G(j)| = 1 we will have G(s)G(1/s) = 1
and (10.5) follows. Indeed, writing G(s) in the factored form we have
G(s) = g ·
(cid:89)
n
s − zn
1 − zns
where zn are the zeros of G(s). Then
G(1/s) = g ·
(cid:89)
1/s − zn
1 − zn/s
(cid:89)
= g ·
1 − zns
s − zn
n
Therefore G(s)G(1/s) = g2. Letting s = j we have
n
g2 = G(j)G(1/j) = G(j)G(−j) = G(j)G∗(j) = |G(j)|2 = 1
and thus G(s)G(1/s) = 1.
Now the poles and zeros of G(s) consist of those of H(s) and their symmetric
counterparts (with respect to the complex plane’s origin). Under the assumption
that H(s) must be stable, all poles of H(s) will be in the left complex semiplane.
Under the additional assumption that H(s) is minimum phase, so will be zeros
of H(s). Respectively H(−s) will contain poles and zeros in the right semiplane.
However, the reciprocation turns left-semiplane values into left-semiplane values
and right-semiplane values into right-semiplane values. Therefore the poles of a
minimum-phase stable H(s) will be mutually reciprocal with the zeros of H(s).
Thus, in order for a minimum phase H(s) to have the tilting amplitude
response symmetry of Fig. 10.2 its poles and zeros must be mutually recipro-
cal. Conversely, given H(s) with mutually reciprocal poles and zeros (which is
thereby minimum phase, assuming H(s) is stable), (10.5) will hold under the
additional requirement |H(j)| = 1. Relaxing the minimum phase requirement
eﬀectivlely means that some zeros will be ﬂipped from the left semiplane into
the right semiplane, which is pretty trivial and doesn’t change the amplitude
response, therefore we will concentrate on minimum phase tilting and shelving
ﬁlters.
412
CHAPTER 10. SPECIAL FILTER TYPES
Construction as a lowpass ratio2
Let G(s) be a ﬁlter (now this is a diﬀerent G(s) than G(s) = H(s)H(−s) we
have been using above) having the following properties: G(s) doesn’t have zeros,
all its poles are lying on the unit circle, and G(0) = 1.
Apparently, G(s) is a lowpass ﬁlter, which should be obvious by considering
the factoring of G(s) into a cascade of 1- and 2-poles. Such G(s) also can be
factored as
G(s) =
N
(cid:89)
n=1
1
s − pn
(where the leading coeﬃcient is 1 due to G(s) being real stable, G(0) = 1,
|pn| = 1 and Re pn < 0). The poles of G(s) lying on the unit circle and being
conjugate symmetric imply the reciprocal symmetry of the poles: if pn is a pole
of G(s) then so is 1/pn.
Let’s apply the cutoﬀ substitution s ← s/M (M ∈ R, M > 0) to G(s). We
obtain
G(s/M ) =
N
(cid:89)
n=1
1
s/M − pn
= M N ·
N
(cid:89)
n=1
1
s − M pn
That is we obtain the ﬁlter with the poles M pn. Respectively, shifting the cutoﬀ
in the opposite direction by the same logarithmic amount, we have
G(M s) =
N
(cid:89)
n=1
1
M s − pn
= M −N ·
N
(cid:89)
n=1
1
s − M −1pn
That is we obtain the ﬁlter with the poles M −1pn.
Since for each pn there is pn(cid:48) = 1/pn, for each M pn there is M −1pn(cid:48) =
1/M pn. That is, the poles of G(s/M ) are mutually reciprocal with the poles of
G(M s) and we can construct
H(s) =
G(s/M )
G(M s)
(10.6)
By construction the poles of G(M s) are the zeros of H(s) and the poles of
G(s/M ) are the poles of H(s). Thus the poles of H(s) are reciprocal to its
zeros and vice versa. However generally |H(j)| (cid:54)= 1 (a little bit later we’ll show
that |H(j)| = M N ). This means that H(s) is not a tilting ﬁlter, but is related
to the tilting ﬁlter by some factor. Noticing that H(0) = G(0)/G(0) = 1, we
conclude that H(s) must be a kind of high-shelving ﬁlter. The conversion to
the tilting ﬁlter is trivial: we can simply divide the result by |H(j)|.
The conversion to the low-shelving ﬁlter looks more complicated, since ap-
parently G(∞) = 0 and we have a 0/0 uncertainty evaluating H(∞). However
we can notice that at s → ∞ we have G(s) ∼ s−N , G(s/M ) ∼ M N s−N and
G(M s) ∼ M −N s−N , thus H(s) ∼ M 2N , that is simply H(∞) = M 2N . We
therefore obtain the low-shelving ﬁlter from the high-shelving one by dividing
by M 2N .
We can also obtain the explicit expression for the value of |H(j)|, where we
can simply use the symmetries of the amplitude response. Since H(s)/|H(j)| is
2The author has learned the approach of constructing a shelving filter as a ratio of two
lowpasses from Teemu Voipio.
10.2. SHELVING AND TILTING FILTERS
413
a tilting ﬁlter, it must have mutually reciprocal amplitude responses at ω = 0
and ω = ∞, that is
(cid:12)
(cid:12)
(cid:12)
(cid:12)
H(0)
|H(j)|
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
(cid:12)
(cid:12)
(cid:12)
(cid:12)
|H(j)|
H(∞)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
from where |H(j)|2 = |H(0)| · |H(∞)| = M 2N and |H(j)| = M N . Thus the
tilting ﬁlter is obtained from the high-shelving one by dividing by M N .
Therefore we have
HHS(s) =
G(s/M )
G(M s)
Htilt(s) = M −N ·
HLS(s) = M −2N ·
G(s/M )
G(M s)
G(s/M )
G(M s)
(10.7a)
(10.7b)
(10.7c)
for the high-shelving, tilting and low-shelving ﬁlter respectively. From the values
H(0), |H(j)|, H(∞) obtained earlier for the high-shelving ﬁlter we thus obtain:
HHS(0) = 1
Htilt(0) = M −N
HLS(0) = M −2N
|HHS(j)| = M N
|Htilt(j)| = 1
|HLS(j)| = M −N
HHS(∞) = M 2N
Htilt(∞) = M N
HLS(∞) = 1
(10.8)
Apparently M can be greater or smaller than 1, corresponding to increasing or
decreasing of the signal level in the respective range. Since M and 1/M are
ﬁlter cutoﬀ factors, M must be positive.
The ﬁlters constructed by the lowpass ratio approach satisfy the symmetry
(10.5), however we know little about the shapes of their amplitude responses.
These shapes can be arbitrary odd functions (if seen in the logarithmic scale),
whereas we would like to obtain the shapes at least resembling those in Figs. 10.1
and 10.2.
Also, apparently the lowpass ratio approach can be easily applied to a But-
terworth G(s), since Butterworth (unit-cutoﬀ) ﬁlters have poles on the unit
circle. On the other hand, while EMQF ﬁlters also have poles on the unit circle,
they don’t have only poles, but also zeros, therefore this method is not directly
applicable to EMQF ﬁlters.3 In order to have a better control of the amplitude
responses and to be able to build tilting and shelving ﬁlters based on EMQF
ﬁlter, we will need to address the problem from a diﬀerent angle.
Construction by mixing
We have already made some attempts of constructing a low-shelving ﬁlters by
mixing the lowpass signal with the input signal, which weren’t too successful.
Instead we could attempt the same mixing in terms of squared amplitude re-
sponse, in which case we at least would not have the eﬀects of the phase response
3It would have been okay, if all zeros of G(s) were at the origin, since in this case the zeros
of G(s/M ) and G(M s) would be also at the origin and therefore would cancel each other.
Particularly, we could have used Butterworth highpass filters in (10.6), but this wouldn’t have
produced any new results compared to Butterworth lowpasses.
414
CHAPTER 10. SPECIAL FILTER TYPES
interfering. Also, rather that constructing a low-shelving ﬁlter, we shall attempt
to construct a tilting ﬁlter, in which case it is easier to express the symmetry
requirement (10.5).
Suppose G(s) is deﬁned by
|G(jω)|2 =
1
1 + f 2(ω)
We construct the tilting squared amplitude response by mixing |G(jω)|2 with
the squared “amplitude response of the input signal”, which is simply 1:
|H(jω)|2 = a2 +
b2
1 + f 2(ω)
=
α2 + β2f 2(ω)
1 + f 2(ω)
where a2 and b2 (or, equivalently, α2 and β2) denote the unknown positive
mixing coeﬃcients. We wish |H(jω)|2 to satisfy (10.5).
Assuming a lowpass f (x), that is f (x) → 0 for x → 0 and f (x) → ∞ for
x → ∞, we notice that |H(0)|2 = α2 and |H(∞)|2 = β2, therefore (10.5) can
be attained only at α2β2 = 1 and we can drop one of these variables obtaining:
|H(jω)|2 =
β−2 + β2f 2(ω)
1 + f 2(ω)
(10.9)
However there apparently are additional restrictions on f (x) which ensure that
(10.5) holds for any ω and not just for ω = 0 and ω = ∞. To ﬁnd these
restrictions let’s substitute (10.9) into (10.5):
β−2 + β2f 2(1/ω)
1 + f 2(1/ω)
=
1 + f 2(ω)
β−2 + β2f 2(ω)
β−4 + f 2(1/ω) + f 2(ω) + β4f 2(1/ω)f (ω) =
= 1 + f 2(1/ω) + f 2(ω) + f 2(1/ω)f (ω)
1 − β−4 = (β4 − 1)f 2(1/ω)f 2(ω)
(β4 − 1)f 2(1/ω)f 2(ω) =
β4 − 1
β4
and
f 2(1/ω)f 2(ω) = β−4
(10.10)
The equation (10.10) thereby ensures that (10.5) will hold.
Let ¯f (ω) = βf (ω), or f (ω) = β−1 ¯f (ω). Then (10.10) becomes4
¯f (1/ω) ¯f (ω) = 1
while (10.9) becomes
|H(jω)|2 =
β−2 + ¯f 2(ω)
1 + β−2 ¯f 2(ω)
= β−2 ·
1 + β2 ¯f 2(ω)
1 + β−2 ¯f 2(ω)
(10.11)
(10.12)
4The other option ¯f (1/ω) ¯f (ω) = −1 implied by (10.10) implies ¯f 2(1) = −1, therefore we
ignore it.
10.2. SHELVING AND TILTING FILTERS
415
That is, we simply want ¯f (ω) satisfying (10.11). Then f (ω) = β−1 ¯f (ω) (for
any arbitrarily picked β) will satisfy (10.10) and respectively H(s) will satisfy
(10.5).
Comparing the above to the lowpass-ratio approach to the construction of
the tilting ﬁlters, that is comparing (10.12) to (10.6) we notice obvious similar-
ities. Essentially (10.12) is a ratio of two lowpasses β−2H1/H2:
|H1(jω)|2 =
1
1 + β−2 ¯f 2(ω)
|H2(jω)|2 =
1
1 + β2 ¯f 2(ω)
with an additional gain of β−2, which occurs since this is a tilting rather than
high-shelving ﬁlter.
Given a Butterworth f (ω) = ωN , we have f (ω/M ) = M −N f (ω), that is
the cutoﬀ substitution ω ← ω/M is equivalent to choosing β = M N , in which
case (10.12) means essentially the same as (10.6). The diﬀerence appears in the
EMQF case where f (ω/M ) (cid:54)= M −N f (ω).
The zeros of the EMQF ﬁlter would have been exactly the problem in the
case of (10.6), since the cutoﬀ substitution also shifts the zeros, and the zeros
of G(s/M ) do not match the zeros of G(M s), respectively they cannot cancel
each other in H(s) and would have resulted in zero amplitude response at the
zeros of the numerator and in inﬁnite amplitude response at the zeros of the
denominator. On the other hand, in (10.12), where the EMQF zeros correspond
to the poles of ¯f , the poles of ¯f will result in identical zeros of the numerator
and of the denominator of |H(jω)|2, thus they will cancel each other. In that
sense, the approach of (10.12) is more general than the one of (10.6).
We can also notice that the right-hand side of (10.12) monotonically maps
the range [0, +∞] of ˜f 2 onto [β−2, β2] if |β| > 1 and onto [β2, β−2] if 0 < |β| < 1.
Since negating β doesn’t have any eﬀect on (10.12), therefore we can restrict β
to β > 0, in which case we can say |Htilt(jω)| is varying between β−1 and β.
Obviously, (10.12) results in
|HHS(jω)|2 =
1 + β2 ¯f 2(ω)
1 + β−2 ¯f 2(ω)
|Htilt(jω)|2 = β−2 ·
|HLS(jω)|2 = β−4 ·
1 + β2 ¯f 2(ω)
1 + β−2 ¯f 2(ω)
1 + β2 ¯f 2(ω)
1 + β−2 ¯f 2(ω)
(10.13a)
(10.13b)
(10.13c)
The values at the key points respectively are (under the restriction β > 0)
|HHS(0)| = 1
|Htilt(0)| = β−1
|HLS(0)| = β−2
|HHS(j)| = β
|Htilt(j)| = 1
|HLS(j)| = β−1
|HHS|(∞) = β2
|Htilt|(∞) = β
|HLS|(∞) = 1
(10.14)
where we also assume that ¯f (0) = 0 and ¯f (∞) = ∞. In the EMQF case, where
¯f = ¯RN this is not true for even N , and we need to understand (10.14) as
referring to the points where ¯f (ω) = 0 and ¯f (ω) = ∞ instead (which we didn’t
explicitly write in (10.14) for the sake of keeping the notation short).
As with M in the lowpass ratio approach, β can be greater than 1 or smaller
than 1.
416
CHAPTER 10. SPECIAL FILTER TYPES
10.3 Fixed-slope shelving
Using Butterworth ﬁlters as a basis for a shelving/tilting ﬁlter is compatible with
both lowpass ratio (10.6) and mixing (10.12) approaches, where both options
are giving equivalent results. For now we will continue the discussion in terms
of the lowpass ratio option (10.6).
We will be interested in shelving ﬁlters obtained from the Butterworth ﬁlters
of the 1st kind by (10.6). Noticing that (10.6) commutes with the Butterworth
transformation:
BN
(cid:21)
(cid:20) G(s/M )
G(M s)
=
BN [G(s/M )]
BN [G(M s)]
(10.15)
we can restrict our discussion to the shelving ﬁlters obtained by the application
of (10.6) to the 1st-order Butterworth lowpass. By (10.15) higher order shelving
ﬁlters will be simply Butterworth transformations of the 1st-order Butterworth
shelving ﬁlters, which is going to be covered in Section 10.5.
Since the 1st-order Butterworth lowpass coincides with the ordinary 1-pole
lowpass, we simply have
and respectively by (10.7)
G(s) =
1
1 + s
HHS(s) =
G(s/M )
G(M s)
=
1 + M s
1 + s/M
= M 2 ·
s + 1/M
s + M
Htilt(s) = M −1HHS(s) = M −1 ·
HLS(s) = M −1Htilt(s) = M −2 ·
1 + M s
1 + s/M
1 + M s
1 + s/M
=
=
M s + 1
s + M
s + 1/M
s + M
= M
s + 1/M
s + M
By (10.8)
HHS(0) = 1
Htilt(0) = M −1
HLS(0) = M −2
|HHS(j)| = M
|Htilt(j)| = 1
|HLS(j)| = M −1
HHS(∞) = M 2
Htilt(∞) = M
HLS(∞) = 1
Implementation
The 1st-order tilting ﬁlter can be implemented as a linear combination of the
lowpass and highpass signals:
Htilt(s) =
M s + 1
s + M
=
s + 1/M
s/M + 1
= M
s/M
s/M + 1
+ M −1
1
s/M + 1
=
= M −1 · HLP(s) + M · HHP(s)
where the cutoﬀ of the 1-pole multimode is at ω = M . The low- and high-
shelving ﬁlters can be obtained from the above mixture by a division a or mul-
tiplication by M :
HHS(s) = M · Htilt(s) = HLP(s) + M 2 · HHP(s)
HLS(s) = M −1 · Htilt(s) = M −2 · HLP(s) + HHP(s)k
10.3. FIXED-SLOPE SHELVING
417
Amplitude response
The example amplitude responses of the 1-pole tilting ﬁlter are presented in
Fig. 10.3, where the formal “cutoﬀ” frequency is denoted as ωmid, being the
middle frequency of the tilting.
Shelving
band
Shelving
band
Transition
band
|H(jω)|, dB
12
+6
0
-6
-12
ωmid/8
ωmid
8ωmid
ω
Figure 10.3: Amplitude responses of a 1-pole tilting ﬁlter for M > 1
(solid) and M < 1 (dashed).
Since the amplitude response of the tilting ﬁlter neither decreases to zero
anywhere, nor does it have a range where it is approximately unity, we can’t
deﬁne pass- and stop-bands. Instead we can refer to the bands on the left and
on the right, where the amplitude response is almost constant, as shelving bands.
The band in the middle where the amplitude response is varying can be referred
to as transition band, as usual.
On the other hand, for low- and high-shelving ﬁlters we can deﬁne one of the
bands, where the amplitude response is approximately unity, as the passband
(Figs. 10.4, 10.5).
Phase response
The representation of the shelving ﬁlter as a ratio of two lowpasses with cutoﬀs
M and M −1 allows an intuitive derivation of the tilting ﬁlter’s phase response,
the latter being equal to the diﬀerence of the lowpass phase responses:
arg HHS(s) = arg
= arg
1 + M s
1 + s/M
1
1 + s/M
= arg
− arg
1 + s/M −1
1 + s/M
1
1 + s/M −1
=
(s = jω)
(since both shelving ﬁlters and the tilting ﬁlter all have identical phase responses,
we picked the one with the most convenient transfer function).
Recalling how the phase response of a 1-pole lowpass looks (Fig. 2.5) we can
conclude that the biggest deviation of the tilting ﬁlter’s phase response from 0◦
CHAPTER 10. SPECIAL FILTER TYPES
Shelving
band
Transition
band
Passband
418
|H(jω)|, dB
+6
0
-6
-12
-18
ωmid/8
ωmid
8ωmid
ω
Figure 10.4: Amplitude responses of a 1-pole low-shelving ﬁlter.
Passband
Transition
band
Shelving
band
|H(jω)|, dB
+6
0
-6
-12
-18
ωmid/8
ωmid
8ωmid
ω
Figure 10.5: Amplitude responses of a 1-pole high-shelving ﬁlter.
(potentially reaching almost ±90◦) should occur in the frequency band between
M and M −1, the deviation being positive if M > 1 > M −1 and negative if
M < 1 < M −1. Outside of this band the phase response cannot exceed ±45◦.
Fig. 10.6 illustrates. Notice that therefore the phase response is close to zero
outside of the transition region.
Transition band width
In order to roughly estimate the width of the transition band of the tilting ﬁlter’s
amplitude response we could divide the decibel diﬀerence in amplitude response
levels at ω → 0 and ω → ∞ by the derivative of the amplitude response at the
10.3. FIXED-SLOPE SHELVING
419
arg H(jω)
π/2
π/4
0
−π/4
−π/2
ωmid/4
ωmid
4ωmid
ω
Figure 10.6: Phase response of the 1-pole shelving/tilting ﬁlters
for M = 4 (positive) and for M = 1/4 (negative). Dashed curves
represent phase responses of the underlying 1-pole lowpasses at
cutoﬀs M and M −1.
middle of the transition band (Fig. 10.7).
|H(jω)|, dB
12
+6
0
-6
-12
ωmid/8
ωmid
8ωmid
ω
Figure 10.7: Estimation of the transition bandwidth of the 1-pole
tilting ﬁlter by approximating the amplitude response by a broken
line tangential to the amplitude response at the middle frequency.
Writing out the derivative of the amplitude response in the logarithmic fre-
quency and amplitude scales (where we use natural logarithms to simplify the
math), we obtain
d
dx
ln |Htilt(jex)|
(cid:12)
(cid:12)
(cid:12)
(cid:12)x=0
= |Htilt(jex)|−1
(cid:12)
(cid:12)
(cid:12)
(cid:12)x=0
·
d
dx
(cid:12)
(cid:12)
|Htilt(jex)|
(cid:12)
(cid:12)x=0
=
420
CHAPTER 10. SPECIAL FILTER TYPES
=
=
=
d
2dx
(cid:19)
|Htilt(jex)| =
d
dx
d
2dx
2M 2(1 + M 2) − 2(M 2 + 1)
2(1 + M 2)2
(cid:18) M 2e2x + 1
e2x + M 2
=
2
d
2dx
|Htilt(jex)|2 =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
2M 2e2x(e2x + M 2) − 2e2x(M 2e2x + 1)
2(e2x + M 2)2
jM ex + 1
jex + M
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
(cid:12)
(cid:12)
(cid:12)
(cid:12)x=0
=
=
M 2 − 1
M 2 + 1
=
M − M −1
M + M −1
(where we have assumed x = 0 throughout the entire transformation chain).
The logarithmic amplitude diﬀerence is ln M − ln M −1 = 2 ln M and thus the
bandwidth in terms of natural logarithmic scale is the ratio of that diﬀerence
and the derivative at x = 0:
∆ln = 2 ln M ·
M + M −1
M − M −1
Introducing the natural-logarithmic amplitude boost m = ln M , we rewrite the
above as
∆ln = 2m ·
where
em + e−m
em − e−m = 2 ·
m
tanh m
=
2
tanhc m
tanhc m =
tanh m
m
is the “cardinal hyperbolic tangent” function, introduced similarly to the more
commonly known cardinal sine function sinc x = sin x
x .
Introducing the decibel diﬀerence betwen the right and left “shelves”
GdB = 20 log10 M 2
we can switch the amplitude scale from natural logarithmic to decibel:
Then
M = em
M 2 = 10GdB/20
2m = ln 10GdB/20 = GdB/20 · ln 10
m = GdB/40 · ln 10 ≈ 0.0576 · GdB
∆ln ≈
2
tanhc (0.0576 · GdB)
Switching from the natural logarithmic bandwidth to the octave bandwidth we
have
eΔln = 2Δoct
∆ln = ∆oct · ln 2
and we have obtained the octave bandwidth formula:
∆oct ≈
2
ln 2 · tanhc (0.0576 · GdB)
≈
2.89
tanhc (0.0576 · GdB)
The graph of the dependency is plotted in Fig. 10.8. At GdB = 0 we have
∆oct ≈ 2.89. At |GdB| → ∞ we have ∆oct ∼ GdB/6, that is the bandwidth
is growing proportionally to the decibel boost.5 Since the shelving boosts are
typically within the range of ±12dB, or maybe ±18dB, we could say that the
typical transition band width of the titling ﬁlter is roughly 3 octaves.
5It’s not difficult to realize that the 6 in the denominator is 1-pole lowpass filter’s rolloff
of 6dB/oct.
10.4. VARIABLE-SLOPE SHELVING
421
∆oct
6
4
2
−40
−20
0
+20
+40
GdB
Figure 10.8: Transition bandwidth (estimated) as a function of the
total decibel boost.
10.4 Variable-slope shelving
With shelving ﬁlters based on Butterworth ﬁlters of the 1st kind we didn’t have
any control over the steepness of the transition slope, or, respectively over the
transition band width. In order to introduce that kind of control we can use
Butterworth ﬁlters of the 2nd kind.
The commutativity relation (10.15) still applies, since it’s independent of
whether the involved ﬁlters are 1st or 2nd kind Butterworth, and we can restrict
our discussion to the 2-pole shelving ﬁlters. Shelving ﬁlters of higher (even)
orders can be obtained from those by Butterworth transformation, which is
going to be covered in Section 10.5.
A generic unit-cutoﬀ 2-pole lowpass ﬁlter
G(s) =
1
s2 + 2Rs + 1
(10.16)
has its poles on the unit circle, no zeros and unity gain at ω = 0, thus the
requirements of the lowpass ratio approach are fulﬁlled and we can obtain the
respective shelving ﬁlters by (10.7):
HHS(s) =
G(s/M )
G(M s)
=
M 2s2 + 2RM s + 1
s2/M 2 + 2Rs/M + 1
= M 2 ·
M 2s2 + 2RM s + 1
s2 + 2RM s + M 2
Htilt(s) = M −2HHS(s) =
M 2s2 + 2RM s + 1
s2 + 2RM s + M 2
HLS(s) = M −2Htilt(s) = M −2 ·
M 2s2 + 2RM s + 1
s2 + 2RM s + M 2 =
s2 + 2Rs/M + 1/M 2
s2 + 2RM s + M 2
where by (10.8)
HHS(0) = 1
Htilt(0) = M −2
HLS(0) = M −4
|HHS(j)| = M 2
|Htilt(j)| = 1
|HLS(j)| = M −2
HHS(∞) = M 4
Htilt(∞) = M 2
HLS(∞) = 1
422
CHAPTER 10. SPECIAL FILTER TYPES
Implementation
In order to construct an implementation of the tilting ﬁlter, we simply express
its transfer function in terms of the SVF modes:
Htilt(s) =
s2 + 2R(s/M ) + 1/M 2
(s/M )2 + 2R(s/M ) + 1
M 2s2 + 2RM s + 1
s2 + 2RM s + M 2 =
M 2(s/M )2 + 2R(s/M ) + 1/M 2
(s/M )2 + 2R(s/M ) + 1
= M −2HLP(s) + HBP1(s) + M 2HHP(s)
=
=
=
where the cutoﬀ of the multimode SVF is ωc = M (notice that we used the
normalized bandpass mode instead of the ordinary bandpass). Respectively
HHS(s) = M 2Htilt(s) = HLP(s) + M 2HBP1(s) + M 4HHP(s)
HLS(s) = M −2Htilt(s) = M −4HLP(s) + M −2HBP1(s) + HHP(s)
Amplitude and phase response
Notably, G(s) deﬁned by (10.16) cannot be conveniently expressed in terms of
(9.18), since
|G(jω)|2 =
1
(ω2 − 1)2 + 4R2ω2 =
1
4R2(1 − R2)
·
1
(cid:18) ω2 + 2R2 − 1
√
1 − R2
2R
(cid:19)2
1 +
Respectively, (10.12) doesn’t apply and we cannot use the associated interpreta-
tion to reason about the shelving amplitude response shapes obtained from G(s).
However, we could notice that at R = 1 the ﬁlter G(s) turns into a squared 1st-
order Butterworth, while at R = 1/
2 it turns into a 2nd-order Butterworth of
the 1st kind, therefore we can apply the results of Section 10.3 concluding that
at least at these values of R we should expect to obtain a resonable shelving
shape.
√
The family of amplitude responses of a 2-pole tilting ﬁlter for various R is
shown in Fig. 10.9. One can see in the picture that R controls the slope, or
equivalently, the width of the transition band, however only a small range of
R generates “reasonable” tilting curves. We’ll analyse this topic in detail a bit
later.
The phase response expressed in terms of a lowpass ratio gives
arg HHS(s) = arg
M 2s2 + 2RM s + 1
s2/M 2 + 2Rs/M + 1
= arg
s2/M −2 + 2Rs/M −1 + 1
s2/M 2 + 2Rs/M + 1
=
= arg
1
(s/M )2 + 2R(s/M ) + 1
− arg
1
(s/M −1)2 + 2R(s/M −1) + 1
(where s = jω). Recalling the 2-pole lowpass phase response (Fig. 4.6) we can
conclude that the biggest deviation of the 2-pole tilting ﬁlter’s phase response
from 0◦ (potentially reaching almost ±180◦) should occur in the frequency band
between M and M −1, the deviation being positive if M > 1 > M −1 and negative
if M < 1 < M −1. Outside of this band the phase response cannot exceed ±90◦.
Fig. 10.10 illustrates. Notice that thus the phase response is close to zero outside
of the transition region.
10.4. VARIABLE-SLOPE SHELVING
423
|H(jω)|, dB
+12
+6
0
-6
-12
R (cid:28) 1
R = 1/
√
2
R (cid:29) 1
R = (M + M −1)/2
ωmid/8
ωmid
8ωmid
ω
Figure 10.9: Amplitude responses of a 2-pole tilting ﬁlter for
M 2 = 2 and various R.
arg H(jω)
π
π/2
0
−π/2
−π
ωmid/4
ωmid
4ωmid
ω
Figure 10.10: Phase response of the 2-pole tilting ﬁlter for M = 4
(positive) and for M = 1/4 (negative). Damping R = 1/4. Dashed
curves represent phase responses of the underlying 2-pole lowpasses
at cutoﬀs M and M −1.
Steepness control
As one could notice from Fig. 10.9, the damping parameter R aﬀects the steep-
ness of the amplitude response slope at ω = 1. Let’s analyse it in more detail.
First, we write Htilt(s) as
Htilt(s) =
M 2s2 + 2RM s + 1
s2 + 2RM s + M 2 =
M 2s + 2RM + 1/s
s + 2RM + M 2/s
=
G(s)
G(1/s)
424
where
CHAPTER 10. SPECIAL FILTER TYPES
G(s) = M 2s + 2RM + 1/s
Then, considering the derivative of the fully logarithmic-scale amplitude re-
sponse at ω = 1, we obtain (assuming x = 0 thoughout the entire transformation
chain):
d
dx
(cid:12)
(cid:12)
ln |Htilt(jex)|
(cid:12)
(cid:12)
(cid:12)x=0
(cid:0)ln |G(jex)| − ln (cid:12)
d
dx
=
=
ln
|G(jex)|
|G(−je−x)|
=
d
dx
ln
|G(jex)|
|G(je−x)|
=
(cid:1) = 2
d
dx
ln |G(jex)| =
d
dx
ln |G(jex)|2 =
(cid:12)G(je−x)(cid:12)
(cid:12)
d
dx
d
|G(jex)|2
dx
|G(jex)|2 =
d
(4R2M 2 + (M 2ex − e−x)2)
dx
d
dx
4R2M 2 + (M 2 − 1)2
|G(j)|2
|jM 2ex + 2RM − je−x|2
=
d
dx
(M 4e2x − 2M + e−2x)
4R2M 2 + (M 2 − 1)2 =
=
2M 4e2x − 2e−2x
4R2M 2 + (M 2 − 1)2 =
2(M 2 − M −2)
4R2 + (M − M −1)2
(10.17)
=
=
=
The maximum possible value is attained at R = 0 and is equal to
d
dx
ln |Htilt(jex)|
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)x=0
=
2(M 2 − M −2)
(M − M −1)2 = 2 ·
M + M −1
M − M −1 < ∞
for M (cid:54)= 1
That is, we can’t reach inﬁnite steepness. Further, as one can see from Fig. 10.9,
for R → 0 the amplitude response gets a peak and a dip, which are generally
undesired for a shelving EQ.
On the other hand, given a suﬃciently large R, we can attain arbitrarily
small steepness. However, for R ≥ 1 the ﬁlter falls apart into a product of two
1-pole tilting ﬁlters.6 As R grows, the 1-pole cutoﬀs get further apart and one
can see the two separate “tilting inﬂection points“ in the amplitude response
(Fig. 10.9).
We need therefore to restrict R to “a reasonable range”. But how do we
deﬁne this range? Let’s analyse several characteristic values of R.
At R = 1 we have a two times vertically stretched (in the decibel scale)
amplitude response of the 1-pole tilting ﬁlter with the same cutoﬀ ωc = M :
M 2s2 + 2M s + 1
s2 + 2M s + M 2 =
√
(cid:19)2
(cid:18) M s + 1
s + M
It should be no surprise that at R = 1/
of the 1-pole tilting ﬁlter with cutoﬀ ωc = M 2:
2 we obtain a Butterworth transform
√
M 2s2 +
√
s2 +
2 · M s + 1
2 · M s + M 2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
2 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)s=jω
=
(1 − M 2ω2)2 + 2M 2ω2
(M 2 − ω2)2s2 + 2M 2ω2 =
6This can be derived from the fact that in this case H(s) has real poles and zeros which are
mutually reciprocal. Thus, each such reciprocal pole/zero pair makes up a 1-pole tilting filter.
The gain M 2 of the tilting 2-pole filter is distributed into two 1-pole tilting filter’s gains, each
equal to M .
10.5. HIGHER-ORDER SHELVING
425
=
1 + M 4ω4
M 4 + ω4 =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
M 2 · jω2 + 1
jω2 + M 2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
(cid:12)
(cid:12)
(cid:12)
(cid:12)
M 2s + 1
s + M 2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)s=jω2
We can also obtain the response identical to the response of the just mentioned
1-pole tilting ﬁlter with cutoﬀ ωc = M 2 by combining such 1-pole tilting ﬁlter
with a “unit-gain” (fully transparent) tilting ﬁlter (s + 1)/(s + 1):
Htilt(s) =
M 2s + 1
s + M 2 =
M 2s + 1
s + M 2 ·
s + 1
s + 1
=
M 2s2 + (M 2 + 1)s + 1
s2 + (M 2 + 1)s + M 2 =
M 2s2 +
=
s2 +
M 2 + 1
M
M 2 + 1
M
· M s + 1
· M s + M 2
=
M 2s2 + (M + M −1) · M s + 1
s2 + (M + M −1) · M s + M 2
Therefore, such response is attained at R = (M + M −1)/2 ≥ 1.
Thus we are having two good candidates for the boundaries of the “reason-
able range of R”. One boundary can be at R = (M + M −1)/2, corresponding
to the amplitude response of the 1-pole tilting ﬁlter with cutoﬀ ωc = M 2, the
other boundary is at R = 1/
2, corresponding to the same response shrunk
horizontally two times.7 The steepness at ω = 1 therefore varies by a factor of
2 within that range, which also can be veriﬁed explicitly:
√
d
dx
(cid:12)
(cid:12)
ln |Htilt(jex)|
(cid:12)
(cid:12)x=0, R=1/
√
2
d
dx
(cid:12)
(cid:12)
ln |Htilt(jex)|
(cid:12)
(cid:12)x=0, R=(M +M −1)/2
=
=
=
2(M 2 − M −2)
2 + (M − M −1)2 ·
(M + M −1)2 + (M − M −1)2
2 + (M − M −1)2
(M + M −1)2 + (M − M −1)2
2(M 2 − M −2)
=
=
2(M 2 + M −2)
M 2 + M −2 = 2
These boundary responses of the “reasonable range of R” can be found among
the responses shown by Fig. 10.9.
10.5 Higher-order shelving
Butterworth shelving of the 1st kind
Given a 1-pole tilting ﬁlter:
Htilt(s) =
M s + 1
s + M
= M
s + M −1
s + M
7Since the 2-pole tilting filter is essentially a ratio of two 2-pole lowpass filters with mu-
2,
tually reciprocal cutoffs, and since these lowpasses obtain a resonance peak at R < 1/
2 or possibly starting from a
it is intuitively clear that either immediately below R = 1/
slightly lower boundary these peaks will show up in amplitude response of the tilting filter.
In Section 10.7 we will establish that at R < 1
2 the filter will go into elliptic range, and it
will follow that the elliptic ripples (showing up as resonance peaks for 2nd-order filters) will
appear immediately below R = 1/
√
√
√
√
2.
426
CHAPTER 10. SPECIAL FILTER TYPES
we have reciprocal “cutoﬀs” in the numerator and the denominator. Respec-
tively the zero is reciprocal to the pole. According to (8.12c) and (8.12d), this
property is preserved by the Butterworth transformation. The dampings of
the poles and zeros obtained after the Butterworth transformation of the 1st
kind depend solely on the transformation order and thus are identical in the
numerator and denominator:
B [Htilt(s)] = M ·
(cid:18) s + M −1/N
s + M 1/N
(cid:19)N ∧1
(cid:89)
·
n
s2 + 2RnM −1/N s + (M −1/N )2
s2 + 2RnM 1/N s + (M 1/N )2
=
(cid:18)
=
M 1/N s + M −1/N
s + M 1/N
(cid:19)N ∧1
·
(M 1/N )2 s2 + 2RnM −1/N s + (M −1/N )2
s2 + 2RnM 1/N s + (M 1/N )2
(cid:89)
n
Therefore we obtain a serial chain of 1- and 2-pole tilting ﬁlters. The low-
and high-shelving ﬁlters are transformed similarly. Alternatively we can simply
reuse the obtained Butterworth transformation of the tilting ﬁlter, multiplying
or dividing it by the factor M .
Notice that the factor being M rather than M N is a kind of a notational
diﬀerence. After the Butterworth transformation of N -th order the original
change of cutoﬀ by the M factor will turn into a change of cutoﬀ by the M 1/N
factor. Raising M 1/N to the N -th power (according to (10.8)) to obtain the
multiplication factors gives M .
Butterworth shelving of the 2nd kind
Remember that the “cutoﬀs” of the numerator and denominator of a 2-pole
tilting ﬁlter are mutually reciprocal, while the “dampings” are equal:
Htilt(s) =
M 2s2 + 2RM s + 1
s2 + 2RM s + M 2 = M 2 ·
so the numerator “cutoﬀ” is M −1 and the denominator “cutoﬀ” is M .
s2 + 2RM −1s + M −2
s2 + 2RM s + M 2
By using the Butterworth transform cutoﬀ property (8.12c) we obtain that
B [Htilt(s)] must have the following form:
B [Htilt(s)] = M 2 ·
N
(cid:89)
n=1
s2 + 2RnM −1/N s + M −2/N
s2 + 2RnM 1/N s + M 2/N
=
=
N
(cid:89)
n=1
M 2/N s2 + 2RnM −1/N s + M −2/N
s2 + 2RnM 1/N s + M 2/N
which is in agreement with the reciprocal cutoﬀ preservation property of the
Butterworth transformation. Thus we have obtained a serial chain of 2-pole
tilting ﬁlters with numerator “cutoﬀ” M −1/N and denominator “cutoﬀ” M 1/N .
The low- and high-shelving ﬁlters can be transformed similarly or obtained from
the transformed tilting ﬁlter.
Wide-range slope
Let H2(s) be a 2-pole tilting ﬁlter. In the discussion of the 2-pole shelving ﬁlters
we mentioned that such ﬁlter smoothly varies its response from the one of a 1-
pole shelving ﬁlter H1(s) to B2 [H1(s)] as R varies from R1 = (M + M −1)/2 to
10.5. HIGHER-ORDER SHELVING
427
√
R2 = 1/
2:
H2(s)
H2(s)
(cid:12)
(cid:12)
(cid:12)
(cid:12)R=R1
(cid:12)
(cid:12)
(cid:12)
(cid:12)R=R2
= H1(s)
= B2 [H1(s)]
(10.18a)
(10.18b)
where the steepness of the amplitude response respectively doubles on that
range.
Now let R initially be equal to R1 and imagine we have smoothly decreased
it to R = R2. Suppose at this moment we swapped the 2-pole tilting ﬁlter
H2(s) with a 4-pole tilting ﬁlter H4(s) = B2 [H2(s)] simultaneously resetting
the damping back to R = R1. Using (10.18) we have
(cid:34)
(cid:35)
B2
(cid:12)
(cid:12)
H2(s)
(cid:12)
(cid:12)R=R1
(cid:12)
(cid:12)
= H2(s)
(cid:12)
(cid:12)R=R2
and thus this swapping doesn’t change the frequency response of the ﬁlter. Now
we can vary R from R1 to R2 again to smoothly double the amplitude response
once more.
So it seems we have found a way to vary the steepness of the tilting ﬁlter by
a factor of 4 without getting the unwanted amplitude response artifacts which
occur in a 2-pole tilting ﬁlter on an excessive range of R. However the problem
is the swapping of H2(s) with H4(s) and back. In mathematical notation the
swapping is seamless, because the transfer function doesn’t change during the
swap. In a real implementation however the swapping means replacing one ﬁlter
structure with another and this will generate a transient, unless the internal
states of the two ﬁlters are perfectly matched at the moment of the swapping.
Recall, however, that at R = R1 the 2-pole H2(s) can be decomposed into
two 1-poles, where the second of the 1-poles is fully transparent:
(cid:12)
(cid:12)
(cid:12)
(cid:12)R=R1
s + 1
s + 1
= H1(s) ·
H2(s)
Applying Butterworth transformation of order N = 2 to both sides we obtain
(cid:20) s + 1
s + 1
= B2 [H1(s)] · B2
= H2(s)
2s + 1
2s + 1
s2 +
s2 +
H4(s)
√
√
(cid:12)
(cid:12)
(cid:12)
(cid:12)R=R1
(cid:12)
(cid:12)
(cid:12)
(cid:12)R=R2
(cid:21)
·
This means that we could have a 4-pole ﬁlter
H4(s) = H2a(s) · H2b(s)
(where H2a(s) and H2b(s) are 2-pole sections) all the time. As long as we are
interested in a 2-pole response H2(s) we let
H2a(s) = H2(s)
s2 +
s2 +
H2b(s) =
√
√
2s + 1
2s + 1
H2(s)(cid:12)
(cid:104)
As we are switching from H2(s)(cid:12)
to B2
neither of the sections
H2a(s) H2b(s) is changed. From this point on we can further change R from R1
(cid:12)R=R2
(cid:12)R=R1
(cid:105)
428
CHAPTER 10. SPECIAL FILTER TYPES
to R2 updating the coeﬃcients of H2a(s) H2b(s) according to H2a(s) · H2b(s) =
B2 [H2(s)].
This procedure can be repeated again, that is, having reached R = R2 for
the 4-pole shelving response, we can replace H4(s) by
H8(s) = B2 [H4(s)] = B4 [H2(s)]
simultaneously resetting R to R = R1. The idea is the same, we decompose
H8(s) into
H8(s) = H4a(s) · H4b(s)
and we let
H4a(s) = H4(s)
H4b(s) = B2
(cid:34)
s2 +
s2 +
(cid:35)
√
√
2s + 1
2s + 1
= B4
(cid:21)
(cid:20) s + 1
s + 1
until the point of the switching from H4(s) to H8(s) where we start having
H4a(s) = B2 [H2a(s)]
H4b(s) = B2 [H2b(s)]
Of course the same procedure can be further repeated as many times as
desired (keeping in mind that the order of the resulting ﬁlter grows exponen-
tially). Thus we can choose some power of 2 as a maximum desired ﬁlter order
and switch this ﬁlter’s response from H2(s) to H4(s) to H8(s) etc. each time R
reaches R2. The steepness of the amplitude response thereby smoothly varies
by a factor equal to the ﬁlter’s order.8
Another way of looking at this is noticing that, as the response steepness
grows, we are traversing the responses deﬁned by H1(s), B2 [H1(s)], B4 [H1(s)],
etc. Therefore we can consider this as if it was a smooth variation of the
Butterworth tilting ﬁlter’s order.9
10.6 Band shelving
The 2-pole bandshelving ﬁlter can be easily obtained by applying the LP to
BP substitution to the 1-pole low-shelving ﬁlter. Or, we can apply the LP to
BP substitution to the 1-pole tilting ﬁlter (obtaining a kind of a “band-tilting”
ﬁlter) and multiply the result by the necessary gain factor.
Let’s do the latter. Given
Htilt(s) =
M s + 1
s + M
=
s + 1/M
s/M + 1
8Note that the relative steepness κ of the amplitude response thereby provides a natural
way to control the steepness variation, where κ = k/k1, where k is the actual derivative of the
amplitude response at the midpoint and k1 is the same derivative for H1(s) (for the currently
chosen tilting amount M ).
9Unfortunately there is no 100% generalization of this process for lowpass, highpass or
bandpass filters, since the rolloff of these filter types is fixed to an integer multiple of 6dB/oct
and can’t be varied in a smooth way.
10.6. BAND SHELVING
429
we perform the substitution
s ←
1
2R
(cid:0)s + s−1(cid:1)
obtaining
H(s) =
=
1
(cid:0)s + s−1(cid:1) + 1/M
2R
1
2RM
M s2 + M −1 · 2RM s + M
s2 + 2RM s + 1
(s + s−1) + 1
=
M s + 2R + M s−1
s + 2RM + s−1 =
M s2 + 2Rs + M
s2 + 2RM s + 1
=
= M HLP(s) + M −1HBP1(s) + M HHP(s)
where the SVF damping is equal to RM , where R is determined by the band-
width of the LP to BP transformation. The obtained ﬁlter could be referred to
as “band-tilting” ﬁlter (Fig. 10.11).
|H(jω)|, dB
12
+6
0
-6
-12
ωc/8
ωc
8ωc
ω
Figure 10.11: Amplitude response of 2-pole band-tilting ﬁlter for
various M .
In order to turn this ﬁlter into the band-shelving ﬁlter, apparently, we have
to divide the response by M :10
H(s) =
s2 + M −2 · 2RM s + 1
s2 + 2RM s + 1
=
= HLP(s) + M −2HBP1(s) + HHP(s) = 1 + (M −2 − 1)HBP1(s)
(mind that the SVF damping is still being equal to RM). Thus, the 2-pole band-
shelving ﬁlter can be implemented by mixing the (normalized) bandpass signal
to the input signal. The amplitude response at the cutoﬀ is H(j) = M −2 which
thereby deﬁnes the shelving gain. The desired bandwidth of the shelving can be
10Alternatively, by multiplying the response by M we obtain a kind of “inverted band-
shelving” filter, where the shelving bands are to the left and to the right of the passband in
the middle.
430
CHAPTER 10. SPECIAL FILTER TYPES
achieved using the properties of the LP to BP substition, namely the formula
(4.20).
It is interesting to observe that the above band-shelving transfer function
can be rewritten as
H(s) =
s2 + 2RM −1s + 1
s2 + 2RM s + 1
(10.19)
that is we have a ratio of two ﬁlters with diﬀerent dampings RM and RM −1,
where the ﬁlters themselves could be lowpass, highpass or bandpass (the impor-
tant thing being that they have identical numerators, which then cancel each
other).
Band shelving of higher orders
The band-shelving Butterworth ﬁlter of the 2nd kind is obtained by applying
the Butterworth transformation to the 2-pole band-shelving ﬁlter (10.19):
H(s) =
s2 + 2RM −1s + 1
s2 + 2RM s + 1
Thus we have a ratio of two 2nd order polynomials both having unit cutoﬀ
but diﬀerent damping. Applying the Butterworth transformation we therefore
obtain a cascade of 2nd-order sections with unit cutoﬀ:
H (cid:48)(s) =
s2 + 2R(cid:48)
ns + 1
s2 + 2Rns + 1
(cid:89)
n
Apparently each such 2nd-order section is a 2-pole bandshelving ﬁlter with
the shelving boost and the bandwidth deﬁned by the parametrs Rn and R(cid:48)
n.
Fig. 10.12 shows the amplitude response.
|H(jω)|, dB
+12
+6
0
-6
ωc/8
ωc
8ωc
ω
Figure 10.12: 4th order band-shelving Butterworth ﬁlter of the 2nd
kind vs. 2nd order band-shelving ﬁlter (dashed line).
Another kind of band-shelving ﬁlter can be obtained by applying the LP
to BP substitution to a Butterworth low-shelving ﬁlter of the 2nd kind. A
useful feature of this approach is that by choosing the order of the low-shelving
ﬁlter (and thus choosing the steepness of the low-shelving ﬁlter’s slope) one can
choose the steepness of the slopes of the band-shelving ﬁlter.
10.7. ELLIPTIC SHELVING
431
10.7 Elliptic shelving
We have mentioned that the mixing approach of (10.12) can be applied to
EMQF ﬁlters. Of all equations (10.13) it’s probably easiest to use (10.13a) to
construct a high-shelving ﬁlter, the other ﬁlters can be derived from it in a
trivial way. As (10.13a) is a monotonic mapping of the range [0, +∞] of ¯f 2
onto the range of |HHS(jω)| contained between 1 and β2, we are going to have
|HHS(jω)| smoothly varying from 1 to β2 as ¯RN varies from 0 to ∞, just with
some ripples in the pass and shelving bands.
Letting ¯f (ω) = ¯RN (ω) in (10.13a):
|HHS(jω)|2 =
1 + β2 ¯R2
1 + β−2 ¯R2
N (ω)
N (ω)
(10.20)
we obtain the amplitude response shown in Fig. 10.13. Notice that due to the
monotonic nature of the mapping (10.13a) the pass and shelving band ripples
do not oscillate around the reference gains 1 and β2 (corresponding to ¯RN = 0
and ¯RN = ∞), but are rather occurring “into the inside” of the range between
1 and β2.
|H(jω)|
β2
β
1
ωmid/8
ωmid
8ωmid
ω
Figure 10.13: Amplitude response of an elliptic high-shelving ﬁlter.
Horizontal dashed lines denote the reference gains 1 and β2.
Implementation
From (10.20) we obtain the pole and zero equations for HHS(s):
1 + β−2 ¯R2
1 + β2 ¯R2
N (ω) = 0
N (ω) = 0
(10.21a)
(10.21b)
where we ignore the poles of ¯RN , since they cancel each other within HHS(s)
anyway. The equations (10.21) are essentially identical to (9.152), where we let
432
CHAPTER 10. SPECIAL FILTER TYPES
λ = 1 and ε = β−1 or ε = β respectively.11
Having obtained the poles and zeros we can deﬁne the leading gain coeﬃcient
g of the cascade form (8.1) from the requirement
H(0) =
(cid:115)
1 + β2 ¯R2
1 + β−2 ¯R2
N (0)
N (0)
=
(cid:115)
1 + β2 (Re jN )2 ˜k
1 + β−2 (Re jN )2 ˜k
We could also use a simpler requirement:
|H(j)| = β
however we should mind the possibility of accidentally obtaining a 180◦ phase
response at ω = 0.
Control parameters
In order to compute the passband (ω (cid:28) 1) ripple amplitude, we can notice that
in the passband the value of ¯R2(ω) varies between 0 and ˜k. By (10.20) the
maximum deviation from the reference gain 1 will be at the gain equal to
(cid:115)
δ =
1 + β2˜k
1 + β−2˜k
(10.22)
thus in the passband |HHS(jω)| varies between 1 and δ. If β > 1 then δ > 1 and
vice versa. By the reciprocal symmetry (10.5) the deviation from the shelving
band’s reference gain β2 is the same, just in the opposite direction, thus in the
shelving band |HHS(jω)| varies between β2 and β2/δ.
From (10.22) it’s easy to notice that reciprocating β reciprocates δ and vice
versa. Therefore without loss of generality, for the sake of simplicity we can
restrict the discussion to β ≥ 1, δ ≥ 1, in which case the passband ripples
occcur within [1, δ] and the shelving band ripples occur within [β2/δ, β2]. The
case of β ≤ 1, δ ≤ 1 will follow automatically, where the ripple ranges will be
[δ, 1] and [β2, β2/δ] respectively.
Recall that the value of ˜k grows simultaneously with k, where the latter is
deﬁning the elliptic transition band [
k]. Thus we are having three user-
k, 1/
facing parameters, each of those being independently related to its respective
variable:12
√
√
Transition bandwidth:
Shelving gain:
Ripple amplitude:
˜k
β
δ
where the dependency between the three variables is given by (10.22).
Apparently at ﬁxed ˜k > 0 the value of δ grows with β and vice versa. At
ﬁxed β > 1, the value of δ grows with ˜k and vice versa. At ﬁxed δ > 1, the
11Thus, differently from how we used (9.152) in the discussion of elliptic lowpass, we treat ε
and λ now as independent variables. This doesn’t affect the solution process of (9.152), since
the respective transformations didn’t use the interdependency of ε and λ.
12We should mind that the dependency between the transition bandwidth and ˜k is
reciprocal-like: larger ˜k means smaller bandwidth. The other two dependencies are straight-
forward.
10.7. ELLIPTIC SHELVING
433
values of β and k change in opposite directions. Thus, if e.g. we want a smaller
transition band, this means we want larger k and larger ˜k, which means larger
δ (given a ﬁxed β). This means there is a tradeoﬀ between the transition band
width (which we usually want small) and the ripple amplitude (which we also
usually want small). There are similar tradeoﬀs between the other two pairs of
the user-facing parameters.
Given any two of the three parameters, we can ﬁnd the third one from
(10.22). The explicit expressions for β and ˜k can be obtained by transforming
(10.22) to
1 + β2˜k = δ2 + β−2˜kδ2
β2 + β4˜k = β2δ2 + ˜kδ2
from where on one hand
β4˜k − (δ2 − 1)β2 − ˜kδ2 = 0
β2 =
δ2 − 1
2˜k
+
(cid:115)(cid:18) δ2 − 1
2˜k
(cid:19)2
+ δ2
(where apparently the restriction is δ ≥ 1), on the other hand
(β4 − δ2)˜k = β2(δ2 − 1)
˜k = β2 δ2 − 1
β4 − δ2
(10.23)
(10.24)
(where 1 ≤ δ < β will ensure 0 < ˜k < 1) and k can be obtained by (9.138).
At β = 1 the formula (10.24) doesn’t work, since the amplitude response of
HHS is simply a horizontal line at unity gain and any of the values of ˜k will do.
Respectively at ˜k = 0 the formula (10.23) doesn’t work, since δ must be equal
to 1 in this case.
Notably, since (10.22) works equally well for β < 1 and δ < 1, so do (10.23)
(under the restriction δ ≤ 1) and (10.24) (under the restriction β < δ ≤ 1). In
practice the numerical evaluation of (10.23) for δ < 1 could raise concerns of
potential precision losses, therefore it’s better to apply (10.23) to the reciprocal
value of δ, which is larger than 1, and then reciprocate the result once again.
Steepness control
As with 2nd kind Butterworth shelving ﬁlters, we would like to be able to
estimate the logarithmic midslope steepness of the elliptic shelving ﬁlter. Eval-
uating the following derivative at x = 0, by (10.20) we have
d
dx
=
(cid:12)
(cid:12)
ln |HHS(jex)|
(cid:12)
(cid:12)
(cid:12)x=0
(cid:0)ln (cid:0)1 + β2 ¯R2
(cid:18) 2β2 ¯RN (ex) ¯R(cid:48)
1 + β2 ¯R2
d
2 dx
1
2
=
=
d
2 dx
ln |HHS(jex)|2 =
d
2 dx
ln
1 + β2 ¯R2
1 + β−2 ¯R2
N (ex)
N (ex)
=
N (ex)(cid:1) − ln (cid:0)1 + β−2 ¯R2
N (ex)ex
N (ex)
2β−2 ¯RN (ex) ¯R(cid:48)
1 + β−2 ¯R2
N (ex)(cid:1)(cid:1) =
N (ex)ex
N (ex)
−
(cid:19)
=
434
CHAPTER 10. SPECIAL FILTER TYPES
=
=
=
β−2 ¯R(cid:48)
β2 ¯R(cid:48)
N (1)
N (1)
1 + β−2 =
1 + β2 −
β2(1 + β−2) − β−2(1 + β2)
(1 + β2)(1 + β−2)
(cid:18) β2
1 + β2 −
(cid:19)
β−2
1 + β−2
¯R(cid:48)
N (1) =
¯R(cid:48)
N (1) =
β2 − β−2
β2 + 2 + β−2
¯R(cid:48)
N (1) =
β2 − β−2
(β + β−1)2
¯R(cid:48)
N (1) =
β − β−1
β + β−1
¯R(cid:48)
N (1)
(10.25)
Recalling the formulas (9.145), (9.150) and Fig. 9.61, we ﬁnd that, as expected,
steepness grows with k and N . Unfortunately, diﬀerently from the 2nd kind
Butterworth case, the expression (10.25) (or, speciﬁcally, (9.145)) is not easily
invertible as a function of k, which means we cannot easily ﬁnd k from the
desired slope steepness. However, for each given N this function’s inverse can
be tabulated and we could use the midslope steepness instead of transition
bandwidth as a control parameter.
Centered ripples
If we allow equiripples in the pass and shelving bands, it would be reasonable
to require that these equiripples are not unipolar but rather centered around
the required reference gains of these bands. We are going now to derive the
respective formulas, which will be slightly simpler to do in terms of the tilting
ﬁlter:
|Htilt(jω)|2 = β−2 1 + β2 ¯R2
1 + β−2 ¯R2
N (ω)
N (ω)
where again, for simplicity of discussion, without loss of generality we will as-
sume β ≥ 1, δ ≥ 1.
√
√
As a ﬁrst step, we shall deﬁne the new reference gains, corresponding to the
logarithmic centers of the equiripples. Since the left shelving band ripples occur
within [β−1, β−1δ], their logarithmic center is at β−1
δ. Similarly, since the
right shelving band ripples occur within [β/δ, β], their logarithmic center is at
√
δ and the new reference gains ˜β−1 and
β/
˜β at the logarithmic centers of the equiripple ranges (Fig. 10.14).
δ. Therefore we introduce ˜β = β/
We want to use ˜β instead of β as one of the three control parameters. Since
the entire framework of elliptic shelving ﬁlters has been developed in terms of
k, β and δ, we’ll need to be able to convert from ˜β to β. At the ﬁrst sight this
√
seems to be trivially done by β = ˜β
δ, however this can be done only if we
know δ.
Recall that we have three control parameters ˜k, β and δ but only two freedom
degrees, which means that we can specify only two of the three parameters, while
the third parameter needs to be found by the respective relations. Similarly,
if the three control parameters are now ˜k, ˜β and δ, we are going to specify
only two of them. So, if we specify ˜β and δ, then indeed we can simply ﬁnd
δ and then ﬁnd ˜k by (10.24). Specifying ˜k and δ is apparently the same
β = ˜β
as before and doesn’t pose any new probleems. However there is yet an option
of specifying ˜k and ˜β, in which case we need to ﬁnd either β or δ, so that the
other variable can be found from (10.23) or (10.22).
√
Let’s ﬁnd β. Substituting the equation (10.22) into β = ˜β
δ we obtain
√
β4 = ˜β4 ·
1 + β2˜k
1 + β−2˜k
10.7. ELLIPTIC SHELVING
435
|H(jω)|
˜β
1
˜β−1
ωmid/8
ωmid
8ωmid
ω
Figure 10.14: Centered reference gains ˜β−1 and ˜β of an elliptic
tilting ﬁlter.
β4 + ˜kβ2 = ˜β4 + ˜k ˜β4β2
β4 − ˜k( ˜β4 − 1)β2 − ˜β4 = 0
β2 = ˜k
˜β4 − 1
2
(cid:118)
(cid:117)
(cid:117)
(cid:116)
(cid:32)
˜k
+
(cid:33)2
˜β4 − 1
2
+ ˜β4
(10.26)
Similarly to (10.23), formula (10.26) also works for β ≤ 1, δ ≤ 1, but due to
numeric reasons in this case it’s better to apply it to the reciprocal δ and then
reciprocate the result.
Thus we have developed a way to express the tilting ﬁlter in terms of the
√
centered reference gains β−1 and β by converting from ˜β to β either by β = ˜β
δ
or by (10.26). For the high- and low-shelving ﬁlters one needs to additionally
take into account that the new reference gains imply diﬀerent multiplication
factors for conversion from tilting to the respective shelving factors:
HHS(s) = ˜β · Htilt(s)
HLS(s) = ˜β−1 · Htilt(s)
so that the centered passband reference gain is at 1 and the centered shelving
reference gain is at ˜β2 or ˜β−1 respectively.
Relation to Butterworth shelving
At k = 0 we have ¯RN (x) = xN and the elliptic shelving ﬁlters turn into respec-
tive 1st-kind Butterworth shelving ﬁlters. However also notice that a 2nd-kind
2 is equal to the 1st-kind
Butterworth shelving ﬁlter of order N at R = 1/
Butteworth shelving of the same order, which in turn is equal to the elliptic
√
436
CHAPTER 10. SPECIAL FILTER TYPES
shelving ﬁlter of the same order at k = 0. That is at R = 1/
three kinds of shelving ﬁlters coincide.13
√
2 and k = 0 all
√
In that sense elliptic shelving can be seen as another way of extending the
2.
2nd-kind Butterworth variable-slope shelving into the range beyond R = 1/
2 (which would result in one large resonance
Instead of reducing R below 1/
peak in each of the pass and/or shelving bands), we could switch to the elliptic
ﬁlter parameters14 obtaining a number of smaller equiripples. This particularly
means that in the wide-range slope technique described in Section 10.5 instead
√
2 we could switch to elliptic equations
of increasing the ﬁlter order at R = 1/
(if we are willing to accept the ripples).
√
√
At N = 2 however there is essentially no diﬀerence between 2nd-kind But-
2 and elliptic shelving, except for diﬀerent formal
terworth shelving at R ≤ 1/
Indeed, both ﬁlters have two poles and two zeros which
control parameters.
are mutually reciprocal and are also having conjugate symmetry. This leaves
only two degrees of freedom, one degree corresponds to choosing the cutoﬀ of
the poles (which simultaneously deﬁnes the cutoﬀ of the zeros as the recipro-
cal value of the poles cutoﬀ) the other degree being the damping of the poles
(which simultaneously deﬁnes the damping of the zeros as both dampings must
be equal). However the ﬁrst degree of freedom is taken by controlling the shelv-
ing gain and the second degree of freedom is taken by varying the R or the
k parameter respectively. Thus the diﬀerence between the two ﬁlters can be
only in how the control parameters are translated to the transfer function and
in the leading gain coeﬃcients of the transfer functions (where we would have
|HHS(0)| = 1 in the Butterworth case and |HHS(0)| = δ in the elliptic case).
Combining with other techniques
Elliptic design of shelving ﬁlters can be combined with other design techniques.
Particulary, we can apply the LP to BP transformation to an elliptic low-
shelving ﬁlter to obtain an elliptic band-shelving ﬁlter.
In principle one also could apply Butterworth transformation to elliptic ﬁl-
ters to increase the slope steepness. However, since we are already having ripples
in the pass and shelving bands, it would be more eﬃcient to simply increase the
order of elliptic ﬁlter, thereby attaining higher slope steepnesses (compared to
applying the Butterworth transformation) at the same ripple amplitude.
10.8 Crossovers
Sometimes we would like to process diﬀerent frequency bands of a signal dif-
ferently. In the simplest case we would want to split the signal into low- and
high-frequency parts, process one of them or both in some way and then merge
13Particularly, notice that (10.25) becomes identical to (10.17) for β = M 2.
14Notice that such switching is completely smooth, as the transfer functions of the filters
are completely identical at this point and the “physical” orders of the filters are identical
too (there is no pole/zero cancellation as we had in the Butterworth filter order switching).
Strictly speaking, the statement that the transfer functions are identical holds only under the
restriction that the phase responses of the filters are in sync, rather that 180◦ off, which is
a matter of the sign in front of the transfer functions. However usually we are having zero
phase responses at ω = 0, therefore the signs will be automatically matched.
10.8. CROSSOVERS
437
them back (Fig. 10.15). This kind of ﬁlters, splitting the signal into diﬀerent
frequency bands are called crossovers.
x(t)
Crossover
HP
LP
Process highs
Process lows
y(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 10.15: The crosssover idea.
In principle we could take any a pair of lowpass and highpass ﬁlters to
build a crossover, but some combinations would work better than the others.
Particularly, imagine that the processing of diﬀerent bands changes the signals
very slightly, or sometimes maybe even doesn’t change them at all. In that case
it would be really nice if the original signal was unaltered by the structure in
Fig. 10.15, that is y(t) = x(t). However, this naturally expected property will
not be given for granted.
Suppose we use a multimode 1-pole as a crossover basis, in which case the
low- and high-pass ﬁlters share the same cutoﬀ. Without loss of generality we
could let ωc = 1 (in other words, the crossover frequency will be at ω = 1):
HLP(s) =
HHP(s) =
1
1 + s
s
1 + s
Adding low- and high-pass transfer functions we have
H(s) = HLP(s) + HHP(s) =
1
1 + s
+
s
1 + s
= 1
Thus, if the low- and high-pass signals are unmodiﬁed by the processing, adding
them together at the end of the network in Fig. 10.15 would restore the original
signal exactly.
However the same doesn’t hold anymore for 2-poles:
HLP(s) =
HHP(s) =
1
1 + 2Rs + s2
s2
1 + 2Rs + s2
in which case we have
H(s) = HLP(s) + HHP(s) =
s2 + 1
s2 + 2Rs + 1
(cid:54)= 1
In fact, as we may recall, the above H(s) is a notch ﬁlter. Of course, we could
add the missing bandpass component, e.g. splitting it equally between the low-
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
438
CHAPTER 10. SPECIAL FILTER TYPES
and high-bands:
1 + Rs
1 + 2Rs + s2 +
s2 + Rs
1 + 2Rs + s2 = 1
but then the rolloﬀ of the resulting low- and high-pass ﬁlters becomes 6dB/oct
instead of former 12dB/oct, leading to the question, why using such 2-pole in
the ﬁrst place when a 1-pole would have done similarly.
Butterworth crossovers
If we relax the requirement of the sum of unprocessed signals being exactly
equal to the original signal and allow a phase shift in the sum, while retaining
the amplitudes, we essentially require that the low- and high-passes should add
to an allpass:
|H(s)| = |HLP(s) + HHP(s)| = 1
Let’s take (1st kind) Butterworth low- and high-passes at the same cutoﬀ ωc = 1:
HLP(s) =
HHP(s) =
1
P (s)
sN
P (s)
where N is the ﬁlter order and P (s) denotes the common denominator of HLP(s)
and HHP(s). Remember that the denominator P (s) is deﬁned by the equation
|P (jω)|2 = 1 + ω2N
while all roots of P (s) must lie in the left complex semiplane.
Adding the low- and high-passes together we obtain
H(s) = HLP(s) + HHP(s) =
1
P (s)
+
sN
P (s)
=
1 + sN
P (s)
Assuming N is odd, for s = jω we get
|H(jω)|2 =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1 + (jω)N
P (jω)
(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
=
1 + ω2N
|P (jω)|2 = 1
(N odd)
Notably, the same property holds for the diﬀerence of Butterworth low- and
high-passes of odd order
|HLP(s) − HHP(s)|2 =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1 − (jω)N
P (jω)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
1 + ω2N
|P (jω)|2 = 1
(N odd)
For an even order however 1 ± sN becomes purely real for s = jω
1 ± sN = 1 ± jN ωN = 1 ± (−1)N/2ωN
and we get either a zero at ω = 1 if the above gets the form 1 − ωN , or, if it
gets the form 1 + ωN then
|H(jω)|2 =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1 + (jω)N
P (jω)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
(cid:0)1 + ωN (cid:1)2
1 + ω2N =
1 + 2ωN + ω2N
1 + ω2N
=
10.8. CROSSOVERS
439
= 1 +
2ωN
1 + ω2N = 1 +
(cid:18) 1 + ω2N
2ωN
(cid:19)−1
= 1 + 2
(cid:19)−1
(cid:18)
ωN +
1
ωN
Apparently the expression in parentheses is symmetric in logarithmic scale
around ω = 1 and attains a minumum at this point, respectively |H(jω)|2
attains a maximum, which we can evaluate by substituting ω = 1, obtaining
√
|H(j)|2 = 2. Respectively |H(j)| =
2 thus the amplitude response of H(s)
has a +3dB bump at ω = 1.
As both HLP(s) and HHP(s) share the same denominator, they can be imple-
mented by a single generalized SVF (the controllable canonical form in Fig. 8.1)
using the modal outputs for the numerators 1 and sN respectively. Alternatively
one could use multimode features of the serial cascade representation. Parallel
representation is also possible, where we would pick up diﬀerent modal mixtures
of the same parallel 2-poles as the low- and high-pass signal respectively.
Linkwitz–Riley crossovers
If instead of Butterworth lowpass and highpass ﬁlters we take squared Butter-
worth ﬁlters:
H(s) = H1(s) + H2(s)
(cid:18) 1
H1(s) = H 2
LP(s) =
P (s)
(cid:19)2
H2(s) = (−1)N H 2
HP(s) = (−1)N
(cid:19)2
(cid:18) sN
P (s)
|P (jω)|2 = 1 + ω2N
(notice the conditional inversion of the squared highpass signal) we do obtain a
perfect allpass H(s) for any N :
H(jω) = H1(jω) + H2(jω) =
1
P 2(jω)
=
1 + (−1)N j2N ω2N
P 2(jω)
=
1 + ω2N
P 2(jω)
=
+ (−1)N (jω)2N
P 2(jω)
P (jω)P (−jω)
P 2(jω)
=
=
P (−jω)
P (jω)
(10.27)
and thus, since P (jω) is Hermitian, |H(jω)| = 1. A crossover designed in this
way is referred to as Linkwitz–Riley crossover. Since the denominators in (10.27)
are identical, we again can use a shared structure, such as a generalized SVF or
a multimode serial cascase to produce the output signals of both H1 and H2.
The parallel representation is problematic, since we are now having repeated
poles due to the squaring of the denominators.15
Note that the phase responses of H1(s) and H2(s) are identical, since the
phase contributions of their numerators are zero, while the phase contributions
of their denominators are identical. This in-phase relationship of the split bands
15In principle, by general considerations, one should be able to connect two identical parallel
representations in series and obtain modal mixtures from those in a fashion similar to the
multimode serial cascade. The author however didn’t verify the feasibility of this approach.
440
CHAPTER 10. SPECIAL FILTER TYPES
is the key feature of Linkwitz–Riley crossovers16 (contrary to the somewhat
common opinion that the key feature of Linkwitz–Riley crossovers is the absence
of the +3dB bump, which, as we have seen is also e.g. the case with odd-order
Butterworth crossovers).
The in-phase relationship actually also includes H(s):
arg H1(jω) = arg H2(jω) = arg H(jω)
Indeed
arg H(jω) = arg P (−jω) − arg P (jω) = −2 arg P (jω) = arg
1
P 2(jω)
where we used the Hermitian property of P (jω).
Generalized Linkwitz–Riley crossovers
The Linkwitz–Riley design consisting of two squared Butterworth ﬁlters is a
special case of a more generic idea which we will discuss below.17
First we need a kind of auxiliary lemma. Let Q(s) be a real polynomial of
s. We now state that the formal frequency response Q(jω) is real nonnegative
if and only if Q(s) can be written in the form Q(s) = P (s)P (−s), where P (s)
is some other real polynomial of s. The proof goes like follows.
Suppose Q(s) = P (s)P (−s). Then for ω ∈ R
Q(jω) = P (jω)P (−jω) = P (jω)P ((jω)∗) = P (jω)P ∗(jω) = |P (jω)|2 ≥ 0
Conversely, suppose Q(jω) ≥ 0. Since Q(jω) is simultaneously real and
Hermitian, it must be even, and so must be Q(s). Therefore it can be factored
into Q(s) = P (s)P (−s). Let’s chose P (s) to contain the left complex semiplane
roots of Q(s), thereby P (−s) will contain the right complex semiplane roots. If
Q(s) has roots on the imaginary axis, these roots will all have even multiplicities
(since otherwise Q(jω) will be changing sign at these points) and therefore we
can split these roots into two identical halves, which we assign to P (s) and
P (−s) respectively. Since Q(s) is real, its poles are conjugate symmetric and
so will be the poles of P (s) and P (−s).
We still need to show that the leading coeﬃcient of P (s) will be real. Let g
denote the leading coeﬃcient of P . Then the leading term of Q(s) = P (s)P (−s)
is gsN g(−s)N = (−1)N g2s2N . By substituting s = jω we obtain the leading
term of Q(jω), which is (−1)N g2(jω)2N = (−1)N g2(−1)N ω2N = g2ω2N . How-
ever the coeﬃcient g2 of the leading term g2ω2N of Q(jω) must be positive,
otherwise Q(jω) would become negative at large ω. Since g2 is positive, g is
real. That completes the proof.
16The author has been made aware of the importance of the in-phase property of Linkwitz–
Riley crossovers by a remark by Teemu Voipio. The author also learned the cascaded phase
correction approach shown in Fig. 10.24 from the same person.
17The idea to generalize the Linkwitz–Riley design arose from a remark by Max Mikhailov,
that Linkwitz–Riley crossovers can be also built based on naive 1-pole lowpasses, which in the
BLT terms can be formally seen as a special kind of high-shelving filters. It is quite possible
that this idea has been already developed elsewhere, however at the time of the writing the
author is not aware of other sources.
10.8. CROSSOVERS
441
Now, given two real polynomials Q1(s) and Q2(s) with real nonnegative
frequency responses, we can construct a third real polynomial as their sum
Q1(s) + Q2(s) = Q(s)
(10.28)
Since the frequency responses of Q1(s) and Q2(s) are nonnegative, so is the
frequency response of Q(s). By the previous discussion, the above equation can
be rewritten as
P1(s)P1(−s) + P2(s)P2(−s) = P (s)P (−s)
(10.29)
Dividing both sides by the right-hand side, we obtain
P1(s)P1(−s)
P (s)P (−s)
+
P2(s)P2(−s)
P (s)P (−s)
= 1
(10.30)
We wish to interpret the two terms in the left-hand side as transfer functions.
However, these functions are not stable, since the roots of P (−s) are lying in
the right semiplane. We can however multiply both parts by P (−s)/P (s):
P1(s)P1(−s)
P 2(s)
+
P2(s)P2(−s)
P 2(s)
=
P (−s)
P (s)
(10.31)
thereby making both ﬁlters stable and turning the right-hand side into an (also
stable) allpass. Also notice that the orders of P1(s) and P2(s) do not exceed
the order of P (s), therefore the terms of (10.31) are nonstrictly proper rational
functions of s, as required for transfer functions of (integrator-based) diﬀerential
ﬁlters. Introducing
H1(s) =
H2(s) =
HAP(s) =
·
·
P1(s)
P (s)
P2(s)
P (s)
P (−s)
P (s)
P1(−s)
P (s)
P2(−s)
P (s)
(10.32a)
(10.32b)
(10.32c)
we rewrite (10.31) as
H1(s) + H2(s) = HAP(s)
(10.33)
and thus we have built a crossover (provided H1(s) is a kind of a lowpass and
H2(s) is a kind of a highpass). Again, the denominators are identical and we
can use a shared structure for H1 and H2.
Notice that (10.33) is simply (10.28) divided by P 2(s). The phase responses
of all terms of (10.28) are apparently zero, therefore the phase responses of all
terms of (10.33) are identical and simply equal to −2 arg P (jω):
arg H1(jω) = arg H2(jω) = arg H(jω) = −2 arg P (jω)
The identical phase responses, as we should remember, are the key feature
of Linkwitz–Riley crossover design, thus we have built a kind of generalized
Linkwitz–Riley crossover.
442
CHAPTER 10. SPECIAL FILTER TYPES
The identical phase responses of H1, H2 and HAP also allow to rewrite
(10.33) in terms of amplitude responses:
|H1(s)| + |H2(s)| = 1
(10.34)
It is often convenient to deﬁne
G1(s) =
G2(s) =
P1(s)
P (s)
P2(s)
P (s)
By (10.32), Hn(s) = Gn(s)G−
|Hn(jω)| = |Gn(jω)|2 and therefore (10.34) turns into
n (s) where G−
n (s) = Pn(−s)/P (s). Apparently
|G1(jω)|2 + |G2(jω)|2 = 1
(10.35)
The interpretation in terms of G1 and G2 suggests another, somewhat more
practical approach to building generalized Linkwitz–Riley crossovers. We start
with a pretty much random ﬁlter G1(s) = P1(s)/P (s), although satisfying
|G1(jω)|2 ≤ 1, so that (10.35) can hold. From P1(s) and P (s), using (10.32), we
obtain H1(s) and HAP(s) and can simply ﬁnd H2(s) as H2(s) = HAP(s)−H1(s).
In principle, the obtained H2(s) can be used as it is, but we can also further
factor it into H2(s) = P2(s)P2(−s)/P 2(s) thereby obtaining G2(s).18 Of course,
in order for H1(s) and H2(s) to count as a “reasonable” crossover, H1(s) must
be a lowpass or lowpass-like ﬁlter (which can be ensured by choosing a lowpass-
like G1(s)), and the obtained H2(s) must be highpass-like. Or the other way
around.
Alternatively we might be able to simply “guess” H1(s) and H2(s) (or, equiv-
alently, P1(s) and P2(s), or G1(s) and G2(s)). E.g. the previously discussed But-
terworth ﬁlter-based Linkwitz–Riley crossover arises by choosing G1(s) to be a
Butterworth lowpass and G2(s) = G1(1/s) to be a Butterworth highpass, which
gives G−
1(s),
H2(s) = (−1)N G2
2(s). The same result is obtained by by choosing P1(s) = 1,
P2(s) = sN (respectively P1(−s) = 1 and P2(−s) = (−1)N sN ). This gives
(10.29) in the form
2 (s) = (−1)N G2(s) and respectively H1(s) = G2
1 (s) = G1(s), G−
P (s)P (−s) = P1(s)P1(−s) + P2(s)P2(−s) = 1 + (−1)N s2N
from where
Q(jω) = 1 + ω2N = |P (jω)|2 = P (jω)P (−jω)
where P (s) is the Butterworth denominator. The equation (10.31) respectively
takes the form
(cid:18) 1
(cid:19)2
P (s)
+ (−1)N
(cid:19)2
(cid:18) sN
P (s)
=
P (−s)
P (s)
which is essentially the same as (10.27).
18In order to show that this factoring is possible, multiply H2(jω) = HAP(jω) − H1(jω)
by P 2(jω), obtaining H2(jω)P 2(jω) = P (jω)P (−jω) − P1(jω)P1(−jω) ≥ 0, where the latter
inequality follows from |G1(jω)|2 ≤ 1.
10.8. CROSSOVERS
443
Symmetric generalized Linkwitz–Riley crossovers
Ideally in (10.33) we would like to have symmetric amplitude responses
|H2(jω)| = |H1(j/ω)|
(10.36)
as it was e.g. the case with Butterworth-based Linkwitz–Riley crossover. Appar-
ently (10.36) is not guaranteed for an arbitrary pair of H1(s) and H2(s) which
satisﬁes (10.33) (where satisfying (10.33) is understood in the sense that the
sum of H1(s) and H2(s) is an allpass). We would like to ﬁnd a way of obtaining
generalized Linkwitz–Riley crossovers satisfying (10.36).
Recall that (10.35) is just another intepretation of the crossover equation
(10.33). On the other hand, compare (10.35) to (10.4). By (10.4), the equation
(10.35) will be satisﬁed by G1 and G2 related through an LP to HP transfor-
mation, if f (1/x) = 1/f (x), where f (x) is the function used to construct G1 by
(9.18).
This is not suﬃcient yet, as besides satisfying (10.35) (and respectively
(10.33)), we need to have the same poles in G1(s) and G2(s), so that they
can share the same denominator P (s). However we have already shown that
G1(s) and G2(s) will have the same poles if f (1/x) = 1/f (x).
Therefore, in order to obtain a generalized Linkwitz–Riley crossover with
symmetric amplitude responses, we need to take G1(s) obtained from f (x) sat-
isfying f (1/x) = 1/f (x) and G2(s) = G1(1/s).
EMQF Linkwitz–Riley crossovers
We already know one function f (ω) satisfying f (1/x) = 1/f (x): the normal-
ized elliptic rational function ¯RN . Therefore EMQF ﬁlters might be a good
candidate for symmetric generalized Linkwitz–Riley crossovers. Notice that at
k → 0 EMQF ﬁlters turn into Butterworth ﬁlters and we obtain a classical
(Butterworth-based) Linkwitz–Riley crossover.
Therefore let G1(s) = P1(s)/P (s) be an EMQF (lowpass) ﬁlter and G2(s) =
G1(1/s) = P2(s)/P (s) be the respective highpass. Recall that the zeros of
elliptic lowpass ﬁlters are positioned on the imaginary axis in pairs symmetric
relatively to the origin, with the exception of the zero at the inﬁnity, which
occurs if the order N of the ﬁlter is odd. Therefore P1(s) can be written as
P1(s) = g1 ·
(cid:89)
(s2 − z2
n)
Im zn>0
which means that P1(−s) = (−1)N P1(s). Respectively P2(s) can be written as
P2(s) = g2 · sN ∧1 (cid:89)
(s2 − 1/z2
n)
Im zn>0
where the sN ∧1 factor arises from the zero of G1(s) occuring at the inﬁnity
which turns into a zero of G2(s) occurring at the origin. Therefore P2(−s) =
(−1)N P2(s).
Therefore H1(s) = G2
1(s), H2(s) = (−1)N G2
2(s) and (10.33) takes the form
G2
1(s) + (−1)N G2
2(s) =
P (−s)
P (s)
Fig. 10.16 shows the example of amplitude responses of H1 and H2.
444
|H(jω)|2
1
0.5
0
CHAPTER 10. SPECIAL FILTER TYPES
1/8
1
8
ω
Figure 10.16: Amplitude responses of EMQF crossover low- (solid)
and high-pass (dashed) outputs.
Centered ripples
Next we will describe a way to further improve the amplitude response of gener-
alized Linkwitz–Riley crossovers. The techniques can be applied to pretty much
any generalized Linkwitz–Riley crossover, but for the sake of simpler presenta-
tion we’ll be using the EMQF crossover as an example.
Recall that the phase responses of H1, H2 and HAP are identical. Let ϕ(ω) =
arg H1(jω) = arg H2(jω) = arg HAP(jω) be this common phase response. Then
we can introduce the zero phase frequency response functions
¯H1(jω) = e−jϕ(ω)H1(jω)
¯H2(jω) = e−jϕ(ω)H2(jω)
¯HAP(jω) = e−jϕ(ω)HAP(jω) ≡ 1
Notice that since arg ¯H1(jω) = arg ¯H2(jω) = arg ¯HAP(jω) = 0, we have
¯H1(jω) = |H1(jω)|
¯H2(jω) = |H2(jω)|
¯HAP(jω) = |HAP(jω)| = 1
That is we can consider ¯H1(jω), ¯H2(jω) and ¯HAP(jω) as amplitude response
functions.
We are now going to construct some linear combinations of the above zero
phase frequency responses. Since they are all related to the original frequency
responses via one and the same factor e−jϕ(ω), linear combinations of ¯H1, ¯H2
and ¯HAP correspond to exactly the same linear combinations of H1, H2 and
HAP. E.g.
α ¯H1(jω) + β ¯H2(jω) = e−jϕ(ω) · (αH1(jω) + βH2(jω))
We can think of these linear combinations as of linear combinations of amplitude
responses, resulting in the new amplitude responses, with the reservation that
10.8. CROSSOVERS
445
the new “amplitude responses” may become negative (which in terms of true
amplitude responses would have been interpreted as changing the phase response
by 180◦).
Consider that the passband ripple amplitude of ¯RN is
(cid:112)˜k, while the stop-
(cid:112)˜k. Respectively the passband ripples of ¯H1 and
band ripple amplitude is 1/
¯H2 oscillate within [1/(1 + ˜k), 1], while the stopband ripples oscillate within
[0, ˜k/(1 + ˜k)], which corresponds to the absolute maximum deviations ˜k/(1 + ˜k)
from the ideal values of 1 (passband) and 0 (stopband).
Note that so far the deviations are unipolar. The deviation from 1 occurs
towards zero, while the deviation from zero occurs towards 1. We could make
these deviations bipolar instead, simultaneously reducing the maximum devi-
ation. The (linear) midpoints of the oscillation ranges are (˜k/2)/(1 + ˜k) for
the stopband and (1 + ˜k/2)/(1 + ˜k) for the passband. We can take the range
[(˜k/2)/(1 + ˜k), (1 + ˜k/2)/(1 + ˜k)] between these middles and stretch it to [0, 1]
This can be achieved by the transformation
¯H (cid:48) = (1 + ˜k) ¯H − ˜k/2 = (1 + ˜k) ¯H −
˜k
2
· ¯HAP
which should be applied to both lowpass and highpass:
¯H (cid:48)
1 = (1 + ˜k) ¯H1 −
¯H (cid:48)
2 = (1 + ˜k) ¯H2 −
˜k
2
˜k
2
· ¯HAP
· ¯HAP
Thereby the deviation amplitude is multiplied by 1 + ˜k, but simultaneously the
deviations become centered around the ideal values 0 and 1, which eﬀectively
halves the deviations. Thus the deviation amplitude is eﬀectively multiplied by
(1 + ˜k)/2 becoming equal simply to ˜k/2 (Fig. 10.17).
¯H (cid:48)
n(jω)
1
0.5
0
1/8
1
8
ω
Figure 10.17: Zero-phase frequency responses of adjusted EMQF
crossover low- (solid) and high-pass (dashed) outputs.
Multiplying the above equations by ejϕ(ω) we obtain the same transformation
446
CHAPTER 10. SPECIAL FILTER TYPES
for H1 and H2:
H (cid:48)
1 = (1 + ˜k)H1 −
H (cid:48)
2 = (1 + ˜k)H2 −
˜k
2
˜k
2
· HAP
· HAP
Note that thereby we still have
H (cid:48)
1 + H (cid:48)
2 = (1 + ˜k)H1 + (1 + ˜k)H2 − ˜kHAP = HAP + ˜kHAP − ˜kHAP = HAP
Phase correction
If we need to do some processing in parallel to the crossover, then we should
keep in mind that the crossover signals are phase shifted, therefore it could be a
good idea to introduce the same phase shift into the signal which bypasses the
crossover.
At this point we will assume that the crossover is (generalized) Linkwitz–
Riley, therefore all phase shifts are identical. In this case the simplest way to
construct a phase-shifted bypass signal is by adding the LP and HP outputs of
the crossover together, which by the previous discussion should be an allpass
signal with exactly the same phase shift as in LP and HP signals (Fig. 10.18).
Notice that the LP and HP outputs of the crossover in Fig. 10.18 correspond to
the H1(s) and H2(s) transfer functions in (10.33). Particularly, if we’re using a
Butterworth or an EMQF crossover, the squared HP signal needs to be inverted
for odd N .
Crossover
HP
LP
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
Process 0
Process 1
•(cid:47)
Process 2
+
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 10.18: Adjusting the phase of the bypass signal.
The approach of Fig. 10.18 doesn’t work if the bypass signal processing path
is not starting from the same point where the crossover is connected. In this
case we might need an explicit phase-correction allpass. Fig. 10.19 shows the
option of doing the phase correction prior to the processing of the bypass signal.
Rather than constructing the correction allpass following the idea of Fig. 10.18
(that is building such an allpass as another crossover with LP and HP out-
puts added), it is more eﬃcient to construct this allpass directly. Indeed, by
(10.32), given a crossover whose order is 2N , the order of the allpass HAP(s) =
P (−s)/P (s) is only N . Therefore it is more eﬃcient to implement the correction
allpass simply as an N -th order ﬁlter:
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
10.8. CROSSOVERS
447
Allpass
Process 0
Crossover
HP
LP
Process 1
Process 2
+
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 10.19: Adjusting the phase of the signal from a diﬀerent
source.
In Fig. 10.19 we could swap the order of the phase correction and the pro-
cessing of the bypass signal as shown in Fig. 10.20. If the processing is nonlinear,
this may result in an audible change in the sound. One could argue that the
option shown in Fig. 10.20 is better, since the nonlinear processing is done on
the original signal, while the allpass correction of the processing results would
be usually inaudible (unless another nonlinear processor is following), and thus
the bypass processing would sound pretty much identical to the one in the ab-
sence of the phase shifts. However, there is a counterargument that all other
processing is done on phase-shifted signals, and it would be more consistent to
do the same for the bypass signal.
Process 0
Allpass
Crossover
HP
LP
Process 1
Process 2
+
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 10.20: Correction allpass at the end of processing.
A more complicated situation arises if we want to stack the crossovers to
make a multiband crossover because in this case the phase correction is needed
even if there is no bypass signal. Consider Fig. 10.21, where A2 denotes an
allpass introducing the phase shift corresponding to the crossover C2. The LP
and HP outputs of the crossover C1 are completely in-phase, therefore the signal
going through the processor P1 is, from the phase shift perspective, essentially
the same as bypass signal of the crossover C2 and thus needs phase correction
equivalent to the phase contribution of C2. Or, looking from a slightly diﬀerent
angle, the input signals of processors P2 and P3 contain phase shifts from both
crossovers, while the input signal of processor P1 contains the phase shift only
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
448
CHAPTER 10. SPECIAL FILTER TYPES
from the ﬁrst crossover and thus needs an additional phase shift by A2.
C1
HP
LP
C2
HP
LP
A2
P3
P2
P1
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 10.21: Phase correction in 3-way crossover mixing.
If the bypass signal processing is present, we could modify the structure
of Fig. 10.21 as shown in Fig. 10.22. An alternative option is presented in
Fig. 10.23 and yet another option (requiring one more corection allpass) in
Fig. 10.24. Notice that Fig. 10.22 does all phase shifting at the beginning and
Fig. 10.24 does all phase shifting at the end, while the structure in Fig. 10.23 is a
kind of in-between mixture of Fig. 10.22 and Fig. 10.24. These ideas generalize
by induction to higher numbers of bands, where in Fig. 10.22 we’ll be adding
new crossover-allpass pairs on the left, whereas in Fig. 10.24 we would be adding
crossovers on the left and allpasses on the right.
C1
HP
LP
HP
•(cid:47)
C2
LP
•(cid:15)
A2
•(cid:47)
+
+
+
P3
P2
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
P1
P0
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 10.22: Phase correction of bypass signal in 3-way crossover
mixing.
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
In four-way crossover mixing there are new options, e.g. there is a symmetric
band splitting option shown in Fig. 10.25. However practically it is not much
diﬀerent from the approach of Fig. 10.22 generalized to 4 bands, since the total
phase shifts in the input signals of all processing units contain the total sums
of the phase shifts associated with all crossovers in either case.
Note that in Fig. 10.25 one could also wish to replace the allpasses A2 and A3
with a single allpass A23 in one of the paths, which just corrects the diﬀerence
between the phase responses of C2 and C3. This is however not possible. Indeed,
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
10.9. EVEN/ODD ALLPASS DECOMPOSITION
449
C1
HP
•(cid:47)
LP
C2
HP
LP
•(cid:15)
+
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
P3
P2
P1
P0
+
A2(cid:79)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 10.23: Another way of phase correction of bypass signal in
3-way crossover mixing.
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
C1
HP
LP
C2
HP
LP
P3
P2
P1
P0
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
A2(cid:79)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
A1(cid:79)
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 10.24: 3-way crossover mixing with all phase correction
done at the end.
assuming identical orders of C2 and C3, their phase responses are identical at
each of the points ω = 0 and ω = ∞. Therefore the phase response of A23
must be equal to zero at both ω = 0 and ω = ∞. But this is not possible for a
diﬀerential allpass.19 The argument becomes somewhat more complicated if the
crossovers are allowed to have diﬀerent orders, where one would need to consider
the factored forms of A2 and A3, essentially reaching the same conclusion.
10.9 Even/odd allpass decomposition
Suppose we are given a ﬁlter H(s) deﬁned by (9.18).
In this section we are
going to show that H(s) is expressible as a linear combination of the “even”
19This would have been formally possible if A23 is allowed to be unstable, however the order
of A23 would have been equal to the sum of the orders of A2 and A3. We mention this because
this has a clear analogy to the phase splitter discussed later in the text.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:15)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:79)
450
CHAPTER 10. SPECIAL FILTER TYPES
C1
HP
LP
A2
C3
A3
C2
HP
LP
HP
LP
P4
P3
P2
P1
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 10.25: Symmetric 4-way crossover.
and “odd” allpasses, that is allpasses based on the even and odd poles of H(s).
Recall that we have deﬁned even poles as solutions of f = j (or equivalently
1 + jf = 0) and odd poles as solutions of f = −j (or equivalently 1 − jf = 0).
Let’s introduce the following notation:
(1 + jf )− =
(cid:89)
(s − pn)
1+jf (−jpn)=0
Re pn<0
(cid:89)
(1 + jf )+ =
(s − pn)
1+jf (−jpn)=0
Re pn>0
(cid:89)
(1 + jf )± =
(s − pn) = (1 + jf )+(1 + jf )−
1+jf (−jpn)=0
that is the product is being taken over all left- or respectively right-semiplane
even poles pn of H(s)H(−s) in the ﬁrst two lines, and over all even poles
of H(s)H(−s) in the third line. We will also use (1 − jf )−, (1 − jf )+ and
(1 − jf )± with similar meanings for the respective products based on the odd
poles of H(s)H(−s). We also introduce
(f )∞ =
(cid:89)
(s − zn)
f (−jzn)=∞
where zn goes over all poles of f (−js), or, equivalently, over all zeros of H(s).
In this notation we could express the construction of H(s) from its poles and
zeros as
H(s) = g? ·
(f )∞
(1 − jf )−(1 + jf )−
=
= H(jω0)
[(1 − jf )−](jω0) · [(1 + jf )−](jω0)
[(f )∞](jω0)
·
(f )∞
(1 − jf )−(1 + jf )−
(10.37)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
10.9. EVEN/ODD ALLPASS DECOMPOSITION
451
where g? denotes a placeholder for the yet unknown gain coeﬃcient, which we
then ﬁnd from the requirement of H(s) to have a speciﬁc value H(jω0) at some
point s = jω0 on the imaginary axis, and where [(1 − jf )−](ω0) denotes the
value of (1 − jf )− at s = jω0 and so on. In the simplest case we will let ω0 = 0,
which gives H(jω0) = H(0) = 1/(cid:112)1 + f 2(0), however we will also need to be
able to take other choices of ω0.
Now let’s introduce the “even” and “odd” allpasses:
He(s) = ge ·
Ho(s) = go ·
(1 − jf )+
(1 + jf )−
(1 + jf )+
(1 − jf )−
(10.38a)
(10.38b)
where the gains ge and go are deﬁned by the conditions He(jω0) = 1 and
Ho(jω0) = 1:
ge =
go =
[(1 + jf )−](jω0)
[(1 − jf )+](jω0)
[(1 − jf )−](jω0)
[(1 + jf )+](jω0)
(note that allpasses deﬁned in this manner can be trivially built as cascades of
2nd- and 1st-order sections). The allpass property of He and Ho follows from
the fact that f (ω) is a real function of ω, therefore the even an odd poles of
H(s)H(−s) (which are respectively the solutions of 1 + jf = 0 and 1 − jf = 0)
are mutually conjugate in terms of ω, that is they are symmetric with respect
to the imaginary axis in terms of s. Figs. 8.11, 8.12 and other similar ﬁgures
illustrate.
Apparently both He and Ho are stable ﬁlters. If additionally f (ω) is an odd
function and ω0 = 0, then He and Ho are real. Indeed, suppose 1+jf (−js) = 0,
that is s is an even pole. Then s∗ is also an even pole since
1 + jf (−js∗) = 1 − jf (js∗) = 1 − jf ((−js)∗) = 1 − j · (f (−js))∗ =
= 1 + (jf (−js))∗ = (1 + jf (−js))∗ = 0∗ = 0
The same can be shown for odd poles. Therefore the poles of each of the He
and Ho are mutually conjugate and, since He(0) = Ho(0) = 1, both ﬁilters are
real. If f (ω) is not an odd function, particularly if f (ω) is even, then the poles
of He and Ho do not have the conjugate symmetry, therefore He and Ho are
essentially complex ﬁlters. However this shouldn’t be a problem, since we will
use He and Ho only as intermediate transformation helpers.
Now we attempt express H(s) as a linear combination of He and Ho. Con-
sider the obvious algebraic relationship:
1 +
1 − jf
1 + jf
= 2
1
1 + jf
(10.39)
where f = f (ω). Equation (10.39), if interpreted in terms of s = jω, can be
understood as a relationship between three transfer functions, the two trans-
fer functions 1 and (1 − jf )/(1 + jf ) in the left-hand side adding up to the
452
CHAPTER 10. SPECIAL FILTER TYPES
doubled transfer function 1/(1 + jf ) in the right-hand side. The poles of these
transfer functions are identical20 and consist of the full set of the even poles of
H(s)H(−s).
By analysing the behavior of these transfer functions for ω ∈ R we also notice
that the two functions in the left-hand side of (10.39) are allpasses, while the
transfer function 1/(1 + jf ) in the right-hand side has an amplitude response
identical to |H(s)|. So, amplitude response-wise (10.39) is already what we are
looking for and we just need to correct it so that it also becomes what we want
transfer function-wise.
Let’s multiply (10.39) by Ho:
Ho(s) +
1 − jf
1 + jf
Ho(s) = 2
1
1 + jf
Ho(s)
(10.40)
Considering the product in the right-hand side of (10.40) we have
1
1 + jf
Ho(s) = g? ·
(f )∞
(1 + jf )±
·
(1 + jf )+
(1 − jf )−
= g? ·
(f )∞
(1 + jf )−(1 − jf )−
Comparing to (10.37) we notice that we essentially have obtained H(s). Match-
ing the values at s = jω0 to ﬁnd g? (and remembering that Ho(jω0) = 1) we
obtain
1
1 + jf
Ho(s) =
H(s)
(1 + jf (ω0))H(jω0)
Considering the second term in the left-hand side of (10.40) we obtain
1 − jf
1 + jf
Ho(s) = g? ·
(1 − jf )±
(1 + jf )±
·
(1 + jf )+
(1 − jf )−
= g? ·
(1 − jf )+
(1 + jf )−
Comparing to (10.38a) we notice that we essentially have obtained He(s). Match-
ing the values at s = jω0 we obtain
1 − jf
1 + jf
Ho(s) =
1 − jf (ω0)
1 + jf (ω0)
He(s)
Thus (10.40) turns into
Ho(s) +
1 − jf (ω0)
1 + jf (ω0)
He(s) = 2
H(s)
(1 + jf (ω0))H(jω0)
or
(1 + jf (ω0))Ho(s) + (1 − jf (ω0))He(s) = 2
H(s)
H(jω0)
(10.41)
Thus we have represented H(s) as a linear combination of Ho(s) and He(s).
If ω0 = 0 and f is an odd function, then f (jω0) = f (0) = 0. Thus we obtain
1 ± jf (ω0) = 1 and H(jω0) = 1/(cid:112)1 + f 2(0) = 1 and therefore (10.41) turns
into
Ho(s) + He(s) = 2H(s)
If the order of f is even, then generally f (0) (cid:54)= 0 and the coeﬃcients of the
linear combination (10.41) are complex. Note that forcing f (0) = 0 or choosing
another ω0 such that f (ω0) = 0 in this case doesn’t help, since the allpasses
themselves are still complex. However, as we already mentioned, this won’t be
a problem for our puproses.
20The identity transfer function 1 in the left-hand side obviously has no poles, but we could
also write it as (1 + jf )/(1 + jf ) in which case it formally has the same poles (which are then
cancelled by the zeros).
10.10. ANALYTIC FILTER
453
10.10 Analytic filter
Sometimes in signal processing we want to deal with the so called analytic
signals, which are deﬁned as signals whose Fourier spectrum doesn’t contain any
negative frequencies (that is the amplitudes of the negative frequency partials
are all zero). Since spectra of real signals must be Hermitian, apparently analytic
signals can’t be real, thus they are essentially complex.
Occasionally there is a need to convert a real signal into an analytic signal
by dropping all of its negative frequency partials. This is very similar to the
lowpass ﬁltering, except that this time we want to dampen not the frequencies
|ω| > 1 but the frequencies ω < 0. Such ﬁlter can be referred to as analytic
filter. The process of removing the negative frequencies from a real signal is also
known as the Hilbert transform, for that reason the analytic ﬁlter is probably
more commonly known under the name Hilbert transformer.
The opposite conversion is simple: we just take the doubled real part of
the analytic signal. That is, given an analytic signal x>0(t) we can restore the
original signal x(t) by
x(t) = 2 Re x>0(t)
(10.42)
This eﬀectively turns each complex partial of the form X(ω)ejωt to a real partial
2·|X(ω)|·cos(ωt+arg X(ω)), which can be equivalently seen as adding a negative
frequency partial X ∗(ω)e−jωt.
Construction from a lowpass
The basic idea of constructing an analytic ﬁlter is simple, we take a unit-cutoﬀ
lowpass ﬁlter (so that the passband is |ω| < 1 and the stopband is |ω| > 1) and
rotate its transfer functions along the imaginary Riemann circle:
H>0(s) = HLP(ρ−j(s))
(10.43)
This eﬀectively rotates the frequency response along the real Riemann circle:
H>0(jω) = HLP(jρ−1(ω))
thereby transforming the passband (−1, 1) to (0, +∞) and the stopband |ω| > 1
to (−∞, 0) (Fig. 10.26).
|H(jω)|
1
∞
−1
0
1
∞
ω
Figure 10.26: Conversion of a unit-cutoﬀ lowpass ﬁlter into an
analytic ﬁlter by a rotation of the real Riemann circle.
454
CHAPTER 10. SPECIAL FILTER TYPES
The poles and zeros of HLP are respectively transformed by the inverse of
ρ−j, which is ρ+j. Therefore the poles ˜pn and zeros ˜zn of H>0 can be explicitly
obtained from the poles pn and zeros zn of HLP by ˜pn = ρ+j(pn), ˜zn = ρ+j(zn).
The gain coeﬃcient of H>0 can be found by equating the frequency responses of
H>0 and HLP at corresponding frequencies, e.g. H>0(j) = HLP(0). Note that
in principle, we can multiply H>0(s) by an arbitrary complex number of unit
magnitude, as this wouldn’t change the ampitude response of H>0. Particularly,
we could let H>0(0) = |HLP(−j)| = 1/(cid:112)1 + f 2(−1).
Parallel allpass implementation
In practical implementation we usually don’t want to deal with complex signals.
In this case the output of the ﬁlter is a fundamentally complex signal, so we
can’t avoid that. However we could try to construct as much as possible of
H>0 staying in real signal domain. Particularly we could attempt to express
H>0 using real ﬁlters, where we then do some postprocessing by mixing the
real outputs of those ﬁlters with possibly complex coeﬃcients. This is indeed
possible.
Suppose HLP is implemented using (9.18). Recall that by (10.41) the lowpass
ﬁlter HLP can be represented as a linear combination of allpasses. This linear
combination must be preserved by the rotation ρ−j in (10.43) giving
2H>0(s)
HLP(ω0)
= (1 + jf (ω0))Ho(ρ−j(s)) + (1 − jf (ω0))He(ρ−j(s))
Since ρ−j rotates along the imaginary axis (in terms of s plane and its
respective Riemann sphere), the allpass property should be preserved by this
transformation and Ho(ρ−j(s)) and Ho(ρ−j(s)) must still be allpasses. It would
be convenient to reexpress Ho(ρ−j(s)) and Ho(ρ−j(s)) in terms of their new
poles after the transformation by ρ−j.
As mentioned, the poles are transformed by ˜pn = ρ+j(pn). Therefore let’s
introduce the new allpasses
˜Ho(s) = ˜go ·
(cid:89)
˜He(s) = ˜ge ·
pn odd
(cid:89)
pn even
s + ˜p∗
n
s − ˜pn
s + ˜p∗
n
s − ˜pn
where ˜go and ˜ge are deﬁned from the conditions ˜Ho(0) = 1, ˜He(0) = 1. Appar-
ently,
Ho(ρ−j(s)) = g? · ˜Ho(s)
He(ρ−j(s)) = g? · ˜He(s)
where g? denote two diﬀerent yet uknown coeﬃcients. Substituting s = 0 into
the above we ﬁnd that these coeﬃcients must be simply equal to Ho(−j) and
He(−j) respectively and therefore
2H>0(s)
HLP(jω0)
= (1 + jf (ω0))Ho(−j) ˜Ho(s) + (1 − jf (ω0))He(−j) ˜He(s)
10.10. ANALYTIC FILTER
455
Up to this point we have been explicitly keeping the freedom of choice of ω0.
This has been done on purpose, as now we can see a good choice for ω0. By
letting ω0 = −1 we have Ho(−j) = 1 and He(−j) = 1 and thereby
2H>0(s)
HLP(−j)
= (1 + jf (−1)) ˜Ho(s) + (1 − jf (−1)) ˜He(s)
or
H>0(s) = HLP(−j) ·
(cid:18) 1 + jf (−1)
2
˜Ho(s) +
1 − jf (−1)
2
(cid:19)
˜He(s)
(10.44)
Equation (10.44) would be an acceptable answer, provided ˜Ho(s) and ˜He(s)
are real ﬁlters. Since ˜Ho(0) = 1 and ˜He(0) = 1 by construction, we only
need to make sure that the poles of each of the ˜Ho(s) and ˜He(s) are conjugate
symmetric.
Recall that the poles of ˜Ho(s) and ˜He(s) are obtained by ˜pn = ρ+j(pn). We
therefore wonder, what would be the relationship between the two preimages
p1, p2 of a conjugate pair of poles ˜p2 = ˜p∗
1. Considering visually the eﬀect of ρ+j
on the Riemann sphere, we could guess that p2 = 1/p∗
1. Verifying algebraically:
˜p2 = ρ+j(p2) = ρ+j(1/p∗
1) = jρ+1(−j/p∗
1) = jρ+1 ((j/p1)∗) = j (ρ+1(j/p1))∗ =
= (−jρ+1(j/p1))∗ = (jρ+1(p1/j))∗ = (jρ+1(−jp1))∗ = (ρ+j(p1))∗ = ˜p∗
1
Therefore, given that poles of HLP have the conjugate reciprocal symmetry
1 (that is, if s is a pole of HLP then so is 1/s∗), the poles of ˜Ho and
p2 = 1/p∗
˜He will have the conjugate symmetry.
The poles of HLP will have the conjugate reciprocal symmetry, given that
f (ω) is a real function such that f (1/x) = 1/f (x). Indeed, suppose f (1/x) =
1/f (x) and 1 + f 2(−js) = 0. Then
1 + f 2(−j/s∗) = 1 + f 2(1/js∗) = 1 + f 2(1/(−js)∗) = 1 + (cid:0)f 2(1/(−js))(cid:1)∗
=
= (cid:0)1 + f 2(1/(−js))(cid:1)∗
=
(cid:18)
1 +
1
f 2(−js)
(cid:19)∗
=
(cid:18) 1 + f 2(−js)
f 2(1/(−js))
(cid:19)∗
= 0∗ = 0
We already know one speciﬁc kind of lowpass ﬁlter where f has such reciprocal
symmetry: the EMQF ﬁlter (with the Butterworth ﬁlter as its limiting case).
Note that EMQF poles not only have conjugate reciprocal symmetry, but are
simply lying on the unit circle, in which case conjugate reciprocation simply
maps the poles to themselves: 1/p∗ = p. Since ρ+j maps the unit circle to the
real axis, the poles ˜pn are real. Also, since ρ+j is a rotation in the direction of
the imaginary axis, it maps left semiplane poles to the left semiplane poles and
thus ˜pn < 0 ∀n. Thus, we are having stable real Ho and He whose poles are
also all real.
Real and imaginary allpasses
The expression (10.44) can be simpliﬁed a bit further. Notice that for an EMQF
ﬁlter we are having f (−1) = (−1)N where N is the ﬁlter’s order. Respectively
456
HLP(−j) = 1/
√
CHAPTER 10. SPECIAL FILTER TYPES
2. Therefore (10.44) turns into
(cid:18) 1 + j
2
(cid:18) 1 − j
2



˜Ho(s) +
˜Ho(s) +
1
√
2
1
√
2
·
·
1 − j
2
1 + j
2
H>0(s) =
(cid:19)
(cid:19)
˜He(s)
˜He(s)
N even
N odd
Recall that we can multiply H>0(s) by any unit-magnitude complex number
without changing the amplitude response. Particulary, we could multiply it by
j1/2 = (1 + j)/
2 obtaining
√
H>0(s) =



˜He(s) + j ˜Ho(s)
2
˜Ho(s) + j ˜He(s)
2
N even
N odd
(10.45)
Thus the real and imaginary parts of the output signal of H>0 are obtained
completely separately from two parallel allpasses ˜Ho and ˜He.
10.11 Phase splitter
There is another, conceptually completely diﬀerent, but closely mathematically
related approach to constructing the Hilbert transformer. Considering a single
positive-frequency complex sinusoidal partial
ejωt = cos ωt + j sin ωt = cos ωt + j cos(ωt − π/2)
(ω > 0)
we notice that the imaginary part of the signal is phase-delayed by 90◦ relatively
to the real part. Now let ˆH>0 denote the analytic ﬁlter operator. That is,
applying ˆH>0 discards the negative frequency partials from the signal. Then,
applying analytic ﬁltering to cos ωt we have
ˆH>0 cos ωt = ˆH>0
ejωt + e−jωt
2
=
ejωt
2
=
cos ωt + j cos(ωt − π/2)
2
(ω > 0)
Respectively, for a general real signal we have
(cid:90) ∞
ˆH>0
a(ω) cos(ωt + ϕ(ω))
dω
2π
=
0
1
2
(cid:90) ∞
0
=
a(ω) cos(ωt + ϕ(ω))
dω
2π
+
j
2
(cid:90) ∞
0
a(ω) cos(ωt + ϕ(ω) − π/2)
dω
2π
Introducing notations:
x(t) =
x−90(t) =
(cid:90) ∞
0
(cid:90) ∞
0
a(ω) cos(ωt + ϕ(ω))
dω
2π
a(ω) cos(ωt + ϕ(ω) − π/2)
dω
2π
(where x−90(t) is the signal x(t) with all real sinusoidal partials phase-shifted
by −90◦) we have
ˆH>0x(t) =
x(t) + jx−90(t)
2
(10.46)
10.11. PHASE SPLITTER
457
Equation (10.46) gives us another approach to the implementation of the ana-
lytic ﬁlter: we take the halved original signal as its own real part, and phase
shift the partials of its real spectrum by −90◦ to obtain the imaginary part.
Also notice that (10.46) is exactly the opposite of (10.42).
Diﬀerently from the approach in Section 10.10 where we didn’t care about
the phase, the approach of (10.46) explicitly preserves the phase of the partials,
thus (10.46) deﬁnes a zero-phase analytic ﬁlter. Unfortunately, as we shall see
later, such ﬁlter cannot be implemented by a stable diﬀerential system. Still, the
whole approach is somewhat more straightforward than the one of Section 10.10.
We will also develop a number of useful explicit expressions, which will be
helpful in the construction of H>0.
In principle, the same expressions could
have been derived in Section 10.10 (as the answers are essentially the same),
however the derivations will be somewhat more direct in the context of the new
approach.
Complex spectral form
Before we get to the construction of the −90◦ phase shifter, we need to reexpress
this phase shifting in terms of complex spectral partials:
ej(ωt+ϕ(ω)) + e−j(ωt+ϕ(ω))(cid:17) dω
2π
x(t) =
(cid:90) ∞
(cid:16)
(cid:16)
ej(ωt+ϕ(ω)−π/2) + e−j(ωt+ϕ(ω)−π/2)(cid:17) dω
2π
=
(cid:16)
e−jπ/2ej(ωt+ϕ(ω)) + ejπ/2e−j(ωt+ϕ(ω))(cid:17) dω
2π
(10.47)
x−90(t) =
a(|ω|)
2
a(|ω|)
2
a(|ω|)
2
0
(cid:90) ∞
0
(cid:90) ∞
0
=
That is we need to phase shift the positive frequency partials by −90◦ and
phase shift the negative frequency partials by +90◦. Since the amplitudes are
unchanged by the phase-shifting, this is an allpass transformation, which we
can denote by the ˆH−90 operator:
x−90(t) = ˆH−90x(t)
Note that the fact that the negative frequencies need to be phase shifted by the
opposite amount is in agreement with the fact that x−90(t), being the imaginary
part of ˆH>0x(t), needs to be a real signal. Therefore ˆH−90 needs to preserve
the hermiticity of the spectrum of x(t). This means that the frequency response
of ˆH−90 must be a Hermitian function, which implies the phase response being
an odd function.
The frequency response of the ˆH−90 allpass is obviously
H−90(jω) =
(cid:40)
−j
j
if ω > 0
if ω < 0
We are still uncertain as to which value to assign to H−90(0).
In principle,
according to (10.47) the zero-frequency partial should be completely killed in
x−90, thus
H−90(jω) = −j sgn ω =



−j
0
j
if ω > 0
if ω = 0
if ω < 0
458
CHAPTER 10. SPECIAL FILTER TYPES
Strictly speaking, this kills the allpass property of ˆH−90 at ω = 0, but this is
actually the only way to keep H−90(jω) Hermitian.
Now we can rewrite (10.46) in the pure operator form
ˆH>0 =
1 + j ˆH−90
2
or in the frequency response form
H>0(jω) =
1 + jH−90(jω)
2
=
1 + sgn ω
2
(10.48)
(10.49)
The complex spectrum interpretation of (10.46) gives another insight into
why does it describe an analytic ﬁlter. Given a positive-frequency complex
sinusoidal signal x(t) = ejωt we have x−90(t) = −jejωt and respectively
ˆH>0x(t) =
ejωt + j · (−j)ejωt
2
=
ejωt + ejωt
2
= x(t)
(ω > 0)
that is x(t) is unchanged by ˆH>0. On the other hand, if ω < 0, then x−90(t) =
jejωt and respectively
ˆH>0x(t) =
ejωt + j · jejωt
2
=
ejωt − ejωt
2
= 0
(ω < 0)
The DC at ω = 0 is neither a positive- nor a negative-frequency partial. Ac-
cording to what we discussed above, it is killed by ˆH−90 and thus
ˆH>01(t) =
1(t) + 0j
2
=
1
2
(ω = 0)
(where 1(t) denotes a signal equal to 1 everywhere).
Rational 90◦ phase shifting allpass
We are looking for an allpass ﬁlter H−90 whose frequency response is
H−90(jω) = −j sgn ω
(10.50)
Apparently H−90(s) can’t be a rational function, since rational functions are
continuous everywhere except at their poles, where they gradually approach
inﬁnity, thus a rational function cannot accommodate a jump from j to −j
which H−90(jω) has at ω = 0. But we still can build a rational H(s) which
approximates the ideal H−90(jω) for s = jω.
As mentioned earlier, the ideal H−90 is an allpass everywhere except at
ω = 0. Since we are building an approximation of H−90 anyway, we can ignore
that fact and build an approximation which is a perfect allpass. This will
simplify our goal, since then we can construct the allpass in terms of its phase
response. Therefore let
ϕ∞(ω) =
(cid:40)
−90◦ ∀ω > 0
+90◦ ∀ω < 0
10.11. PHASE SPLITTER
459
be the ideal phase response of our allpass.21 So, how can we build a rational
allpass transfer function approximating ϕ∞(ω)?
Consider the fact that the frequency response of an allpass can be explicitly
written in terms of its phase response:
Using (9.11a) we can rewrite the same as
H(jω) = ejϕ(ω)
(cid:18)
H(jω) = ρ+1
j tan
(cid:19)
ϕ(ω)
2
However ρ+1, being a 1st-order rational function, maps rational functions to
rational functions of the same order and back. Thus, if we have a rational
function Φ(ω) of some order N such that
Φ(ω) = tan
ϕ(ω)
2
(10.51)
then jΦ(ω) and H(jω) = ρ+1(jΦ(ω)) will also be rational functions of the same
order N and the phase response of H will be equal to ϕ(ω).
Letting s = jω we rewrite H(jω) = ρ+1(jΦ(ω)) as
H(s) = ρ+1(jΦ(−js))
(10.52)
Will H(s) be a real function of s? Since ϕ(ω) must be real odd, so must be
Φ(ω). This implies that it must be representable in the form Φ(ω) = ωΦ2(ω2)
where Φ2 is some other real function. Therefore
H(s) = ρ+1(jΦ(−js)) = ρ+1
(cid:0)j · (−js)Φ2((−js)2)(cid:1) = ρ+1(sΦ2(−s2))
and thus H(s) is real.
Before proceeding to the construction of Φ(ω) we would like to give one
warning. The allpass transfer functions H(s), which will arise from the appli-
cation of (10.52) to the obtained Φ(ω), will be unstable. This corresponds to
the fact that phase responses of stable diﬀerential allpasses cannot stay around
±90◦ over a large range of ω.22 This is a fundamental limitation, which we’ll
have to deal with. Later in this section we will describe a way of addressing this
problem.
Construction of Φ(ω)
The ideal Φ(ω) is apparently
Φ∞(ω) = tan
ϕ∞(ω)
2
=
(cid:40)
−1 ∀ω > 0
∀ω < 0
1
21Since we want our allpass approximation of H−90 to be a real filter, its phase response
must be odd, which leaves only two possible values at ω = 0: ϕ(0) = 0 or ϕ(0) = 180◦. If
we formally include ω = ∞ into the range of frequencies of interest, then we notice that the
phase response at ω = ∞ has the same two options.
22In order to convince oneself that this is indeed so, one could factor a generic stable allpass
tranfer function into 1st- and 2nd-order sections and consider their phase responses, which
are monotonically decaying.
460
CHAPTER 10. SPECIAL FILTER TYPES
We wish to ﬁnd a rational Φ(ω) ≈ Φ∞(ω). This will ensure ϕ(ω) ≈ ϕ∞(ω).
Let f (x) be a real rational function satisfying the unit-cutoﬀ lowpass condi-
tions (9.21). We would like to compose f (x) with other functions in such a way
that the result is an approximation of Φ∞. This composition should still result
in a real rational function and, ideally, also preserve the order of f . Therefore,
good candidates for the elements of such composition are the rotations of real
Riemann circle ρ±1.
As a ﬁrst step, we map the pass- and stop-band areas of f (x) (that is
f (x) ≈ 0 and f (x) ≈ ∞) to the areas where Φ(x) = ±1. This is achieved
by Φ(x) = ρ±1(f (x)) (where the ± signs are matched). We thereby obtain
Φ(x) which has the desired values in the “pass”- and “stop”-bands, however the
bands themselves are incorrectly positioned on the argument axis, still coincid-
ing with the pass- and stop-bands of a unit-cutoﬀ lowpass. We could ﬁx this
by a real Riemann circle rotation of the argument. Which turns our candidate
compositions into
Φ(x) = ρ±1(f (ρ±1(x)))
(10.53)
where we initially treat the ± signs as independent.
However actually the ± signs in (10.53) cannot be independent. E.g. if we
choose the “inner rotation” (the rotation of the argument of f (x)) to be ρ+1,
this maps the original lowpass passband |x| (cid:28) 1 to −∞ (cid:28) x (cid:28) 0.
In this
area we want Φ(x) = 1, therefore we have to choose the “outer rotation” (the
rotation of the value of f (x)) to be ρ+1 as well.
In a similar way we could
choose both rotations to be ρ−1. This means that the ± signs in (10.53) must
be matched.
Intuitively it is clear that any “lowpass” kind of f (x) should result in (10.53)
giving an approximation of Φ∞. However we also need Φ(ω) to be an odd
function. Let’s see what kind of restriction this means for f (x). By (9.13) the
rotations ρ±1 map the odd symmetry to the reciprocal symmetry, which means
that
Φ(−ω) = −Φ(ω) ⇐⇒ f (1/x) = 1/f (x)
which eﬀectively brings us to the idea to use the EMQF function f (x) = ¯RN (x)
(or the Butterworth ﬁlter function f (x) = xN as its limiting case).
Having chosen f (x) = ¯RN (x) we can reﬁne the formula (10.53) a little. Sup-
pose we chose the “+” signs in (10.53). Then Φ(0) = ρ+1(f (1)) = ρ+1(1) = ∞.
Vice versa, if we choose the “−” signs, then Φ(0) = ρ−1(f (−1)) = ρ+1((−1)N )
which is 0 if N is odd and ∞ if N is even. In principle, this is not a very big
problem, and both options are valid, but it would be just nice to have Φ(0) = 0
and respectively H(0) = 1 all the time. This is achieved by changing (10.53)
into
Φ(ω) = −ρ−1(f (ρ+1(ω)))
(10.54)
The readers can convince themselves that (10.54) also gives an approximation
of Φ∞ and that in this case Φ(0) = 0 regardless of N . Fig. 10.27 illustrates.
Explicit expression for EMQF Φ(ω)
Sticking to the idea to use the EMQF function f (x) = ¯RN (x) we will refer
to the 90◦ phase shifter that we are constructing as “EMQF phase shifter”.
Even though this is some kind of a misnomer, this should provide a pretty clear
identiﬁcation of the approach we use.
10.11. PHASE SPLITTER
461
Φ(ω)
∞
1
0
−1
∞
∞
−1
1
∞
ω
Figure 10.27: Φ(ω) obtained from (10.54) and f (x) = ¯RN (x) for
even (solid) and odd (dashed) N .
Substituting f (x) = ¯RN (x) into (10.54) we obtain
Φ(ω) = −ρ−1( ¯RN (ρ+1(ω)))
(10.55)
Since ¯RN is a real rational function of order N , so is Φ(ω).
In the real period-based preimage representation terms we have
x = cdK u
v = N u
¯RN (x) = cd ˜K(v)
Expressing (10.55) in the same terms we have
(cid:0) cdK u(cid:1)
ω = ρ−1
v = N u
Φ(ω) = −ρ−1
(cid:0) cd ˜K v(cid:1)
which by (9.111) turns into
u
2
ω = − ndK2
v
2
u
2
Φ(ω) = nd ˜K2
= N
v
2
Replacing u/2 with u and v/2 with v we obtain
ω = − ndK2 u
v = N u
Φ(ω) = nd ˜K2
v
462
CHAPTER 10. SPECIAL FILTER TYPES
and ﬁnally, switching to the explicit scaling form:
ω = − nd(u, k2)
˜K2
K2
v = N
u =
˜K (cid:48)
2
K (cid:48)
2
Φ(ω) = nd(v, ˜k2)
u
(10.56a)
(10.56b)
(10.56c)
where K2 = K(k2) and ˜K2 = K(˜k2) are the quarter periods corresponding to
the elliptic moduli k2 = L2(k) and ˜k2 = L2(˜k), that is k2 and ˜k2 are obtained by
the double Landen transformation from k and ˜k. Note that, since double Landen
transformation solely changes the quarter period ratios K (cid:48)/K and ˜K (cid:48)/ ˜K by a
factor of 4, the degree equation (9.112) stays essentially the same: ˜K (cid:48)
2/ ˜K2 =
N K (cid:48)
2/K2. Thus the imaginary periods of the two nd functions are matched,
while the real period is scaled by N .
Turning the representation form into the explicit form we obtain, e.g. using
real period argument normalization
Φ(ω) = nd ˜K2
(cid:16)
N nd
−1
K2
(cid:17)
(−ω)
(10.57)
Expression (10.57) deﬁnes another (normalized) elliptic rational function. Dif-
ferently from the already familiar ¯RN , this function has equiripples around ±1
in the bands centered around ω = ±1. Strictly speaking the amplitudes of
the upwards- and downwards-pointing ripples of Φ(ω) (shown in Fig. 10.27)
are not equal, rather, the values are mutually reciprocal at the upwards- and
downward-pointing peaks. It is just that in arctangent scale the reciprocal values
correspond to equal deviations from 1 or from −1. Therefore the true equiripple
behavior occurs in the arctangent rather than linear scale. However according
to (10.51) the function ϕ(ω) (which is our true goal) is exactly the arctangent
scale representation of Φ(ω). Therefore ϕ(ω) will have true equiripples. For the
sake of clarity we provide a graph of ϕ(ω) in Fig. 10.28, however notice that the
only diﬀerence between Figs. 10.28 and 10.27.
is the labelling of the vertical
axis.
Bands of EMQF Φ(ω)
It is instructive to analyse Φ(ω) in terms of its bands in the preimage domain.
The readers can convince themselves that the bands are:
Transition band 1:
Passband 1:
Transition band 2:
Passband 2:
(cid:112)k(cid:48)
2
|ω| ≤ (cid:112)k(cid:48)
2 ≤ω ≤ 1/(cid:112)k(cid:48)
|ω| ≥ 1/(cid:112)k(cid:48)
2 ≤ω ≤ −(cid:112)k(cid:48)
2
2
2
−1/(cid:112)k(cid:48)
where in the transition band 2 we have |Φ(ω)| ≤
|Φ(ω)| ≤
(cid:113)
˜k(cid:48)
2
(cid:113)
(cid:113)
−1/
˜k(cid:48)
2 ≤Φ(ω) ≤ −
˜k(cid:48)
2
depends on N
(cid:113)
˜k(cid:48)
2 ≤Φ(ω) ≤ 1/
(cid:113)
˜k(cid:48)
2
(cid:113)
˜k(cid:48)
2 for even N and |Φ(ω)| ≥
1/
˜k(cid:48)
2 for odd N . Fig. 9.51 can be referred to as an illustration.
It is not diﬃcult to realize that that the “passband” ripples of Φ(ω) are
essentially obtained from the ripples that the nd function has on the real axis
(cid:113)
10.11. PHASE SPLITTER
463
ϕ(ω)
π
π/2
∞
−1
0
1
∞
ω
−π/2
−π
Figure 10.28: ϕ(ω) obtained from (10.57) and (10.51) for even
(solid) and odd (dashed) N .
(which thereby results in the upwards-pointing peaks being reciprocal to the
downwards-pointing peaks) and on a parallel line which is away from the real
axis by one half of its imaginary period. The details are left as an exercise to
the reader.
Poles and zeros of EMQF phase shifter
We would like to construct H(s) from its poles and zeros. Since H(s) is an
allpass, it is suﬃcient to ﬁnd the poles, while the zeros can be trivially obtained
from the poles. However we could also consider obtaining the zeros explicitly.
Starting with the equations H(s) = ∞ and H(s) = 0 we apply the inverted
(10.52), which is Φ(−js) = −jρ−1(H(s)), yielding
Φ(−js) = ∓j
(10.58)
where “−” should be taken for poles and “+” for zeros.
At this point there are diﬀerent possibilities how to continue. Particu-
larly, we could apply (10.55) which gives ¯RN (ρ+1(−js)) = ±j or equivalently
¯RN (−jρ+j(s)) = ±j. This would be pretty much the same as what we have
been solving in Section 10.10.23
It could be more interesting and practical,
though, to take a diﬀerent path, which will allow us to obtain simple explicit
expressions for the poles and zeros of H(s). The obtained poles and zeros will
be of course the same, since we are solving the same equations, just in a diﬀerent
way.
Let’s use the preimage representation (10.56) to solve (10.58), in a similar
way to how we were solving the pole equations for other ﬁlter types. Recall
that by the imaginary argument property, nd is essentially the same as cd, just
rotated 90◦ in its complex argument plane. Therefore, while cd was generating
quasielliptic curves for its argument moving parallel to the real axis, nd will
generate the same curves for its argument moving parallel to the imaginary
23Except that we would obtain both stable and unstable poles this time, since there is no
explicit restriction of the solutions having to be in the left semiplane.
464
CHAPTER 10. SPECIAL FILTER TYPES
In order to solve (10.58), we would like the curves to go through ±j,
axis.
however the movement parallel to the imaginary axis in the preimage domain
is not very useful for solving (10.58), since the imaginary periods are matched
for the preimages of ω and Φ(ω), and therefore we will not obtain all possible
solutions.
We should rather move parallel to the real axis. Apparently, in this case
we won’t generate quasielliptic curves in the representation domain, but rather
the kind of lines shown in Fig. 9.54. Since cd and respectively nd take each
value only once within a quater-period grid cell, and since the values ±j occur
on horizontal lines where nd turns into j sc or −j sc (Fig. 9.51), we need to
move in one of these lines. The representation will then simply move along the
imaginary axis24 in one and the same direction, looping through the ∞ point.
2 as the principal preimage line we
have nd(v, ˜k2) = j sc(Re v, ˜k2). We wish to have representation moving upwards
along the imaginary axis therefore the preimages need to move towards the right
(going along the line Im v = ˜K (cid:48)
2). In terms of u the same movement corresponds
to moving along the line
Choosing the horizontal line Im v = ˜K (cid:48)
Im u =
K (cid:48)
2
˜K (cid:48)
2
Im v = K (cid:48)
2
where the direction of movement of u is, obviously, also towards the right.
Notice that any other possible choices of the principal preimage line of v do not
generate any additional solutions of (10.58), since ω will be simply traversing
along the entire imaginary axis in any case.
The value Φ(ω) = nd(v, ˜k2) moving upwards along the imagniary axis will
be traversing the points ±j at
v = j ˜K (cid:48)
2 +
(cid:19)
˜K2
+ n
(cid:18) 1
2
(n ∈ Z)
where at even n we’ll have Φ(ω) = j and at odd n we’ll have Φ(ω) = −j. That
is, even n correspond to zeros and odd n correspond to poles. The values of u
are respectively
u = j
˜K (cid:48)
2 +
K (cid:48)
2
˜K (cid:48)
2
(cid:19)
+ n
(cid:18) 1
2
K2
N ˜K2
˜K2 = jK (cid:48)
2 +
1
2 + n
N
K2
from where
ω = − nd u = −j sc
(cid:18) 1
2 + n
N
(cid:19)
K2, k2
from where by s = jω we obtain
s = sc
(cid:18) 1
2 + n
N
(cid:19)
K2, k2
(10.59)
where even n correspond to zeros and odd n correspond to poles. Note that in
the Butterworth limit k2 → 0 the equation (10.59) turns into
s = tan
(cid:19)
(cid:18) π
2
·
1
2 + n
N
24Apparently the imaginary axis belongs to the family of lines shown in Fig. 9.54, being the
boundary case between the two groups of lines on the left and on the right.
10.11. PHASE SPLITTER
465
Since all values in (10.59) are real, the solutions given by (10.59) are also
real. That is the poles and zeros of H(s) are real and H(s) can be factored into
1st-order allpasses.
Since the period of sc is 2K2, there are 2N diﬀerent values of s given by
(10.59). Half of them are zeros and the other half are poles, thus there are
N zeros and N poles, where the poles and zeros are inverleaved (Fig. 10.29).
Apparently, n can run over any range of 2N consecutive integers. A particularly
convenient range is n = −N . . . (N − 1). In this case n = −1 and n = 0 give
one pole/zero pair where the pole and the zero are mutually opposite. This
pole/zero pair corresponds to the lowest-cutoﬀ 1-pole allpass factor, which is
stable since the pole is obtained from n = −1. The values n = 1 and n = −2
give another pole/zero pair corresponding to the next 1-pole factor, which is
unstable since the pole is obtained from n = 1. The third 1-pole factor will be
stable again etc.
-5
-4
-3
-2
-1
0
1
2
3
4
5
s
Figure 10.29: Poles (black dots) and zeros (white squares) of an
EMQF phase shifter H(s) for N = 4.
One could notice in Fig. 10.29 that there is reciprocal symmetry within the
set of poles and zeros of H(s). That is if s is a pole or a zero of H(s), then so
is 1/s. Apparently, this is due to the property (9.74b) of the elliptic tangent
function sc.
We could also derive a simple rule for remembering, whether for n = −1 one
obtains a pole or a zero, that is whether the closest to zero negative value of s
given by (10.59) is a stable allpass factor’s pole or an unstable allpass factor’s
zero. First, notice that negating the allpass’s cutoﬀ is equivalent to the sub-
stitution ω ← −ω. Since the frequency response of a real ﬁlter is Hermitian,
its phase response is odd, thus negating ω is equivalent to negating the phase
response. Thus, since the phase responses of stable allpasses are decreasing,
the phase responses of unstable (negative cutoﬀ) allpasses are increasing. Now
consider the phase shifter H(s) which is a product of stable and unstable all-
passes. Intuitively, as ω starts to increase from 0, the phase response ﬁrst has
to decrease to approximately −90◦, therefore the allpass factor with the lowest
cutoﬀ in the chain must be stable.25
Bandwidth of EMQF phase shifter
In our discussion of the bands of EMQF Φ(ω) we have established that the
equiripple “passband” ranges are
<ω < −k(cid:48)1/2
−k(cid:48)−1/2
2
2 <ω < k(cid:48)−1/2
k(cid:48)1/2
2
2
25The same reasoning can be applied to (10.45), where we want the imaginary signal to be
phase shifted by −90◦ compared to the real one. Therefore the lowest-cutoff allpass factor
(corresponding to the pole closest to the origin) must be in the imaginary signal’s allpass.
Since the poles of the allpasses in (10.45) are obtained by Riemann sphere rotation ρ+j , the
pole closest to the origin will be obtained from the pole closest to −j, which is an even pole
for N odd and an odd pole for N even.
466
that is
CHAPTER 10. SPECIAL FILTER TYPES
2 < |ω| < k(cid:48)−1/2
k(cid:48)1/2
where we could notice that the logarithmic center of the “passband” is thereby
at ω = 1.
2
Respectively, the logarithmic bandwidth ∆ expressed in octaves is a loga-
rithm base 2 of the ratio of the passband’s boundaries:
∆ = log2
k(cid:48)−1/2
2
k(cid:48)1/2
2
= log2 k(cid:48)−1
2 = − log2 k(cid:48)
2
which gives us a way to immediately ﬁnd k(cid:48)
2 from a given bandwidth:26
2 = 2−Δ
k(cid:48)
Since the boundaries of the bands of ϕ(ω) are identical to the bands of Φ(ω),
the above formulas equally apply to the bands of ϕ(ω).
The value of ˜k(cid:48)
2, which eﬀectively deﬁnes the amplitude of the ripples, can
be computed (after having constructeed Φ(ω)) from
2 = −Φ(k(cid:48)1/2
˜k(cid:48)1/2
2
)
However it is more practical to directly compute the deviation of arg H(jk(cid:48)1/2
)
from the target value −90◦ (after having constructed H(s)). According to the
above formula, ω = k(cid:48)1/2
should be the point of maximum phase deviation
) = arg H(jk(cid:48)1/2
(within the equiripple range) and thus the deviation of ϕ(k(cid:48)1/2
)
from −90◦ should give the amplitude of the equiripples.
2
2
2
2
Since ˜k(cid:48)
2 and k(cid:48)
2 increase or decrease simultaneously, H(s) will get larger
ripple amplitudes for larger bandwidths and vice versa. Increasing the order N
will result in a smaller ripple amplitude for the same bandwidth.
Apparently the “passband” doesn’t need to be centered at ω = 1 and can
be shifted to any other center frequency by the cutoﬀ substitution s ← s/ωc.
This raises a related question of prewarping, where we could notice that the
situation is pretty similar to the prewarping of a normalized 2-pole bandpass
ﬁlter (discussed in connection with the LP to BP transformation in Section 4.6).
Therefore the suggested way of handling the prewarping of H(s) consists of the
following steps:
1. Given the desired “passband” [ω1, ω2]:
ω1 = ωc · 2−Δ/2
ω2 = ωc · 2Δ/2
prewarp its boundaries separately:
˜ω1 = µ(ω1)
˜ω2 = µ(ω2)
26Note that if desired, we can also find k from k(cid:48)
2 by (9.110) (where we let k0 = k).
10.11. PHASE SPLITTER
467
thereby obtaining the new prewarped “passband” of a diﬀerent bandwidth
and center frequency:
(cid:112)
˜ωc =
˜∆ = log2
˜ω1 ˜ω2
˜ω2
˜ω1
2. Given the new bandwidth and assuming a unit center frequency, construct
the allpass H(s) as previously described in this section.
3. Apply the cutoﬀ substitution s ← s/˜ωc to H(s), which eﬀectively means
multiplying the cutoﬀs of the underlying 1-poles by the new center fre-
quency ˜ωc.
This approach eﬀectively implements an idea similar to the usage of a single
prewarping point discussed in Section 3.8, which takes care of preserving the
correct ratios between the cutoﬀs of the individual ﬁlters in the system. In prin-
ciple it could be okay to prewarp each of the 1-pole factors of H(s) individually
instead, however that apparently will somewhat destroy the optimality of the
equiripple ϕ(ω).
Phase splitter
Half (or approximately half, if N is odd) of the poles of H(s) are unstable and we
can’t implement H(s) directly. However, there is one trick which allows to work
around this limitation. Before describing this trick we will switch the notation
back from H(s) to H−90(s) to highlight the fact that the ﬁlter performs a −90◦
phase shift (of the positive frequencies).
Let’s factor H−90(s) into a product of two allpasses:
H−90(s) = H+(s)H−(s)
where H+(s) contains only the right-semiplane (unstable) poles and H−(s) con-
tains only the left-semiplane (stable) poles. As usual, we could assume or re-
quire that H+(0) = 1 and H−(0) = 1, which is achievable, given ϕ(ω) = 0 and
respectively H(0) = 1.
Given a signal x(t) = est we wish to obtain the signal y(t) = H−90(s)x(t).
Consider two other signals:
x(cid:48)(t) = H −1
y(cid:48)(t) = H −1
+ (s)x(t)
+ (s)y(t) = H−(s)x(t)
+ (s) = 1/H+(s). Notice that H −1
where H −1
+ is a stable allpass and so is appar-
ently H−, thus x(cid:48)(t) and y(cid:48)(t) can be obtained from x(t) by processing x(t) by
stable allpasses H −1
+ and H−. Notice that
y(cid:48)(t) = H−(s)x(t) = H−(s)H+(s)x(cid:48)(t) = H−90(s)x(cid:48)(t)
that is y(cid:48)(t) and x(cid:48)(t) are in a 90◦ phase shift relationship.
Apparently the same idea applies to abitrary x(t), which we can express in
the operator notation as
x(cid:48)(t) = ˆH −1
+ x(t)
(10.60a)
468
CHAPTER 10. SPECIAL FILTER TYPES
+ y(t) = ˆH−x(t)
y(cid:48)(t) = ˆH −1
y(cid:48)(t) = ˆH−90x(cid:48)(t)
(10.60b)
(10.60c)
where ˆH−90 is the operator denoting the processing of a signal by the ﬁlter
H−90. Thus, even though we cannot phase-shift the input signal x by 90◦, we
can obtain two derived allpass signals x(cid:48) and y(cid:48), where the phase diﬀerence
between x(cid:48) and y(cid:48) is 90◦. Respectively, the combined signal
x>0(t) =
x(cid:48)(t) + jy(cid:48)(t)
2
=
ˆH −1
+ + j ˆH−
2
x(t) =
= ˆH −1
+
x(t) + jy(t)
2
= ˆH −1
+
1 + j ˆH−90
2
x(t)
(10.61)
is an analytic version of x(t), where the phase shift of this analytic version
relatively to x(t) is deﬁned by H −1
+ . The approach of generating two allpass
signals which are in a 90◦ phase relationship is referred to as phase splitting and
is illustrated in Fig. 10.30. Since x(cid:48)/2 is the real part of the analytic signal and
y(cid:48)/2 is the imaginary part, the allpass H −1
+ produces the (doubled) real part
and the allpass H− produces the (doubled) imaginary part and therefore we can
refer to H −1
+ and H− as real and imaginary allpasses respectively.
x(t)
•(cid:47)
H −1
+
x(cid:48)(t)
H−
y(cid:48)(t)
Figure 10.30: Phase splitter.
Notice that (10.61) is essentially the same as we have in (10.45), where Ho
and He are corresponding to H −1
+ and H− (where which speciﬁc ﬁlter corre-
sponds to which depends on the order N ). Thus (10.45) also describes a phase
splitter, just obtained from a diﬀerent angle.
10.12 Frequency shifter
Even though frequency shifter is not a ﬁlter in the strict sense, its most critical
part will be based around the Hilbert transformer, which is a ﬁlter. For that
reason the discussion of frequency shifters may belong to the ﬁlter topic.
Suppose we are given a signal x(t) represented by its complex spectrum:
x(t) =
(cid:90) ∞
−∞
X(ω)ejωt dω
2π
By multiplying the signal x(t) with a complex sinusoidal signal ejΔω·t we eﬀec-
tively shift the frequencies of all partials by ∆ω:
y(t) = ejΔω·tx(t) = ejΔω·t
(cid:90) ∞
−∞
X(ω)ejωt dω
2π
=
(cid:90) ∞
−∞
X(ω)ejΔω·tejωt dω
2π
=
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
10.12. FREQUENCY SHIFTER
=
(cid:90) ∞
−∞
X(ω)ej(ω+Δω)t dω
2π
469
(10.62)
This is not very interesting, since given a real x(t) we obtain a complex y(t).
Obviously, it’s because we multiplied by the complex signal ejΔω·t. In terms
of signal spectra, the spectrum of x(t) was Hermitian, however by shifting the
spectrum by ∆ω we destroyed the Hermitian propetry.
However, this is also not exactly what we want if we think of frequency shift-
ing. The complex spectrum is a more or less purely mathematical concept, while
the one more intuitively related to our hearing of sounds is the real spectrum,
and it’s the partials of the real spectrum whose frequencies we’d rather want to
shift. That is, given
x(t) =
(cid:90) ∞
0
a(ω) cos(cid:0)ωt + ϕ(ω)(cid:1) dω
2π
we wish to obtain
y(t) =
(cid:90) ∞
0
a(ω) cos(cid:0)(ω + ∆ω)t + ϕ(ω)(cid:1) dω
2π
(10.63)
Notably, if ∆ω < 0, then some of the frequencies ω + ∆ω in (10.63) will be
negative and will alias with the positive frequencies of the same absolute mag-
nitude. This can be either ignored, or x(t) can be preﬁltered to make sure it
doesn’t contain frequencies below −∆ω. So, except for the just mentioned high-
pass preﬁltering option, the possible aliasing of the negative frequencies doesn’t
aﬀect the subsequent discussion.
We can rewrite (10.63) as
a(ω) cos(cid:0)(ω + ∆ω)t + ϕ(ω)(cid:1) dω
2π
a(ω) cos(cid:0)∆ωt + ωt + ϕ(ω)(cid:1) dω
2π
=
=
y(t) =
=
=
(cid:90) ∞
0
(cid:90) ∞
0
(cid:90) ∞
0
= cos ∆ωt ·
= cos ∆ωt ·
0
(cid:90) ∞
0
a(ω)
(cid:16)
cos ∆ωt cos(cid:0)ωt + ϕ(ω)(cid:1) − sin ∆ωt sin(cid:0)ωt + ϕ(ω)(cid:1)(cid:17) dω
2π
(cid:90) ∞
=
a(ω) cos(cid:0)ωt + ϕ(ω)(cid:1) dω
2π
−
− sin ∆ωt ·
(cid:90) ∞
0
a(ω) sin(cid:0)ωt + ϕ(ω)(cid:1) dω
2π
=
a(ω) cos(cid:0)ωt + ϕ(ω)(cid:1) dω
2π
−
− sin ∆ωt ·
(cid:90) ∞
(cid:16)
a(ω) cos
0
= x(t) cos ∆ωt − x−90(t) sin ∆ωt
ωt + ϕ(ω) −
π
2
(cid:17) dω
2π
=
(10.64)
where
x−90(t) =
(cid:90) ∞
0
(cid:16)
a(ω) cos
ωt + ϕ(ω) −
π
2
(cid:17) dω
2π
is a signal obtained from x(t) by phase-shifting all partials by −90◦.
In the
470
CHAPTER 10. SPECIAL FILTER TYPES
operator notation the same can be expressed as
y(t) = cos ∆ωt · x(t) − sin ∆ωt · ˆH−90x(t) =
(cid:16)
cos ∆ωt − sin ∆ωt · ˆH−90
(cid:17)
x(t)
(10.65)
We have already found out how to obtain a −90◦ phase shifted signal in
Section 10.11, except that we also found than such signal cannot be directly
obtained. We will address this slightly later, while for now we shall take a
diﬀerent look at the same problem of frequency shifting.
Analytic signal approach
Looking again at (10.62) we can notice that the positive frequency partials are
correctly shifted and it’s the negative frequency partials which make trouble.
So, if the negative partials weren’t there in the ﬁrst place:
x>0(t) =
(cid:90) ∞
0
X(ω)ejωt dω
2π
we would have obtained
y>0(t) = ejΔω·tx>0(t) = ejΔω·t
(cid:90) ∞
0
X(ω)ejωt dω
2π
=
(cid:90) ∞
0
X(ω)ejΔω·tejωt dω
2π
=
=
(cid:90) ∞
0
X(ω)ej(ω+Δω)t dω
2π
(10.66)
Comparing (10.66) to (10.63) we notice that they essentially consist of the same
frequency partials, except that y>0(t) is missing the negative part of its spec-
trum. The negative part of the spectrum can be restored by (10.42), and thus
(10.66) and (10.63) are related via
This is easier to see in the operator notation:
y(t) = 2 Re y>0(t)
y(t) = 2 Re y>0(t) = 2 Re (cid:0)ejΔωtx>0(t)(cid:1) = 2 Re
(cid:32)
ejΔωt 1 + j ˆH−90
= 2 Re
= Re
x(t)
(cid:33)
(cid:16)
2
(cid:16)
(cid:17)
ejΔωt ˆH>0x(t)
=
(cid:17)
ejΔωt(1 + j ˆH−90)
x(t) =
(cid:17)
(cid:16)
((cid:48)cos∆ωt + j sin ∆ωt)(1 + j ˆH−90)
x(t) =
= Re
(cid:16)
=
cos ∆ωt − sin ∆ωt ˆH−90
(cid:17)
x(t)
which is identical to (10.65), thus both approaches are equivalent.
Implementation
Let ˆH −1
+ be the allpass from (10.60). Multiplying (10.65) by ˆH −1
+ we obtain
ˆH −1
+ y(t) = ˆH −1
+
(cid:16)
cos ∆ωt − sin ∆ωt · ˆH−90
(cid:17)
(cid:16)
=
cos ∆ωt · ˆH −1
+ − sin ∆ωt · ˆH−90 ˆH −1
+
x(t) =
x(t) =
(cid:17)
10.12. FREQUENCY SHIFTER
(cid:16)
=
cos ∆ωt · ˆH −1
+ − sin ∆ωt · ˆH−
(cid:17)
x(t)
471
(10.67)
If we are willing to accept the phase-shifted signal ˆH −1
+ y(t) instead of y(t) (and
as it seems, we don’t have much other choice) a frequency shifter can be simply
implemented by the structure in Fig. 10.31.
x(t)
•(cid:47)
H −1
+
H−
y(t)
cos ∆ωt
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
sin ∆ωt
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 10.31: Frequency shifter.
Notably, replacing ∆ω by −∆ω in (10.67) we obtain
ˆH −1
+ y(t) = ˆH −1
+
(cid:16)
cos ∆ωt + sin ∆ωt · ˆH−90
(cid:17)
+ + sin ∆ωt · ˆH−
cos ∆ωt · ˆH −1
(cid:16)
=
(cid:17)
x(t) =
x(t)
(10.68)
This means that we can extend the frequency shifter in Fig. 10.31 to a one that
shifts simultaneously in both directions, obtaining the diagram in Fig. 10.32.27
x(t)
•(cid:47)
H −1
+
H−
cos ∆ωt
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
sin ∆ωt
•(cid:47)
(cid:31)(cid:63)(cid:63)(cid:63)(cid:63)(cid:63)(cid:63)(cid:63)(cid:63)(cid:63)(cid:63)
(cid:63)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)(cid:127)
•
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
+
y+(t)
y−(t)
Figure 10.32: A bidirectional frequency shifter.
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Adding together the frequency-shifted signals from (10.67) and (10.68) we
notice that
(cid:16)
ˆH −1
+
cos ∆ωt − sin ∆ωt · ˆH−90
(cid:16)
(cid:17)
+
+ ˆH −1
+
cos ∆ωt + sin ∆ωt · ˆH−90
(cid:17)
= ˆH −1
+ · 2 cos ∆ωt
or
(cid:16)
cos ∆ωt · ˆH −1
+ − sin ∆ωt · ˆH−
(cid:16)
cos ∆ωt · ˆH −1
+
(cid:17)
+
+ + sin ∆ωt · ˆH−
(cid:17)
= ˆH −1
+ · 2 cos ∆ωt
27The signal notations y+ and y= denote the positive- and negative-shifted signals respec-
+ and H− which
tively and shouldn’t be confused with the “+” and “−” subscripts of H −1
denote the stable and unstable poles.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:63)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:31)
(cid:47)
(cid:47)
472
CHAPTER 10. SPECIAL FILTER TYPES
That is, the sum of y+ and y− in Fig. 10.32 essentially produces the ring mod-
ulation of x(t) by cos ∆ωt, except that the result of this ring modulation is
doubled and phase-shifted by ˆH −1
+ . So frequency-shifting and ring-modulation
by a sinusoid seem are very closely related. The same can be analyzed in the
complex spectral domain:
cos ∆ωt · x(t) =
=
=
(cid:90) ∞
−∞
(cid:90) ∞
ejΔωt + e−jΔωt
2
X(ω)ejωtejΔωt dω
2π
X(ω)ej(ω+Δω)t dω
2π
−∞
(cid:90) ∞
1
2
1
2
−∞
X(ω)ejωt dω
2π
(cid:90) ∞
=
+
+
1
2
1
2
−∞
(cid:90) ∞
−∞
X(ω)ejωte−jΔωt dω
2π
X(ω)ej(ω−Δω)t dω
2π
=
Thus in the case of the ring modulation by a sinusoid, the partials are frequency-
shifted in both directions.
Aliasing
If ∆ω > 0 then for some partials the sum ω + ∆ω may exceed the Nyquist
frequency, respectively they will alias to 2π − (ω + ∆ω) (assuming unit sampling
period T = 1). This kind of aliasing is similar to the one occurring at ω+∆ω < 0
in case of ∆ω < 0, however, while the aliasing around ω = 0 also occurs in the
analog case, aliasing around Nyquist frequency is a purely digital phenomenon.
It is therefore up to the eﬀect designer, whether the aliasing around ω = 0
should be prevented, or allowed. The aliasing at Nyquist is however usually
undersired. It can be avoided by preﬁltering the frequency band [π − ∆ω, π],
which can be done by a lowpass ﬁlter with a cutoﬀ around π − ∆ω. Notice that
π − ∆ω is a discrete-time cutoﬀ value and thus doesn’t need prewarping.
The aliasing around ω = 0 can be prevented in a similar way by using a
highpass with a cutoﬀ at −∆ω (since in this case we assume ∆ω < 0, the
cutoﬀ will thereby be positive). Note that since the phase splitter has a limited
bandwidth, one also may consider ﬁltering out the signal outside that bandwidth
anyway, regardless of ∆ω.
10.13 Remez algorithm
The equiripple behavior of Chebyshev polynomials and elliptic rational functions
is a characteristic feature of the so-called minimax approximations. TN , TN ,
RN , ¯RN and the function Φ(ω) used to build the phase splitter all provide
speciﬁc analytic-form solutions to speciﬁc minimax problems. However, in a
more general situation we might want a numerical solution approach.28
Suppose we are given a function f (x) and its approximation ˜f (x). There
are diﬀerent ways to measure the quality of the approximation. One way to
28The description of Remez algorithm (which is a numerical minimax optimization algo-
rithm) was included into earlier revisions of this book as an alternative to the use of elliptic
functions to construct phase splitters. Now that the book is strongly focusing on elliptic
functions anyway, the discussion of Remez algorithm might feel almost redundant. However
the author felt that this is still quite valuable resource to be simply dropped from the book.
Particularly, Remez algorithm is useful for building low-cost approximations of functions, al-
though, depending on the context, minimax solutions are not necessarily the best ones for a
given purpose.
10.13. REMEZ ALGORITHM
473
measure this quality is the maximum error of the approximation on the given
interval of interest x ∈ [a, b]:
E = max
[a,b]
(cid:12)
(cid:12)
(cid:12)
˜f (x) − f (x)
(cid:12)
(cid:12)
(cid:12)
(10.69)
We therefore wish to minimize the value of E. That is we want to minimize the
maximum error of the approximation. Such approximations are hence called
minimax approximations.29
Gradient search methods do not work well for minimax optimizations. There-
fore a diﬀerent method, called Remez algorithm,30 needs to be used. As of today,
internet resources concerning the Remez algorithm seem quite scarce, nor does
this method seem to be a subject of common math textbooks. This might
suggest that Remez algorithm belongs to a rather esoteric math area. The al-
gorithm itself, however, is very simple. We will therefore cover the essentials of
that algorithm in this book.31
Suppose ˜f (x) is a polynomial:
˜f (x) =
N
(cid:88)
n=0
anxn
(10.70)
Apparently, there are N +1 degrees of freedom in the choice of ˜f (x), each degree
corresponding to one of the coeﬃcients an. Therefore we can force the function
˜f (x) to take arbitrarily speciﬁed values at N + 1 arbitrarily chosen points ¯xn.
Particularly, we can require
˜f (¯xn) = f (¯xn)
n = 0, . . . , N
or equivalently require the error to be zero at ¯xn:
˜f (¯xn) − f (¯xn) = 0
n = 0, . . . , N
(10.71)
(notice that the equations (10.71) are linear in respect to the unknowns an
and therefore are easily solvable). If the points ¯xn are approximately uniformly
spread over the interval of interest [a, b] then intuitively we can expect ˜f (x) to
be a reasonably good approximation of f (x) (Fig. 10.33).
This based on the uniform zero spacing approximation is however not the
best one. Indeed, instead let ¯xn equal the (properly scaled) zeros of the Cheby-
shev polynomial of order N + 1:
¯xn =
a + b
2
TN +1(zn) = cos(cid:0)(N + 1) arccos zn
b − a
2
zn
+
¯xn ∈ (a, b)
zn ∈ (−1, 1)
(cid:1) = 0
zn = − cos
1
2 + n
N + 1
π
n = 0, . . . , N
29The maximum of the absolute value of a function is also the L∞ norm of the function.
Therefore minimax approximations are optimizations of the L∞ norm.
30The Remez algorithm should not be confused with the Parks–McClellan algorithm. The
latter is a specific restricted version of the former. For whatever reason, the Parks–McClellan
algorithm is often referred to as the Remez algorithm in the signal processing literature.
31The author’s primary resource for the information about the Remez algorithm was the
documentation for the math toolkit of the boost library by J.Maddock, P.A.Bristow, H.Holin
and X.Zhang.
474
CHAPTER 10. SPECIAL FILTER TYPES
˜f (x) − f (x)
5 × 10−4
0
π
2
x
Figure 10.33: The error of the 4-th order polynomial approxima-
tions of sin x on [0, π/2]. The approximation with uniformly spaced
zeros at 9◦, 27◦, 45◦, 63◦, 81◦ (solid line) and the one with Cheby-
shev zeros (dashed line). The empty square-shaped dots at the
extrema of the error are the control points of the Remez algorithm.
where the minus sign in front of the cosine ensures that zn are in ascending order.
Comparing Chebyshev zeros approximation (the dashed line in Fig. 10.33) to
the uniform zeros approximation, we can see that the former is much better
than the latter, at least in the minimax sense.
A noticeable property of the Chebyshev zeros approximation clearly observ-
able in Fig. 10.33 is that the extrema of the approximation error (counting the
extrema at the boundaries of the interval [a, b]!) are approximately equal in
absolute magnitude and have alternating signs. This is a characteristic trait of
minimax approximations: the error extrema are equal in magnitude and alter-
nating in sign.
So, we might attempt to build a minimax approximation by trying to sat-
isfy the equiripple error oscillation requirement. That is, instead of seeking
to minimize the maximum error, we simply seek an error which oscillates be-
tween the two boundaries of opposite sign and equal absolute value. Somewhat
surprisingly, this is a much simpler task.
Intuitive description of Remez algorithm
Consider the solid line graph in Fig. 10.33. Intuitively, imagine a “control point”
at each of the extrema. Now we “take” the control point which has the largest
error (the one at x = 0) and attempt to move it towards the x axis, reducing
the error value at x = 0. Since there are 6 control points (4 at local extrema
plus 2 at the boundaries), but only 5 degrees of freedom (corresponding to
the coeﬃcients an), at least one of the other control points needs to move (or
several or all of them can move). Intuitively it’s clear that if we lower the error
at x = 0, then it will grow at some other points of [a, b]. However, since we have
10.13. REMEZ ALGORITHM
475
the largest error at x = 0 anyway, we can aﬀord the error growing elsewhere
on [a, b], at least for a while. Notice that during such change the x positions of
control points will also change, since the extrema of the error do not have to
stay at the same x coordinates.
As the error elsewhere at [a, b] becomes equal in absolute magnitude to the
one at x = 0, we have two largest-error control points which need to be moved
simultaneously from now on. This can be continued until only one “free” control
point remains. Simultaneously reducing the error at 5 of 6 control points we
thereby increase the error at the remaining control point. At some moment both
errors will become equal in absolute magnitude, which means that the error at
all control points is equal in absolute magnitude. Since the control points are
located at the error extrema, we have thereby an equiripple oscillating error.
Remez algorithm for polynomial approximation
Given ˜f (x) which is a polynomial (10.70), the process of “pushing the control
points towards zero” has a simple algorithmic expression. Indeed, we seek ˜f (x)
which satisﬁes
˜f (ˆxn) + (−1)nε = f (ˆxn)
n = 0, . . . , N + 1
(10.72)
where ˆxn are the (unknown) control points (including ˆx0 = a and ˆxN +1 = b)
and ε is the (unknown) signed maximum error. Thus, the unknowns in (10.72)
are an (the polynomial coeﬃcients), ˆxn (the control points at the extrema) and
ε (the signed maximum error). Notice that the equations (10.72) are linear in
respect to an and ε, which leads us to the following idea.
Suppose we already have some initial guess for ˜f (x), like the uniform zero
polynomial in Fig. 10.33 (or the Chebyshev zero polynomial, which is even
better). Identifying the extrema of ˜f (x) − f (x) we obtain a set of control points
ˆxn. Now, given these ˆxn, we simply solve (10.72) for an and ε (where we have
N + 2 equations and N + 2 unknowns in total), thereby obtaining a new set of
an. In a way this is cheating, because ˆxn are not the control points anymore,
since they are not anymore the extrema of the error (and if they were, we
would already have obtained a minimax approximation by simply ﬁnding these
new an). However, the polynomial deﬁned by the new an has a much better
maximum error (Fig. 10.34)!
So we simply update the control points ˆxn to the new positions of the ex-
trema and solve (10.72) again. Then again update the control points and solve
(10.72) and so on. This is the Remez algorithm for polynomial approximation.
We still need to reﬁne some details about the algorithm though.
- The function f (x) should be reasonably well-behaved (whatever that could
mean) in order for Remez algorithm to work.
- As a termination condition for the iteration we can simply check the
equiripple property of the error at the control points. That is, having
obtained the new an, we ﬁnd the new control points ˆxn and then compute
the errors εn = ˜f (ˆxn) − f (ˆxn). If the absolute values of εn are equal up to
the speciﬁed precision, this means that we have an approximation which
is minimax up to the speciﬁed error, and the algorithm may be stopped.
476
CHAPTER 10. SPECIAL FILTER TYPES
˜f (x) − f (x)
5 × 10−4
0
π
2
x
Figure 10.34: The approximation error before (dashed line) and
after (solid line) a single step of the Remez polynomial approxi-
mation algorithm. The empty square-shaped dots are the control
points.
- The initial approximation ˜f (x) needs to have the alternating sign prop-
erty. This is more or less ensured by using (10.71) to construct the initial
approximation. A good choice for ¯xn (as demonstrated by Fig. 10.33) are
the roots of the Chebyshev polynomial of order one higher than the order
of the approximating polynomial ˜f (x).32
- The control points ˆxn are the zeros of the error derivative ( ˜f − f )(cid:48) (except
for ˆx0 = a and ˆxN +1 = b). There is exactly one local extremum on each
interval (¯xn, ¯xn+1) between the zeros of the error. Therefore, ˆxn+1 can
be simply found as the zeros of the error derivative by bisection of the
intervals (¯xn, ¯xn+1).
- After having obtained new an, the old control points ˆxn are not the ex-
trema anymore, however the errors at ˆxn are still alternating in sign.
Therefore the new zeros ¯xn (needed to ﬁnd the new control points by
bisection) can be found by bisection of the intervals (ˆxn, ˆxn+1).
Restrictions and variations
Often it is desired to obtain a function which is odd or even, or has some
other restrictions. This can be done by simply ﬁxing the respective an, thereby
reducing the number of control variables an and reducing the number of control
points ˆxn and zero crossings ¯xn accordingly.
Remez algorithm can also be easily modiﬁed to accommodate a weight func-
32This becomes kind of intuitive after considering Chebyshev polynomials as some kind of
minimax approximations of the zero constant function f (x) ≡ 0 on the interval [−1, 1].
10.13. REMEZ ALGORITHM
477
tion in the minimax norm (10.69):
E = max
[a,b]
(cid:16)
W (x) ·
(cid:12)
(cid:12)
(cid:12)
(cid:12)
˜f (x) − f (x)
(cid:12)
(cid:12)
(cid:17)
W (x) > 0
The error function therefore turns into W (x)( ˜f (x) − f (x)), while the minimax
equations (10.72) turn into
˜f (ˆxn) + (−1)nW −1(ˆxn)ε = f (ˆxn)
n = 0, . . . , N + 1
(where W −1(x) is the reciprocal of W (x)).
Remez algorithm for rational approximation
Instead of using a polynomial ˜f (x), better approximations can be often achieved
by rational ˜f (x):
˜f (x) =
N
(cid:88)
n=0
anxn
1 +
M
(cid:88)
n=1
bnxn
(10.73)
Besides being able to deliver better approximations in certain cases, rational
functions can be often useful for obtaining approximations on inﬁnite intervals
such as [a, +∞), because by varying the degrees of the numerator and denomi-
nator the asymptotic behavior of ˜f (x) at x → ∞ can be controlled.
For a rational ˜f (x) deﬁned by (10.73) the minimax equations (10.72) become
nonlinear in respect to the unknowns ε and bn, although they are still linear in
respect to the unknowns an:
N
(cid:88)
i=0
(cid:32)
ai ˆxi
n + (−1)n
1 +
M
(cid:88)
i=1
(cid:33)
(cid:32)
bi ˆxi
n
ε =
1 +
(cid:33)
bi ˆxi
n
f (ˆxn)
M
(cid:88)
i=1
(10.74)
n = 0, . . . , N + M + 1
Notice that the number of degrees of freedom is now N + M + 1. The equations
(10.74) can be solved using diﬀerent numeric methods for nonlinear equation
solution, however there is one simple trick.33 Rewrite (10.74) as
N
(cid:88)
i=0
ai ˆxi
n + (−1)nε
M
(cid:88)
i=1
(cid:32)
bi ˆxi
n + (−1)nε =
1 +
(cid:33)
bi ˆxi
n
f (ˆxn)
M
(cid:88)
i=1
Now we pretend we don’t know the free term ε, but we do know the value of ε
before the sum of bi ˆxi
n:
N
(cid:88)
i=0
ai ˆxi
n + (−1)nε0
(cid:32)
bi ˆxi
n + (−1)nε =
1 +
M
(cid:88)
i=1
M
(cid:88)
i=1
(cid:33)
bi ˆxi
n
f (ˆxn)
(10.75)
where ε0 is this “known” value of ε. The value of ε0 can be estimated e.g. as
the average absolute error at the control points ˆxn. Then (10.75) are linear
33This trick is adapted from the boost library documentation and sources.
478
CHAPTER 10. SPECIAL FILTER TYPES
equations in respect to an, bn and ε and can be easily solved. Having obtained
the new an and bn, we can obtain a new estimation for ε0 and solve (10.75)
again. We repeat until the errors ˜f (ˆxn) − f (ˆxn) at the control points ˆxn become
equal in absolute vlaue up to a necessary precision. At this point we can consider
the solution of (10.74) as being obtained to a suﬃcient precision and proceed
with the usual Remez algorithm routine (ﬁnd the new ¯xn, new ˆxn etc.)
Here are some further notes.
- In principle the solution of (10.74) doesn’t need to be obtained to a very
high precision, except in the ﬁnal step of the Remez algorithm. However,
in order to know whether the current step is the ﬁnal one or not, we
need to know the true control points, so that we can estimate how well
the equiripple condition is satisﬁed. Ultimately, this is a question of the
computational expense of ﬁnding the new control points vs. computing
another iteration of (10.75).
- Sometimes, if the equations are strongly nonlinear, the trick (10.75) may
fail to converge. In this case one could attempt to use the discussed be-
low more general Newton–Raphson approach (10.81), where the damping
parameter may be used to mitigate the convergence problems.
- In regards to the problem of choice of the initial ˜f (x) for the rational
Remez approximation, notice that the zero error equations (10.71) take
the form
N
(cid:88)
an ¯xn = f (¯xn)
1 +
bn ¯xn
(cid:32)
M
(cid:88)
(cid:33)
which is fully linear in respect to an and bn, and can be easily solved.
n=0
n=1
Other kinds of approximating functions
In certain cases one could use even more complicated forms of ˜f (x), which
are neither polynomial nor rational. In the general case such function ˜f (x) is
controlled by a number of parameters an:
˜f (x) = ˜f (x, a1, a2, . . . , aN )
(notice that this time the numbering of an is starting at one, so that there are
N parameters in total, giving N degrees of freedom). The minimax equations
(10.72) become
˜f (ˆxn, a1, a2, . . . , aN ) + (−1)nε = f (ˆxn)
n = 0, . . . , N
(10.76)
Introducing functions
φn(a1, a2, . . . , aN , ε) = ˜f (ˆxn, a1, a2, . . . , aN ) + (−1)nε − f (ˆxn)
we rewrite the equations (10.76) as
φn(a1, a2, . . . , aN , ε) = 0
n = 0, . . . , N
(10.77)
Introducing vector notation
Φ = (cid:0)φ0 φ1
. . . φN
(cid:1)T
10.13. REMEZ ALGORITHM
479
a = (cid:0)a1 a2
. . . aN ε(cid:1)T
we rewrite (10.77) as
Φ(a) = 0
(10.78)
Apparently, (10.78) is a vector form of (10.72), except that now we consider
it as a generally nonlinear equation. Both the function’s argument a and the
function’s value Φ(a) have the dimension N + 1, therefore the equation (10.78)
is fully deﬁned.
Diﬀerent numeric methods can be applied to solving (10.78). We will be
particularly interested in the application of multidimensional Newton–Raphson
method. Expanding Φ(a) into Taylor series at some ﬁxed point a0 we transform
(10.78) into:
Φ(a0) +
∂Φ
∂a
(a0) · ∆a + o(∆a) = 0
(10.79)
where ∂Φ/∂a is the Jacobian matrix and a = a0 +∆a. By discarding the higher
order terms o(∆a), the equation (10.79) is turned into
∆a = −
(cid:18) ∂Φ
∂a
(cid:19)−1
(a0)
· Φ(a0)
The equation (10.80) implies the Newton–Raphson iteration scheme
an+1 = an − α ·
(cid:18) ∂Φ
∂a
(cid:19)−1
(an)
· Φ(an)
(10.80)
(10.81)
where the damping factor α is either set to unity, or to a lower value, if the
nonlinearity of Φ(a) is too strong and prevents the iterations from convergening.
The initial value a0 is obtained from the initial settings of the parameters an
and the estimated initial value of ε. As for the rational ˜f (x), the initial value
of ε can be estimated e.g. as the average error at the control points.
Similarly to the rational approximation case, the solution of (10.78) doesn’t
need to be obtained to a very high precision during the intermediate steps of the
Remez algorithm. However the same tradeoﬀ between computing the iteration
step (10.81) and ﬁnding the new control points applies.
The choice of the initial ˜f (x) can be done based on the same principles. The
zero error equations (10.71) turn into
φn(a1, a2, . . . , aN , 0) = 0
n = 1, . . . , N
(notice that compared to (10.77) we have set ε to zero and we have N rather
than N + 1 equations). Letting
¯Φ = (cid:0)φ1 φ2
¯a = (cid:0)a1 a2
. . . φN
(cid:1)T
. . . aN
(cid:1)T
we have an N -dimensional nonlinear equation
¯Φ(¯a) = 0
which can be solved by the same Newton–Raphson method:
¯an+1 = ¯an − α ·
(cid:19)−1
(cid:18) ∂ ¯Φ
∂¯a
(¯a0)
· ¯Φ(¯a0)
(10.82)
480
CHAPTER 10. SPECIAL FILTER TYPES
10.14 Numerical construction of phase splitter
For the sake of a demonstration example we are now going to use Remez algo-
rithm to build an approximation of the ideal 90◦ allpass phase shifter deﬁned
by (10.50), while deliberately staying away from the entire framework of el-
liptic functions. The obtained results shall be identical to the ones previously
obtained analytically.
We will retain the mentioned allpass property in the approximation, there-
fore let H(s) denote the allpass which should approximate the ideal phase shifter
(10.50). Using serial decomposition, H(s) can be decomposed into series of 2-
and 1-pole allpasses. Since we aim to have H(s) with as ﬂat (actually, constant
in the range of interest) phase response as possible, 2-poles seem to be less use-
ful than 1-poles, due to steeper phase responses of the former (Figs. 10.35 and
10.36).
Restricting ourselves to using just 1-poles we have:
H(s) =
N
(cid:89)
n=1
An(s) =
N
(cid:89)
n=1
ωn − s
ωn + s
(10.83)
where ωn are the cutoﬀs of the 1-pole allpasses An(s). Notice that the speciﬁc
form of specifying H(s) in (10.83) ensures H(0) = 1 ∀N , that is we wish to have
a 0◦ rather than −180◦ phase response at ω = 0.
Now the idea is the following. Suppose N = 0 in (10.83) (that is we have
no 1-pole allpasses in the serial decomposition yet). Adding the ﬁrst allpass A1
at the cutoﬀ ω1 we make the phase response of (10.83) equal to the one of a
1-pole allpass (Fig. 10.35). From ω = 0 to ω = ωn the phase response is kind
of what we expect it to be:
it starts at arg H(0) = 0 and then decreases to
arg H(jωn) = −π/2. However, after ω = ωn it continues to decrease, which is
not what we want. Therefore we insert another allpass A2 with a negative cutoff
−ω2:
H(s) =
ω1 − s
ω1 + s
·
−ω2 − s
−ω2 + s
0 < ω1 < ω2
Clearly, A2 is unstable. However, we already know that unstable components
of H(s) are not a problem, since they simply go into the H −1
+ part of the phase
splitter.
The phase response of a negative-cutoﬀ allpass (Fig. 10.37) is the inversion
of Fig. 10.35. Therefore, given suﬃcient distance between ω1 and ω2, the phase
response of H will ﬁrst drop below −π/2 (shortly after ω = ω1) and then at some
point turn around and grow back again (Fig. 10.38). Then we insert another
positive-cutoﬀ allpass A3, then a negative-cutoﬀ allpass A4 etc., obtaining if not
an equiripple approximation of −90◦ phase response, then something of a very
similar nature (Fig. 10.39).
The curve in Fig. 10.39 has two obvious problems. The ripple amplitude is
way too large. Furthermore, in order to obtain this kind of curve, we need to
position the cutoﬀs ωn pretty wide apart (4 octaves between the neighboring
cutoﬀs is a safe bet). We would like to position the cutoﬀs closer together,
thereby reducing the ripple amplitude, however the uniform spacing of the cut-
oﬀs doesn’t work very well for denser spacings of the cutoﬀs. We need to ﬁnd a
way to identify the optimum cutoﬀ positions.
10.14. NUMERICAL CONSTRUCTION OF PHASE SPLITTER
481
arg H(jω)
0
−π/2
−π
−2π
arg H(jω)
0
−π
−2π
ωc/8
ωc
8ωc
ω
Figure 10.35: Phase response of a 1-pole allpass ﬁlter.
ωc/8
ωc
8ωc
ω
Figure 10.36: Phase response of a 2-pole allpass ﬁlter.
Using cutoﬀs of alternating signs, we rewrite the transfer function expression
(10.83) as
H(s) =
N
(cid:89)
n=1
An(s) =
N
(cid:89)
n=1
(−1)n+1ωn − s
(−1)n+1ωn + s
0 < ω1 < ω2 < . . . < ωN (10.84)
(the cutoﬀ of A1 needs to be positive in order for the phase response of H to
have a negative derivative at ω = 0). Considering that the phase response of a
1-pole allpass with cutoﬀ ωc is
H(jω) = −2 arctan
ω
ωc
the phase response of the serial decomposition (10.84) is
ϕ(x) = arg H(jω) = 2
N
(cid:88)
(−1)n arctan
n=1
ω
ωn
= 2
N
(cid:88)
n=1
(−1)n arctan ex−an (10.85)
482
arg H(jω)
π
π/2
0
CHAPTER 10. SPECIAL FILTER TYPES
ωc/8
ωc
8ωc
ω
Figure 10.37: Phase response of a negative-cutoﬀ 1-pole allpass
ﬁlter.
arg H(jω)
0
−π/2
−π
ω1
ω2
ω
Figure 10.38: Phase response of a pair of a positive-cutoﬀ and a
negative-cutoﬀ 1-pole allpass ﬁlters. Frequency scale is logarith-
mic.
ω = ex
ωn = ean
where x and an are the logarithmic scale counterparts of ω and ωn (essentially
these are the pitch-scale values, we have just used e rather than 2 as the base
to simplify the expressions of the derivatives of ϕ). The reason to use the
logarithmic scale in (10.85) is that the phase responses of 1-pole allpasses are
symmetric in the logarithmic scale, therefore the entire problem gets certain
symmetry and uniformity.
Now we are in a position to specify the minimax approximation problem of
construction of the phase shifter H−90. We wish to ﬁnd the minimax approx-
imation of f (x) ≡ −π/2 on the speciﬁed interval x ∈ [xmin, xmax], where the
approximating function ϕ(x) needs to be of the form (10.85).
10.14. NUMERICAL CONSTRUCTION OF PHASE SPLITTER
483
arg H(jω)
0
−π/2
−π
ω
Figure 10.39: Phase response of a series of alternating positive-
cutoﬀ and negative-cutoﬀ 1-pole allpass ﬁlters. Frequency scale is
logarithmic.
The approximating function ϕ(x) has N parameters:
ϕ(x) = ϕ(x, a1, a2, . . . , aN )
which can be found by using the Remez algorithm for approximations of general
form. Notably, for larger N and smaller intervals [xmin, xmax] the problem be-
comes more and more nonlinear, requiring smaller damping factors α in (10.81)
and (10.82). The damping factors may be chosen by restricting the lengths
|an+1 − an| and |¯an+1 − ¯an| in (10.81) and (10.82).
In order to further employ the logarithmic symmetry of the problem (al-
though this is not a must), we may require xmin + xmax = 0 corresponding to
ωminωmax = 1. Then the following applies.
- Due to the symmetry ωminωmax = 1 the obtained cutoﬀs ωn will also be
symmetric: ωnωN +1−n = 1. (Actually they will be symmetric relatively
ωminωmax no matter what the ωmin and ωmax are, but it’s convenient
to
to have this symmetry more explicitly visible.)
√
- Using this symmetry the number of cutoﬀ parameters can be halved (for
odd N the middle cutoﬀ ω(N +1)/2 is always at unity and therefore can be
also excluded from the set of varying parameters). Essentially we simply
restrict ϕ(x) to be an odd (for odd N ) or even (for even N ) function of x.
- The obtained symmetric range [ωmin, ωmax] can be scaled by an arbitrary
constant A by scaling the allpass cutoﬀs by the same constant:
[ωmin, ωmax] ← [Aωmin, Aωmax]
ωn ← Aωn
Figs. 10.40 and 10.41 contain example approximations of H−90(s) obtained by
cutoﬀ optimization (for the demonstration purposes, the approximation orders
have been chosen relatively low, giving the phase ripple amplitude of an order
484
CHAPTER 10. SPECIAL FILTER TYPES
of magnitude of 1◦). The readers are encouraged to compare these pictures
(qualitatively, since the speciﬁed ﬁlter orders and bandwidths do not match) to
Fig. 10.28.
arg H(jω)
−80◦
−85◦
−90◦
−95◦
1/16
1
16
ω
Figure 10.40:
H−90(s).
8th-order minimax approximation of the ideal
arg H(jω)
−80◦
−85◦
−90◦
−95◦
1/16
1
16
ω
Figure 10.41:
H−90(s).
7th-order minimax approximation of the ideal
Instead of solving the initial approximation equation (10.82) there is a diﬀer-
ent approach, which generally results in the nonlinearity of Φ(a) not so strongly
aﬀecting the algorithm convergence. We could take the manually constructed
(10.84) with 4-octave spaced cutoﬀs ωn+1 = 16ωn as our initial approximation.
The formal range of interest could contain two additional octaves on each side:
ωmin = ω1/4, ωmax = 4ωN . Employing the logarithmic symmetry, we center the
whole range around ω = 1, so that ωminωmax = 1.
Using (10.81) (in the logarithmic scale x) we reﬁne the initial approximation
to the ripples of equal amplitude. Then we simply shrink the range a little bit.
SUMMARY
An eﬃcient shrinking substitution is using the geometric averages:
ωmin ←
ωmax ←
√
√
ωminω1
ωmaxωN
485
(10.86)
The substitution (10.86) doesn’t aﬀect the control points ˆxn or the zeros ¯xn of
the Remez algorithm. Therefore after the substitution the Remez algorithm can
be simply run again. Then the substitution is performed again, and so on, until
we shrink the interval [ωmin, ωmax] to the exact desired range.34
Notice that the approximations on the intermediate ranges [ωmin, ωmax] do
not need to be obtained with a very high precision, since their only purpose is
to provide a starting point for the next application of the Remez algorithm on a
smaller range. It is only the Remez algorithm on the exact desired range, which
needs to be run to a high precision. This can noticeably improve the algorithm’s
running time.
SUMMARY
We have discussed various approaches to the construction of shelving ﬁlters,
crossovers and Hilbert transformers. The basis for the construction happened
to be mostly EMQF ﬁlters, with 1st-kind Butterworth as their limiting case.
The slope control in higher-order shelving ﬁlters was implemented using 2nd-
kind Butterworth ﬁlters, although EMQF ﬁlters can also be used here with the
drawback of having ripples in the pass and shelving bands.
Further reading
S.J.Orfanidis, Lecture notes on elliptic filter design (available on the au-
thor’s webpage).
M.Kleehammer, Mathematical development of the elliptic filter (available
in QSpace online repository).
Elliptic filter (Wikipedia artile).
L.M.Milne-Thomson, Jacobian elliptic functions and theta functions and
Elliptic Integrals (contained in Handbook of mathematical functions by
M.Abramowitz and I.A.Stegun, available on the internet).
34Of course at the last step we simply set ωmin and ωmax to the desired values, rather than
perform the substitution (10.86).
486
CHAPTER 10. SPECIAL FILTER TYPES
Chapter 11
Multinotch filters
Multinotch ﬁlters have various uses. One of their most common applications
is in phaser and ﬂanger eﬀects, which are built by modulating the parameters
(in the simplest and the most common case just the cutoﬀ) of the respective
multinotch by an LFO. The main diﬀerence between a phaser and a ﬂanger is
that in the former the multinotch ﬁlter is based around a chain of diﬀerential
allpass ﬁlters, while in the latter the allpass chain is replaced by a delay (thus
making a comb ﬁlter).
11.1 Basic multinotch structure
Let G(s) be an arbitrary allpass:
|G(jω)| = 1
arg G(jω) = ejϕ(ω)
where ϕ(ω) is the allpass’s phase response, and consider the transfer function
of the form
H(s) =
1 + G(s)
2
(11.1)
corresponding to the system in Fig. 11.1.
x(t)
•(cid:47)
G(s)
+
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
y(t)
Figure 11.1: A basic multinotch. G(s) is an allpass.
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Writing out the amplitude response of H(s) we have
|H(jω)|2 =
=
=
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
1 + ejϕ
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(1 + cos ϕ)2 + sin2 ϕ
4
1 + cos ϕ + j sin ϕ
2
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
=
2 + 2 cos ϕ
4
=
1 + cos ϕ
2
= cos2 ϕ
2
487
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
488
and
Thus
CHAPTER 11. MULTINOTCH FILTERS
|H(jω)| =
(cid:12)
(cid:12)
(cid:12)
(cid:12)
cos
ϕ(ω)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
|H(jω)| = 1 ⇐⇒ ϕ = 2πn
|H(jω)| = 0 ⇐⇒ ϕ = π + 2πn
(n ∈ Z)
The points |H(jω| = 1 where the amplitude response of H(s) is maximal are
referred to as peaks and the points |H(jω| = 0 where the amplitude response
of H(s) is minimal are referred to as notches. So the peaks occur where the
phase response of the allpass G(s) is zero and the notches occur where the
phase response of the allpass G(s) is 180◦. This is also fully intuitive: when
the phase response of G(s) is zero, both mixed signals add together, when the
phase response of G(s) is 180◦, both mixed signals cancel each other.
The ﬁlters whose phase response contains several notches are referred to as
multinotch ﬁlters. Apparently the ﬁlter in Fig. 11.1 is a multinotch.
11.2 1-pole-based multinotches
The allpass G(s) can be arbitrary. However there are some commonly used op-
tions. One of such options is to use a chain of identically tuned 1-pole allpasses:
G(s) = GN
1 (s)
G1(s) =
1 − s
1 + s
The phase response of a 1-pole allpass according to (2.13) is
arg G1(jω) = −2 arctan ω
Respectively
ϕ(ω) = arg G(jω) = N arg G1(jω) = −2N arctan ω
(Fig. 11.2). The symmetry of the graph of ϕ(ω) in the logarithmic frequency
scale is apparently due to the same symmetry of the phase response of the 1-pole
allpass.
So the peaks occur whenever
ϕ = −2N arctan ω = −2πn ⇐⇒ ω = tan
2πn
2N
= tan
πn
N
and the notches occur whenever
ϕ = −2N arctan ω = −π − 2πn ⇐⇒ ω = tan
Or, combining peaks and notches together, we have
π + 2πn
2N
= tan
π
2 + πn
N
ϕ = −2N arctan ω = −πn ⇐⇒ ω = tan
πn
2N
Since we need 0 ≤ ω ≤ +∞, the range of values of n is obtained from
0 ≤
πn
2N
≤
π
2
11.3. 2-POLE-BASED MULTINOTCHES
489
arg G(jω)
0
−π
−2π
−3π
−4π
−5π
−6π
ωc/8
ωc
8ωc
ω
Figure 11.2: Phase response of a chain of 6 identical 1-pole all-
passes. Black dots correspond to multinotch’s peaks, white dots
correspond to multinotch’s notches.
giving
0 ≤ n ≤ N
Thus the total count of peaks plus notches is N + 1. Noticing that the peaks
correspond to even values of n and notches correspond to odd values of n we
have the following pictures:
If N is even there are N/2 + 1 peaks (including the ones at ω = 0 and ω =
+∞) and N/2 notches. Figs. 11.3 and 11.4 illustrate.
If N is odd there are (N + 1)/2 peaks (starting at the one at ω = 0) and (N +
1)/2 notches (the last notch occuring at ω = +∞). Fig. 11.5 illustrates.
Odd counts are less commonly used due to unsymmetric shape of the amplitude
response.
11.3
2-pole-based multinotches
Instead of 1-pole allpasses we could use 2-pole allpasses:
G(s) = GN
2 (s)
G2(s) =
1 − 2Rs + s2
1 + 2Rs + s2
490
|H(jω)|
1
0.5
0
CHAPTER 11. MULTINOTCH FILTERS
ωc/8
ωc
8ωc
ω
Figure 11.3: Amplitude response of a multinotch built around a
chain of 4 identical 1-pole allpasses.
|H(jω)|
1
0.5
0
ωc/8
ωc
8ωc
ω
Figure 11.4: Amplitude response of a multinotch built around a
chain of 6 identical 1-pole allpasses.
Note that at R = 1 we obtain an equivalent of a chain of 2N 1-pole allpasses.
According to (4.24) and (4.5) the phase response of a 2-pole allpass is
arg G2(jω) = −2 arccot
ω−1 − ω
2R
or, in terms of logarithmic frequency scale (where we also use (4.6))
arg G2(jex) = −2 arccot
− sinh x
R
Thus
ϕ(ω) = arg G(jω) = N arg G2(jω) = −2N arccot
ϕ(ex) = arg G(jex) = N arg G2(jex) = −2N arccot
ω−1 − ω
2R
− sinh x
R
11.3. 2-POLE-BASED MULTINOTCHES
491
|H(jω)|
1
0.5
0
ωc/8
ωc
8ωc
ω
Figure 11.5: Amplitude response of a multinotch built around a
chain of 5 identical 1-pole allpasses.
Thus this time ϕ(ω) is going from 0 to −2πN , which means that we obtain only
symmetric amplitude responses, similar to the ones which we were getting for
even numbers of 1-pole allpasses. Fig. 11.6 illustrates. By adjusting the value
of R we change the steepness of the phase response and thereby the distance
between the notches.
|H(jω)|
1
0.5
0
R = 0.3
R = 5
R = 1
ωc/8
ωc
8ωc
ω
Figure 11.6: Amplitude response of a multinotch built around a
chain of 2 identical 2-pole allpasses (at diﬀerent damping values).
The ﬁrst notch occurs at ϕ = −π, that is
or
−2N arccot
− sinh x
R
= −π
sinh x = −R cot
π
2N
from where we can obtain the logarithmic position of the ﬁrst notch
x = − sinh−1 (cid:16)
R cot
(cid:17)
π
2N
< 0
492
CHAPTER 11. MULTINOTCH FILTERS
The logarithmic position of the last notch is respectively −x and the logarith-
mic bandwidth (in base e) is therefore −2x, while the respective bandwidth in
octaves is −2x/ ln 2:
∆ =
2
ln 2
sinh−1 (cid:16)
R cot
(cid:17)
π
2N
Notice the obvious similarly of the above formula to (4.19).
11.4 Inversion
By multiplying an allpass ﬁlter’s output by −1 we obtain another allpass. At
frequencies where the phase response was 0◦ we thereby obtain 180◦ and vice
versa. This means that if such allpass is used as a core of the multinotch in
Fig. 11.1, inverting the allpass’s output will swap the peak and notch positions
(compare Fig. 11.7 vs. Fig. 11.4).
|H(jω)|
1
0.5
0
ωc/8
ωc
8ωc
ω
Figure 11.7: Amplitude response of a multinotch built around a
chain of 6 identical 1-pole allpasses with inversion (compare to
Fig. 11.4).
The structure of Fig. 11.1 can be modiﬁed as shown in Fig. 11.8 to accomo-
date optional inversion.
x(t)
•(cid:47)
G(s)
±1
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
+
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
y(t)
Figure 11.8: Multinotch from Fig. 11.1 with optional inversion.
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
11.5. COMB FILTERS
11.5 Comb filters
493
A delay is also an allpass. It is not a diﬀerential allpass, since it’s not based on
integrators, but it is still an allpass. Indeed, taking the delay equation
y(t) = x(t − T )
where T is delay time and letting x(t) = Aest we have
y(t) = Aes(t−T ) = e−sT · Aest = e−sT · x(t)
Since the delay is linear (in the sense that a delayed linear combination of
two signals is equal to the same linear combination of these signals delayed
separately) we could apply (2.7) which means that the transfer function of the
delay is
Apparently
H(s) = e−sT
|H(jω)| = 1
arg H(jω) = −ωT
and thus the delay is an allpass.
Therefore we can use the delay as the allpass core of the multnotch ﬁlter
in Fig. 11.1. Letting G(s) = e−sT we have ϕ(ω) = −ωT (Fig. 11.9). The
peak/notch equation is respectively
from where
−ωT = −πn
ω =
πn
T
= 2π ·
n
2T
or, in ordinary frequency scale
f =
n
2T
The peaks and notches are therefore harmonically spaced with a step of 1/2T
Hertz (Fig. 11.10). The amplitude response in Fig. 11.10 looks like a comb.
Hence this kind of multinotch ﬁlters are referred to as comb filters.
Since the peaks and notches of the comb ﬁlter’s amplitude response occur at
f = n/2T , the frequency 1/2T is the fundamental frequency of this harmonic
series. It is convenient to use this frequency as comb’s ﬁlter formal cutoﬀ fc =
1/2T .
If there is no inversion, then (excluding the DC peak at f = 0) the peaks of
the amplitude response are located at frequencies 2fc, 4fc, 6fc, etc. This makes
the perceived fundamental frequency of the comb ﬁlter (especially in the case of
a strong resonance1) rather be 2fc. However in the case of inversion the peaks
are located at fc, 3fc, 5fc, etc., giving an impression (which is stronger in the
case of a strong resonance) of an odd-harmonics-only signal at frequency fc.
1Resonating multinotches will be discussed later in this chapter.
CHAPTER 11. MULTINOTCH FILTERS
494
arg G(jω)
0
−π
−2π
−3π
−4π
−5π
0
1
2T
2
2T
3
2T
4
2T
5
2T
f
Figure 11.9: Phase response of a delay. Black dots correspond to
multinotch’s peaks, white dots correspond to multinotch’s notches.
The frequency scale is linear!
|H(jω)|
1
0.5
0
1
2T
3
2T
5
2T
7
2T
9
2T
f
Figure 11.10: Amplitude response of a multinotch built around a
delay (comb ﬁlter). The frequency scale is linear!
11.6. FEEDBACK
11.6 Feedback
495
Suppose we introduce feedback into the structure of Fig. 11.1 as shown Fig. 11.11.
Now the output of the allpass G(s) is not anymore purely the allpassed input
signal. Let’s introduce the notation ˜y(t) for the post-allpass signal (as shown
in Fig. 11.11) and ˜G(s) for the respective transfer function (in the sense of
˜y = G(s)x for complex exponential x). We also introduce the pre-allpass signal
˜x(t), but we are not going to use it for now. Then we are having
˜G(s) =
G(s)
1 − kG(s)
x(t)
•(cid:47)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
˜x(t)
G(s)
˜y(t)
•(cid:47)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
y(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 11.11: Multinotch from Fig. 11.1 with added feedback. Note
that this ﬁgure is showing a poor mixing option.
The transfer function of the entire multinotch thereby turns into
H(s) =
1 + ˜G(s)
2
=
1
2
·
1 − kG(s) + G(s)
1 − kG(s)
=
1
2
·
1 + (1 − k)G(s)
G(s)
or, in frequency response terms
H(jω) =
1
2
·
1 + (1 − k)ejϕ
1 − kejϕ
We can immediately notice that as soon as k > 0 the numerator of the frequency
response doesn’t turn to zero anymore, respectively we are not having fully deep
notches in the amplitude response (Fig. 11.12).
Instead of mixing ˜y(t) with x(t) let’s mix it with ˜x(t), as shown in Fig. 11.13.
The transfer function corresponding to the signal ˜x(t) in Fig. 11.11 is
˜G(s)
G(s)
=
1
1 − kG(s)
and thus we obtain
H(s) =
(cid:18)
1
2
·
1
1 − kG(s)
+
G(s)
1 − kG(s)
(cid:19)
=
1
2
·
1 + G(s)
1 − kG(s)
This transfer funtion looks much better, since it preserves fully deep notches.
The frequency response turns into
H(jω) =
1
2
·
1 + ejϕ
1 − kejϕ
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
496
|H(jω)|
1.5
1
0.5
0
CHAPTER 11. MULTINOTCH FILTERS
ωc/8
ωc
8ωc
ω
Figure 11.12: Amplitude response of the multinotch in Fig. 11.11
built around a chain of 6 identical 1-pole allpasses at k = 0.5.
Dashed curve corresponds to k = 0 (the same response as in
Fig. 11.4).
x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
•(cid:47)
˜x(t)
G(s)
˜y(t)
•(cid:47)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
y(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 11.13: Multinotch from Fig. 11.1 with added feedback and
corrected mixing.
which varies between
H(jω) =
1
2
·
1 + 1
1 − k
=
1
1 − k
when ϕ = 2πn
(11.2a)
and
H(jω) =
1
2
·
1 − 1
1 − k
= 0
when ϕ = π + 2πn
(11.2b)
The amplitude response is then
|H(jω)|2 =
=
=
1
4
1
4
1
2
·
·
·
1 + cos ϕ + j sin ϕ
1 − k cos ϕ − jk sin ϕ
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
(cid:12)
(cid:12)
(cid:12)
(cid:12)
·
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1 + ejϕ
1 − kejϕ
2 1
4
(1 + cos ϕ)2 + sin2 ϕ
(1 − k cos ϕ)2 + k2 sin2 ϕ
=
1
4
·
1 + cos ϕ
1 + k2 + 2k − 2k(1 + cos ϕ)
=
=
2 + 2 cos ϕ
1 + k2 − 2k cos ϕ
cos2 ϕ
2
(1 + k)2 − 4k cos2 ϕ
2
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
11.6. FEEDBACK
497
Again one can see that H(jω)| = 1/(1 − k) when cos2(ϕ/2) = 1 and H(jω) = 0
when cos2(ϕ/2) = 0. Thus the eﬀect of the feedback in Fig. 11.13 is that the
peaks become 1/(1 − k) times higher (given 0 < k < 1) and notches stay intact.
Fig. 11.14 illustrates. Observe that the peaks become higher and narrower.2
|H(jω)|
2
1.5
1
0.5
0
ωc/8
ωc
8ωc
ω
Figure 11.14: Amplitude response of the multinotch in Fig. 11.13
built around a chain of 6 identical 1-pole allpasses at k = 0.5.
Dashed curve corresponds to k = 0 (the same response as in
Fig. 11.4).
We could combine the feedback (Fig. 11.13) and the inversion (Fig. 11.8), as
shown in Fig. 11.15. Apparently the inversion only adds another 180◦ to ϕ(ω),
swapping peaks and notches. Therefore the results of the previous discussion of
Fig. 11.13 equally apply to Fig. 11.15.
x(t)
•(cid:47)
˜x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
G(s)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
±1
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
˜y(t)
•(cid:47)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
1/2
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
y(t)
Figure 11.15: Multinotch with feedback and inversion.
As we should recall from the discussion of ladder ﬁlters, the feedback be-
comes unstable when the total gain across the feedback loop, computed at a
2Since ˜x = x + k ˜y, instead of simple averaging y = (˜x + ˜y)/2 we could have had
y =
x + k ˜y + ˜y
2
=
1
2
x +
1 + k
2
˜y
however this doesn’t seem to give any benefits compared to the previous option, while we need
to adjust the mixing coefficient for ˜y depending on the feedback amount, which is rather a
drawback.
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
498
CHAPTER 11. MULTINOTCH FILTERS
frequency where the total phase shift across the feedback loop is zero, exceeds
1. Apparently in the case of Fig. 11.15 the zero total phase shift is occurring
exactly at the frequencies where the multinotch has peaks, while the total feed-
back loop gain at these frequencies is simply k. Therefore the multinotch ﬁlter
becomes unstable at k = 1 and the suggested range of k is 0 ≤ k < 1.3 Note
that the presence of the inversion doesn’t really change the stable range of k,
since the allpass G(s) is anyway delivering all possible phase shifts across the
frequency range 0 ≤ ω < +∞, and there always will be frequencies at which the
total feedback loop phase shift is zero (thereby producing amplitude response
peaks), regardless of whether the inversion is on or oﬀ. Thus the feedback loop
will be stable as long as |k| < 1.
Feedback shaping
Being essentially a ladder allpass, the multnotch in Fig. 11.15 can accomodate
feedback shaping, as discussed in Section 5.4. Notably, as long as the amplitude
responses of the shaping ﬁlters do not exceed 1, neither will the total feedback
loop gain (since in the absence of shaping ﬁlters the feedback loop gain is exactly
1 at all frequencies). This means that the stability of the feedback loop for
|k| < 1 will not be destroyed, no matter what the phase responses of the shaping
ﬁlters are.
11.7 Dry/wet mixing
So far we have been mixing the allpass-processed signal and the input signal
(or, if we are using feedback, the pre-allpass signal ˜x(t) with the post-allpass
signal ˜y(t)) in equal amounts:
y =
˜x + ˜y
2
Let’s crossfade the multinotch ﬁlter output signal with the input signal:
y = a
˜x + ˜y
2
+ (1 − a)x
(11.3)
If the multinotch is being used as a part of a phaser or ﬂanger eﬀect, the input
signal is commonly referred to as the dry signal while the multinotch output
signal (˜x + ˜y)/2 is referred to as the wet signal.4
According to (11.2a) the phase response of the feedback multinotch at the
peak is zero, therefore the peak, having the height 1/(1−k) should mix naturally
with the input signal (correspoding to the transfer function equal to 1 every-
where), producing a smooth crossfade between 1/(1 − k) and 1 in the amplitude
3Negative values of k lower the amplitude response peaks below 1, simultaneously making
them wider and respectively making the notches narrower. Being narrower, such notches
become less audible, even if we compensate for the amplitude loss by multiplying the signal
by 1 − k, thus the case of k < 0 is less common.
4Sometimes just the allpass output signal ˜y is referred to as the wet signal. Such termi-
nology is however more appropriate for an effect such as e.g. chorus, where the main idea of
the effect is pitch detuning produced by delay modulation. In comparison e.g. in a flanger
the main idea of the effect is the appearance of the notches, while pitch detuning, if present
at all, is rather a modulation artifact. Thus, in absence of strong modulation, the output of
the flanger’s delay will be hardly distinguishable by ear from the dry signal, not really being
“wet”.
11.7. DRY/WET MIXING
499
response at this frequency. This is indeed the case and the amplitude response
of a multinotch will nicely crossfade into a unity gain response (Fig. 11.16). The
respective structure is shown in Fig. 11.17.
|H(jω)|
2
1.5
1
0.5
0
ωc/8
ωc
8ωc
ω
Figure 11.16: Amplitude response of the multinotch in Fig. 11.13
built around a chain of 6 identical 1-pole allpasses at k = 0.5 with
a dry/wet mixing ratio of 50%. Dashed curve corresponds to a
dry/wet mixing ratio of 100% (same response as in Fig. 11.14).
x(t)
•(cid:47)
•(cid:47)
˜x(t)
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
G(s)
(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77) (cid:111)
k
(cid:49)(cid:49)(cid:49)
(cid:13)(cid:13)(cid:13)
1 − a
˜y(t)
•(cid:47)
+
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
±1
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
a/2
+
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
y(t)
Figure 11.17: Multinotch with feedback, inversion and dry/wet
mixing.
Since ˜x = x + k ˜y, we can rewrite (11.3) as
y = a
(cid:16)
=
x + k ˜y + ˜y
2
(cid:17)
a
2
1 −
x +
(x + (1 + k)˜y) + (1 − a)x =
a
2
+ (1 − a)x =
a
2
(1 + k)˜y
Thus, even though normally 0 ≤ a ≤ 1, we could let a grow all the way to a = 2,
in which case only the allpass output ˜y (albeit boosted by 1 + k) will be present
in the output signal.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:15)
(cid:15)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:111)
(cid:79)
(cid:79)
(cid:15)
(cid:15)
(cid:15)
(cid:15)
500
CHAPTER 11. MULTINOTCH FILTERS
11.8 Barberpole notches
Consider the frequency shifter in Fig. 10.31 and let’s replace ∆ω · t with some
ﬁxed value ∆ϕ, obtaining a similar structure shown in Fig. 11.18.5
x(t)
•(cid:47)
H −1
+
H−
y(t)
cos ∆ϕ
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
(cid:77)(cid:77)(cid:77)(cid:113)(cid:113)(cid:113)
sin ∆ϕ
+
−
(cid:32)(cid:39)(cid:33)(cid:38)(cid:34)(cid:37)(cid:35)(cid:36)
Figure 11.18: Barberpole allpass, obtained from the frequency
shifter in Fig. 10.31.
Across the supported bandwidth of the frequency shifter the phase diﬀerence
between the allpasses H −1
+ and H− is 90◦. That is
ϕ+(ω) − ϕ−(ω) = 90◦
where
or simply
ϕ+(ω) = arg H −1
+ (jω)
ϕ−(ω) = arg H−(jω)
H−(s) = −jH −1
+ (s)
The frequency response of the structure in Fig. 11.18 (within the supported
bandwidth of the frequency shifter) is thereby
G(jω) = H −1
= H −1
= H −1
+ (jω) · cos ∆ϕ − H−(jω) · sin ∆ϕ =
+ (jω) · cos ∆ϕ + jH −1
+ (jω) · (cos ∆ϕ + j sin ∆ϕ) = ejΔϕ · H −1
+ (jω) · sin ∆ϕ =
+ (jω)
from where we repsectively obtain
|G(jω)| = (cid:12)
+ (jω)(cid:12)
(cid:12)ejΔϕ(cid:12)
arg G(jω) = arg ejΔϕ + arg H −1
(cid:12)H −1
(cid:12) · (cid:12)
(cid:12) = 1
+ (jω) = arg H −1
+ (jω) + ∆ϕ
(11.4a)
(11.4b)
That is, G(s) is an allpass and by varying ∆ϕ we can arbitrarily oﬀset its phase
response! Of course, this holds only within the frequency shifter’s bandwidth,
but nevertheless it’s a very remarkable property.
But what does the phase response of G(s) actually look like? Apparently
it depends on the details of H −1
+ and H−
are built from 1-pole allpasses obtained by minimax optimization of the phase
diﬀerence (e.g. by using formula eq:ellip:PhaseSplit:PolesZeros), the phase re-
sponses of H −1
+ and H− will look like the ones in Fig. 11.19, where we ﬁrst
should concentrate on the phase responses shown by solid lines.
+ and H− implementations.
If H −1
5The author was introduced to the approach of using the frequency shifter structure to
implement barberpole phasers and flangers by Dr. Julian Parker.
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:47)
(cid:79)
(cid:79)
(cid:47)
(cid:47)
11.8. BARBERPOLE NOTCHES
501
arg G(jω)
0
−π
−2π
−3π
−4π
−5π
−6π
1/32
1
32
ω
Figure 11.19: Phase responses of H −1
+ and H− (each consisting of
six 1-pole allpasses). The frequency shifter bandwidth is 10 octaves
(bounded by vertical dashed lines at ω = 1/32 and ω = 32). Black
dots correspond to multinotch’s peaks arising out of H −1
+ , white
dots correspond to the respective notches. Dashed curves show
“aliased” phase responses.
Aside from being 90◦ apart across the frequency shifter bandwidth, the phase
responses in Fig. 11.19 do not look much diﬀerent from the phase responses we
have been using earlier, such as e.g. in Fig. 11.2. Thus H −1
+ or H− will provide
a decent allpass to be used in a multinotch. By using (11.4b) we can obtain an
oﬀset phase response of H −1
+ as the phase response of G(s), which will result in
shifted peaks and notches of the multinotch (compared to their positions arising
out of H+−1).
However, recall that the phase is deﬁned modulo 360◦. That is a phase re-
sponse of −20◦ is exactly the same as the phase response of −380◦ or of −740◦
etc. This has been shown by the dashed curves in Fig. 11.19, they represent
alternative interpretations or “aliased” versions of the “principal” (solid-line)
phase responses. Notice how the black and white dots on the aliased responses
of H −1
+ correspond to exactly the same peak and notch frequencies as the ones
arising out of the principal phase responses (reﬂecting the fact that it doesn’t
matter if we use a principal or an aliased phase response to determine peak and
notch positions). By oﬀsetting the phase response of H −1
+ (visually this corre-
sponds to a vertical shifting of the responses in Fig. 11.19) we simultaneously
oﬀset all its aliases by the same amount.
502
CHAPTER 11. MULTINOTCH FILTERS
Imagine that ∆ϕ is increasing, thus the principal and aliased responses of
H −1
+ in Fig. 11.19 are continuously moving upwards, and the notches and peaks
are continuously moving to the right.6 In turn, each of the peaks and notches
will disappear on the right at ω = +∞ simultaneously reappearing from the left
at ω = 0. Thus the peaks and notches will move “endlessly” from left to the
right. Respectively if ∆ϕ is decreasing, they will move from right to the left.
This is the so-called barberpole effect.
In reality, however, the peaks and notches will not move all the way to ω =
+∞ or ω = 0. At some point they will leave the frequency shifter bandwidth, at
which moment (11.4b) will no longer hold. Particularly, the amplitude response
of G(s) will no longer stay allpass. At ω = 0 we have H −1
+ (0) = H−(0) = 1,
which means that
G(0) = 1 · cos ∆ϕ + 1 · sin ∆ϕ = cos ∆ϕ + sin ∆ϕ =
√
2 · cos
(cid:16)
∆ϕ −
(cid:17)
π
4
which means that the amplitude response of G(s) at ω = 0 can get as large as
√
2. The same situation occurs at ω = +∞. Respectively, if the multinotch
2. The explosion can be prevented
contains feedback, it will explode at k = 1/
by introducing low- and high-pass or -shelving ﬁlters into the feedback loop.7
√
Thus, we have built a barberpole phaser, where the peaks and notches can
move endlessly to the left or to the right. The same technique cannot be directly
used to build a barberpole ﬂanger, since, while we have a phase splitter acting
as a diﬀerential allpass, we do not have a phase splitter acting as a delay. This
would not be even possible in theory, since the phase response of a delay must be
proportional to the frequency (this is the property which ensures the harmonic
spacing of comb ﬁlter’s peaks and notches), but adding any constant to such
phase response will destroy this property. What is however possible is using an
allpass arising out of a serial connection of a delay and a barberpole allpass in
Fig. 11.18. This would destroy the perfect harmonic spacing of ﬂanger’s peaks
and notches, but one gets a barberpole eﬀect in return, as the phase responses
of the delay and the barberpole allpass add up.
SUMMARY
Multinotch ﬁlters can be build by mixing a signal with its allpassed version,
where the allpass could be a diﬀerential allpass or a delay, the latter resulting in
a comb ﬁlter. Inverting the allpass’s output swaps the peaks and the notches.
Adding feedback makes the peaks more prominent.
6In a practical implementation Δϕ would not be able to increase endlessly, as at some point
it will leave the representable range of values. If floating point representation is used, precision
losses will become intolerably large even before the value gets out of range. However, we don’t
really need to increase or decrease Δϕ endlessly, since what matters in the end (according to
Fig. 11.18) are the values of its sine and cosine. Thus we could wrap Δϕ to the range [−π, π],
or work directly with sine and cosine values (in which case it’s convenient to treat them as
real and imaginary parts of a complex number ejΔϕ).
√
7Particularly, for the 1-pole lowpass (or any 1st kind Butterworth lowpass) we have
2 ∀ω ≥ ωc, while outside of that range we still have |H(jω)| ≤ 1. There-
|H(jω)| ≤ 1/
fore such filter, placed at the upper boundary of the frequency shifter’s bandwidth, will be
guaranteed to mitigate the unwanted amplitude response boost in the high frequency range.
A highpass of the same kind placed at the lower boundary of the frequency shifter’s bandwidth
will perform the same in the low frequency range.
History
The revision numbering is major.minor.bugﬁx. Pure bugﬁx updates are not
listed here.
1.0.2
(May 18, 2012)
ﬁrst public revision
1.1.0
(June 7, 2015)
- TSK ﬁlters
- frequency shifters
- further minor changes
2.0.0alpha
(May 28, 2018)
- redone: TSK/SKF ﬁlters
- 8-pole ladder ﬁlters
- expanded: nonlinearities
- expanded: phasers and ﬂangers (now found under the title multinotch
filters)
- Butterworth transformations of the 1st and 2nd kinds
- classical signal processing ﬁlters (Butterworth, Chebyshev, elliptic)
- redone: shelving ﬁlters
- redone: Hilbert transformers
- crossovers
- state-space form
- transient responses
- many further smaller changes
503
504
HISTORY
2.1.0
(October 28, 2018)
- generalized ladder ﬁlters
- elliptic ﬁlters of order 2N (discussed as order-2N elliptic rational functions)
- elliptic shelving ﬁlter (and elliptic rational function) midslope steepness
derivation
Index
1-pole
Jordan, 37, 278
1-pole ﬁlter, 7, 199
transposed, 30
2-pole ﬁlter, 95
4-pole ﬁlter, 133
8-pole ladder ﬁlter, 158
allpass ﬁlter, 29, 119
SKF, 158
TSK, 158
allpass substitution, 92
amplitude
elliptic, 351
of oscillations around ∞, 344
amplitude response, 13, 51
analytic ﬁlter, 453, 456
analytic signal, 453
antisaturator, 211
arctangent scale, 313
bandpass ﬁlter, 95, 293, 303
barberpole, 502
BIBO, 21
bilinear transform, 57
inverse, 58
topology-preserving, 81
unstable, 89
bisection, 189
BLT, 57
BLT integrator, see trapezoidal inte-
grator
Butterworth ﬁlter, 103, 286, 294, 319
1st kind of, 286
2nd kind of, 294
Butterworth transformation, 283, 285
1st kind of, 286
2nd kind of, 294
canonical form, 79
cascade decomposition, 275
Chebyshev ﬁlter, 337, 344
type I, 337
type II, 344
Chebyshev polynomial, 332
double-reciprocated, 344
renormalized, 335
comb ﬁlter, 493
complex exponential, 5
complex impedances, 12
complex sinusoid, 1
controllable canonical form, 271
coupled-form resonator, 253
crossover, 437
cutoﬀ, 8, 14
of a pole, 109
of a zero, 109
parameterization of, 15, 275
cutoﬀ modulation, 40, 264
damping
in SVF, 100
of a pole, 109
of a zero, 109
DC oﬀset, 2
degree
of transformation, 383
degree equation, 383
delayless feedback, 73
DF1, 79
DF2, 79
diagonal form, 247, 278
diﬀerentiator, 91
diode clipper, 208
diode ladder ﬁlter, 164, 200
Dirac delta, 4
direct form, 79
discrimination factor, 387
eigenfunction, 9
elliptic ﬁlter, 400
minimum Q, 405
505
506
INDEX
elliptic function, 351
evaluation of, 374
normalized, 363
normalized-argument, 372
elliptic modulus, 350
elliptic rational function, 384
normalized, 389
renormalized, 394
ellitic integral, 350
EMQF, 405
equiripple, 309
equiripples, 332
even roots/poles, 288, 322, 449
ﬁlter
1-pole, 7, 199
2-pole, 95
4-pole, 133
allpass, 29, 119
analytic, 453, 456
bandpass, 95, 293, 303
Butterworth, 103, 286, 294, 319
comb, 493
elliptic, 405
highpass, 18, 95, 138, 292, 303
highpass TSK, 154
ladder, 133, 275, 305
lowpass, 7, 95, 133, 290, 302
lowpass SKF, 155
lowpass TSK, 154
multimode, 25, 95, 141, 276
multinotch, 487
normalized bandpass, 111
notch, 119
peaking, 121
Sallen–Key, 152, 154
shelving, 27, 118, 410
SKF, 154
stable, 21
tilting, 410
transposed, 30
TSK, 152
unit-gain bandpass, 111
ﬁxed-point iteration, 184
Fourier integral, 3
Fourier series, 2
Fourier transform, 3
frequency response, 13, 51
frequency shifter, 468
gain element, 8
generalized SVF, 271
hard clipper, 177, 198
harmonics, 2
Hermitian, 3
highpass ﬁlter, 18, 95, 138, 292, 303
Hilbert transform, 453
Hilbert transformer, 453, 456
hyperbolic functions, 325
imaginary Riemann circle, 312
instantaneous gain, 75
instantaneous oﬀset, 75
instantaneous response, 75
instantaneous smoother, 85
instantaneously unstable
feedback, 85
integrator, 8
BLT, see integrator, trapezoidal
naive, 47
trapezoidal, 53, 269
integratorless feedback, 239
Jacobian elliptic function, 351
evaluation of, 374
normalized, 363
normalized-argument, 372
Jordan 1-pole, 37, 278
Jordan 2-pole, 253, 278
Jordan cell, 256
real, 259
Jordan chain, 39, 257
Jordan normal form, 256
ladder ﬁlter, 133, 275
2-pole allpass, 158
8-pole, 158
bandpass, 146
diode, 164, 200
generalized, 305
highpass, 145
modes of, 141
OTA, 201
transistor, 199
Landen transformation, 374
Laplace integral, 5
Laplace transform, 5
linearity, 11
Linkwitz–Riley crossover, 439
INDEX
507
lowpass ﬁlter, 7, 14, 95, 133, 290, 302
LP to BP substitution, 114, 294
LP to BP transformation, 114
LP to BS substitution, 117
LP to HP substitution, 24
LP to HP transformation, 24
real diagonal form, 252, 278
real Riemann circle, 310
reference gain, 431
Remez algorithm, 472
representation, 320
resonance, 100, 106
Riemann circle
matrix exponential, 244
maximum phase, 24
MIMO
SKF, 155
minimax approximation, 472, 473
minimum phase, 24
minimum Q, 405
modular angle, 350
modulus
ellitpic, 350
imaginary, 312
real, 310
Riemann sphere, 309
rotations of, 314
rolloﬀ, 15, 21
Sallen–Key
highpass, 155
lowpass, 155
MIMO, 155
multimode ﬁlter, 25, 95, 141, 276
multinotch ﬁlter, 487
Sallen–Key ﬁlter, 152, 154
saturator, 174
N -th degree transformation, 383
naive integrator, 47
Newton–Raphson method, 186
nonstrictly proper, 11
normalized bandpass ﬁlter, 111
notch ﬁlter, 119
observable canonical form, 273
odd roots/poles, 288, 322, 449
OTA ladder ﬁlter, 201
parallel representation, 278
partial fraction expansion, 278
partials, 2
passband, 14, 97, 111
peaking ﬁlter, 121
phase response, 13, 51
phase splitter, 468
pole, 19, 35, 53
cutoﬀ of, 109
damping of, 109
preimage
of a representation, 320
prewarping, 62, 115
prewarping point, 65
principal values, 326
quarter period
imaginary, 354
quarter-period, 353
asymptotically linear, 176
bounded, 175
bounded-range, 175
compact-range monotonic, 175
slower than linear, 176
unbounded, 176
unbounded-range, 176
selectivity factor, 387
selfoscillation, 101, 128, 136, 180
selfoscillation point, 129
serial cascade, 275
shelving band, 417
shelving ﬁlter, 118, 410
1-pole, 27
SKF, 154
allpass, 158
highpass, 155
lowpass, 155
MIMO, 155
soft clipper, 177
spectrum, 2
stability, 21, 35, 268
time-varying, 42
state space, 239
state-space form, 237
state-variable ﬁlter, 95
steady state, 33
steady-state response, 33
stopband, 14, 97
substitution
LP to BP, 114, 294
INDEX
508
LP to BS, 117
LP to HP, 24
summator, 8
SVF, 95
generalized, 271
tilting ﬁlter, 410
time-invariant, 10
time-varying system, 42
topology, 42
topology-preserving transform, 59, 81
TPBLT, 81
TPT, 59, 81
transfer function, 11, 33, 50, 182
transfer matrix, 242, 267
transformation
LP to BP, 114
LP to HP, 24
transient response, 33, 245, 268
transistor ladder ﬁlter, 199
transition band, 14, 97, 417
transposition, 30, 243
trapezoidal integrator, 53, 269
trigonometric functions, 325
TSK
allpass, 158
TSK ﬁlter, 152
highpass, 154
lowpass, 154
uniform positiveness, 41
uniformly positive function, 264
unit delay, 48
unit-gain bandpass ﬁlter, 111
waveshaper, 173
z-integral, 46
z-transform, 46
zero, 19, 53
cutoﬀ of, 109
damping of, 109
zero-delay feedback, 74, 183
